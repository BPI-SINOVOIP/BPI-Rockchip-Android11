From 388cffc84faefd2cb599767a7a4e48a21be966cf Mon Sep 17 00:00:00 2001
From: Zhen Chen <chenzhen@rock-chips.com>
Date: Tue, 19 Feb 2019 15:58:23 +0800
Subject: [PATCH 2/3] Revert "MALI: rockchip: upgrade midgard DDK to
 r18p0-01rel0"

This reverts commit 91842c9d8b4f21a431643d97269d142d9524cd9a.

Change-Id: I94234f5d9b9c4399d7b7d0f4185889bb74b37edc
---
 .../devicetree/bindings/arm/mali-midgard.txt       |   30 +-
 .../devicetree/bindings/power/mali-opp.txt         |  163 --
 drivers/gpu/arm/midgard/Kbuild                     |   80 +-
 drivers/gpu/arm/midgard/Kconfig                    |   24 +-
 drivers/gpu/arm/midgard/Makefile                   |    2 +-
 drivers/gpu/arm/midgard/backend/gpu/Kbuild         |   10 +-
 .../arm/midgard/backend/gpu/mali_kbase_devfreq.c   |  207 +--
 .../backend/gpu/mali_kbase_gpuprops_backend.c      |    5 -
 .../gpu/arm/midgard/backend/gpu/mali_kbase_jm_as.c |  179 +-
 .../gpu/arm/midgard/backend/gpu/mali_kbase_jm_hw.c |  116 +-
 .../midgard/backend/gpu/mali_kbase_jm_internal.h   |   13 +-
 .../gpu/arm/midgard/backend/gpu/mali_kbase_jm_rb.c |  266 +--
 .../midgard/backend/gpu/mali_kbase_js_backend.c    |    5 +-
 .../midgard/backend/gpu/mali_kbase_mmu_hw_direct.c |   74 +-
 .../midgard/backend/gpu/mali_kbase_pm_backend.c    |   30 +-
 .../gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca.c |    5 +-
 .../midgard/backend/gpu/mali_kbase_pm_ca_devfreq.c |  129 --
 .../midgard/backend/gpu/mali_kbase_pm_ca_devfreq.h |   55 -
 .../arm/midgard/backend/gpu/mali_kbase_pm_defs.h   |   21 +-
 .../arm/midgard/backend/gpu/mali_kbase_pm_driver.c |  257 +--
 .../midgard/backend/gpu/mali_kbase_pm_internal.h   |   14 +-
 .../midgard/backend/gpu/mali_kbase_pm_metrics.c    |    4 +-
 .../arm/midgard/backend/gpu/mali_kbase_pm_policy.c |    6 +-
 .../backend/gpu/mali_kbase_power_model_simple.c    |  171 ++
 .../backend/gpu/mali_kbase_power_model_simple.h    |   47 +
 drivers/gpu/arm/midgard/ipa/Kbuild                 |   24 -
 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.c       |  585 -------
 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.h       |  148 --
 .../gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.c   |  219 ---
 .../gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.h   |   49 -
 .../gpu/arm/midgard/ipa/mali_kbase_ipa_simple.c    |  222 ---
 .../gpu/arm/midgard/mali_base_hwconfig_features.h  |   90 +-
 .../gpu/arm/midgard/mali_base_hwconfig_issues.h    |  160 +-
 drivers/gpu/arm/midgard/mali_base_kernel.h         |  175 +-
 drivers/gpu/arm/midgard/mali_base_kernel_sync.h    |   47 +
 drivers/gpu/arm/midgard/mali_kbase.h               |   38 +-
 .../gpu/arm/midgard/mali_kbase_10969_workaround.c  |    2 +-
 .../gpu/arm/midgard/mali_kbase_config_defaults.h   |   74 +-
 drivers/gpu/arm/midgard/mali_kbase_context.c       |   23 +-
 drivers/gpu/arm/midgard/mali_kbase_core_linux.c    | 1705 +++++---------------
 drivers/gpu/arm/midgard/mali_kbase_ctx_sched.c     |  203 ---
 drivers/gpu/arm/midgard/mali_kbase_ctx_sched.h     |  131 --
 .../gpu/arm/midgard/mali_kbase_debug_job_fault.c   |    7 +-
 .../gpu/arm/midgard/mali_kbase_debug_mem_view.c    |   73 +-
 drivers/gpu/arm/midgard/mali_kbase_defs.h          |  194 +--
 drivers/gpu/arm/midgard/mali_kbase_device.c        |   33 +-
 drivers/gpu/arm/midgard/mali_kbase_dma_fence.c     |  257 ++-
 drivers/gpu/arm/midgard/mali_kbase_dma_fence.h     |   23 +-
 drivers/gpu/arm/midgard/mali_kbase_event.c         |    6 +-
 drivers/gpu/arm/midgard/mali_kbase_fence.c         |  196 ---
 drivers/gpu/arm/midgard/mali_kbase_fence.h         |  266 ---
 drivers/gpu/arm/midgard/mali_kbase_fence_defs.h    |   51 -
 drivers/gpu/arm/midgard/mali_kbase_gator_api.c     |    6 +-
 .../gpu/arm/midgard/mali_kbase_gator_hwcnt_names.h |    8 +-
 .../midgard/mali_kbase_gator_hwcnt_names_tsix.h    |  291 ----
 drivers/gpu/arm/midgard/mali_kbase_gpu_id.h        |   16 +-
 drivers/gpu/arm/midgard/mali_kbase_gpuprops.c      |  214 +--
 drivers/gpu/arm/midgard/mali_kbase_gpuprops.h      |   22 +-
 .../gpu/arm/midgard/mali_kbase_gpuprops_types.h    |   12 +-
 drivers/gpu/arm/midgard/mali_kbase_hw.c            |  223 +--
 drivers/gpu/arm/midgard/mali_kbase_hw.h            |   17 +-
 drivers/gpu/arm/midgard/mali_kbase_hwaccess_jm.h   |   40 +-
 drivers/gpu/arm/midgard/mali_kbase_ioctl.h         |  656 --------
 drivers/gpu/arm/midgard/mali_kbase_ipa.c           |  431 +++++
 drivers/gpu/arm/midgard/mali_kbase_ipa.h           |   41 +
 drivers/gpu/arm/midgard/mali_kbase_ipa_tables.h    |  104 ++
 drivers/gpu/arm/midgard/mali_kbase_jd.c            |  171 +-
 drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.c    |  142 +-
 drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.h    |    4 +-
 drivers/gpu/arm/midgard/mali_kbase_js.c            |  474 ++++--
 drivers/gpu/arm/midgard/mali_kbase_js.h            |  159 +-
 drivers/gpu/arm/midgard/mali_kbase_js_ctx_attr.c   |    2 +
 drivers/gpu/arm/midgard/mali_kbase_js_defs.h       |   90 +-
 drivers/gpu/arm/midgard/mali_kbase_js_policy.h     |  763 +++++++++
 drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c |  292 ++++
 drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h |   81 +
 drivers/gpu/arm/midgard/mali_kbase_mem.c           |  481 ++----
 drivers/gpu/arm/midgard/mali_kbase_mem.h           |   96 +-
 drivers/gpu/arm/midgard/mali_kbase_mem_linux.c     |  734 +++++----
 drivers/gpu/arm/midgard/mali_kbase_mem_linux.h     |   23 +-
 drivers/gpu/arm/midgard/mali_kbase_mem_lowlevel.h  |    2 +-
 drivers/gpu/arm/midgard/mali_kbase_mem_pool.c      |   14 +-
 .../arm/midgard/mali_kbase_mem_profile_debugfs.c   |    4 +-
 drivers/gpu/arm/midgard/mali_kbase_mmu.c           |   95 +-
 drivers/gpu/arm/midgard/mali_kbase_mmu_mode.h      |    2 +-
 .../gpu/arm/midgard/mali_kbase_mmu_mode_aarch64.c  |    2 +-
 drivers/gpu/arm/midgard/mali_kbase_mmu_mode_lpae.c |   10 +-
 drivers/gpu/arm/midgard/mali_kbase_softjobs.c      |  453 +++---
 drivers/gpu/arm/midgard/mali_kbase_sync.c          |  182 +++
 drivers/gpu/arm/midgard/mali_kbase_sync.h          |  195 +--
 drivers/gpu/arm/midgard/mali_kbase_sync_android.c  |  537 ------
 drivers/gpu/arm/midgard/mali_kbase_sync_common.c   |   43 -
 drivers/gpu/arm/midgard/mali_kbase_sync_file.c     |  339 ----
 drivers/gpu/arm/midgard/mali_kbase_sync_user.c     |  156 ++
 drivers/gpu/arm/midgard/mali_kbase_tlstream.c      |  246 +--
 drivers/gpu/arm/midgard/mali_kbase_tlstream.h      |  221 +--
 drivers/gpu/arm/midgard/mali_kbase_uku.h           |   13 +-
 drivers/gpu/arm/midgard/mali_kbase_vinstr.c        |   46 +-
 drivers/gpu/arm/midgard/mali_midg_regmap.h         |   33 +-
 drivers/gpu/arm/midgard/platform/devicetree/Kbuild |   12 +-
 .../devicetree/mali_kbase_config_platform.h        |    9 +-
 drivers/gpu/arm/midgard/platform/juno_soc/Kbuild   |   19 +
 .../arm/midgard/platform/juno_soc/juno_mali_opp.c  |   84 +
 .../platform/juno_soc/mali_kbase_config_juno_soc.c |  156 ++
 .../platform/juno_soc/mali_kbase_config_platform.h |   84 +
 drivers/gpu/arm/midgard/platform/rk/Kbuild         |    4 +-
 drivers/gpu/arm/midgard/platform/vexpress/Kbuild   |    8 +-
 .../platform/vexpress/mali_kbase_config_platform.h |    9 +-
 .../arm/midgard/platform/vexpress_1xv7_a57/Kbuild  |    4 +-
 .../vexpress_1xv7_a57/mali_kbase_config_platform.h |    9 +-
 .../platform/vexpress_6xvirtex7_10mhz/Kbuild       |    8 +-
 .../mali_kbase_config_platform.h                   |    9 +-
 drivers/gpu/arm/midgard/protected_mode_switcher.h  |   64 -
 drivers/gpu/arm/midgard/sconscript                 |   21 +-
 drivers/gpu/arm/midgard/tests/Kbuild               |   17 -
 drivers/gpu/arm/midgard/tests/Kconfig              |   17 -
 .../gpu/arm/midgard/tests/include/kutf/kutf_mem.h  |   65 -
 .../midgard/tests/include/kutf/kutf_resultset.h    |  121 --
 .../arm/midgard/tests/include/kutf/kutf_suite.h    |  508 ------
 .../arm/midgard/tests/include/kutf/kutf_utils.h    |   55 -
 drivers/gpu/arm/midgard/tests/kutf/Kbuild          |   20 -
 drivers/gpu/arm/midgard/tests/kutf/Kconfig         |   22 -
 drivers/gpu/arm/midgard/tests/kutf/Makefile        |   29 -
 drivers/gpu/arm/midgard/tests/kutf/kutf_mem.c      |   94 --
 .../gpu/arm/midgard/tests/kutf/kutf_resultset.c    |   95 --
 drivers/gpu/arm/midgard/tests/kutf/kutf_suite.c    | 1041 ------------
 drivers/gpu/arm/midgard/tests/kutf/kutf_utils.c    |   71 -
 drivers/gpu/arm/midgard/tests/kutf/sconscript      |   21 -
 .../arm/midgard/tests/mali_kutf_irq_test/Kbuild    |   20 -
 .../arm/midgard/tests/mali_kutf_irq_test/Kconfig   |   23 -
 .../arm/midgard/tests/mali_kutf_irq_test/Makefile  |   51 -
 .../mali_kutf_irq_test/mali_kutf_irq_test_main.c   |  257 ---
 .../midgard/tests/mali_kutf_irq_test/sconscript    |   30 -
 drivers/gpu/arm/midgard/tests/sconscript           |   37 -
 134 files changed, 5976 insertions(+), 12218 deletions(-)
 delete mode 100644 Documentation/devicetree/bindings/power/mali-opp.txt
 delete mode 100644 drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.c
 delete mode 100644 drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.h
 create mode 100644 drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.c
 create mode 100644 drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.h
 delete mode 100644 drivers/gpu/arm/midgard/ipa/Kbuild
 delete mode 100644 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.c
 delete mode 100644 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.h
 delete mode 100644 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.c
 delete mode 100644 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.h
 delete mode 100644 drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_simple.c
 create mode 100644 drivers/gpu/arm/midgard/mali_base_kernel_sync.h
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_ctx_sched.c
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_ctx_sched.h
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_fence.c
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_fence.h
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_fence_defs.h
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names_tsix.h
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_ioctl.h
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_ipa.c
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_ipa.h
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_ipa_tables.h
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_js_policy.h
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_sync.c
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_sync_android.c
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_sync_common.c
 delete mode 100644 drivers/gpu/arm/midgard/mali_kbase_sync_file.c
 create mode 100644 drivers/gpu/arm/midgard/mali_kbase_sync_user.c
 create mode 100644 drivers/gpu/arm/midgard/platform/juno_soc/Kbuild
 create mode 100644 drivers/gpu/arm/midgard/platform/juno_soc/juno_mali_opp.c
 create mode 100644 drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_juno_soc.c
 create mode 100644 drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_platform.h
 delete mode 100644 drivers/gpu/arm/midgard/protected_mode_switcher.h
 delete mode 100644 drivers/gpu/arm/midgard/tests/Kbuild
 delete mode 100644 drivers/gpu/arm/midgard/tests/Kconfig
 delete mode 100644 drivers/gpu/arm/midgard/tests/include/kutf/kutf_mem.h
 delete mode 100644 drivers/gpu/arm/midgard/tests/include/kutf/kutf_resultset.h
 delete mode 100644 drivers/gpu/arm/midgard/tests/include/kutf/kutf_suite.h
 delete mode 100644 drivers/gpu/arm/midgard/tests/include/kutf/kutf_utils.h
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/Kbuild
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/Kconfig
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/Makefile
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/kutf_mem.c
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/kutf_resultset.c
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/kutf_suite.c
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/kutf_utils.c
 delete mode 100644 drivers/gpu/arm/midgard/tests/kutf/sconscript
 delete mode 100644 drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kbuild
 delete mode 100644 drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kconfig
 delete mode 100644 drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Makefile
 delete mode 100644 drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
 delete mode 100644 drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/sconscript
 delete mode 100644 drivers/gpu/arm/midgard/tests/sconscript

diff --git a/Documentation/devicetree/bindings/arm/mali-midgard.txt b/Documentation/devicetree/bindings/arm/mali-midgard.txt
index 0722d0a..46b704b 100644
--- a/Documentation/devicetree/bindings/arm/mali-midgard.txt
+++ b/Documentation/devicetree/bindings/arm/mali-midgard.txt
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2013-2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2013-2015 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -33,6 +33,8 @@ Optional:
 Documentation/devicetree/bindings/regulator/regulator.txt for details.
 - operating-points : Refer to Documentation/devicetree/bindings/power/opp.txt
 for details.
+- snoop_enable_smc : SMC function ID to enable CCI snooping on the GPU port(s).
+- snoop_disable_smc : SMC function ID to disable CCI snooping on the GPU port(s).
 - jm_config : For T860/T880. Sets job manager configuration. An array containing:
 	- 1 to override the TIMESTAMP value, 0 otherwise.
 	- 1 to override clock gate, forcing them to be always on, 0 otherwise.
@@ -43,14 +45,12 @@ for details.
 - power_model : Sets power model parameters. Note that this model was designed for the Juno
 	        platform, and may not be suitable for other platforms. A structure containing :
 	- compatible: Should be arm,mali-simple-power-model
-	- dynamic-coefficient: Coefficient, in pW/(Hz V^2), which is multiplied
-	  by v^2*f to calculate the dynamic power consumption.
-	- static-coefficient: Coefficient, in uW/V^3, which is multiplied by
-	  v^3 to calculate the static power consumption.
-	- ts: An array containing coefficients for the temperature scaling
-	  factor. This is used to scale the static power by a factor of
-	  tsf/1000000, where tsf = ts[3]*T^3 + ts[2]*T^2 + ts[1]*T + ts[0],
-	  and T = temperature in degrees.
+	- voltage: Voltage at reference point. Specified in mV.
+	- frequency: Frequency at reference point. Specified in MHz.
+	- dynamic-power: Dynamic power at reference frequency and voltage. Specified in mW.
+	- static-power: Static power at reference frequency. Specified in mW.
+	- ts: An array containing coefficients for the temperature scaling factor.
+	  Used as : tsf = ts[3]*T^3 + ts[2]*T^2 + ts[1]*T + ts[0], where T = temperature
 	- thermal-zone: A string identifying the thermal zone used for the GPU
 - system-coherency : Sets the coherency protocol to be used for coherent
 		     accesses made from the GPU.
@@ -58,12 +58,6 @@ for details.
 	- 0  : ACE-Lite
 	- 1  : ACE
 	- 31 : No coherency
-- ipa-model : Sets the IPA model to be used for power management. GPU probe will fail if the
-	      model is not found in the registered models list. If no model is specified here,
-	      a gpu-id based model is picked if available, otherwise the default model is used.
-	- mali-simple-power-model: Default model used on mali
-- protected-mode-switcher : Phandle to device implemented protected mode switching functionality.
-Refer to Documentation/devicetree/bindings/arm/smc-protected-mode-switcher.txt for one implementation.
 
 Example for a Mali-T602:
 
@@ -88,8 +82,10 @@ gpu@0xfc010000 {
 	>;
 	power_model {
 		compatible = "arm,mali-simple-power-model";
-		static-coefficient = <2427750>;
-		dynamic-coefficient = <4687>;
+		voltage = <800>;
+		frequency = <500>;
+		static-power = <500>;
+		dynamic-power = <1500>;
 		ts = <20000 2000 (-20) 2>;
 		thermal-zone = "gpu";
 	};
diff --git a/Documentation/devicetree/bindings/power/mali-opp.txt b/Documentation/devicetree/bindings/power/mali-opp.txt
deleted file mode 100644
index 22be9ba..0000000
--- a/Documentation/devicetree/bindings/power/mali-opp.txt
+++ /dev/null
@@ -1,163 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-* ARM Mali Midgard OPP
-
-* OPP Table Node
-
-This describes the OPPs belonging to a device. This node can have following
-properties:
-
-Required properties:
-- compatible: Allow OPPs to express their compatibility. It should be:
-  "operating-points-v2", "operating-points-v2-mali".
-
-- OPP nodes: One or more OPP nodes describing voltage-current-frequency
-  combinations. Their name isn't significant but their phandle can be used to
-  reference an OPP.
-
-* OPP Node
-
-This defines voltage-current-frequency combinations along with other related
-properties.
-
-Required properties:
-- opp-hz: Nominal frequency in Hz, expressed as a 64-bit big-endian integer.
-  This should be treated as a relative performance measurement, taking both GPU
-  frequency and core mask into account.
-
-Optional properties:
-- opp-hz-real: Real frequency in Hz, expressed as a 64-bit big-endian integer.
-  If this is not present then the nominal frequency will be used instead.
-
-- opp-core-mask: Shader core mask. If neither this or opp-core-count are present
-  then all shader cores will be used for this OPP.
-
-- opp-core-count: Number of cores to use for this OPP. If this is present then
-  the driver will build a core mask using the available core mask provided by
-  the GPU hardware.
-
-  If neither this nor opp-core-mask are present then all shader cores will be
-  used for this OPP.
-
-  If both this and opp-core-mask are present then opp-core-mask is ignored.
-
-- opp-microvolt: voltage in micro Volts.
-
-  A single regulator's voltage is specified with an array of size one or three.
-  Single entry is for target voltage and three entries are for <target min max>
-  voltages.
-
-  Entries for multiple regulators must be present in the same order as
-  regulators are specified in device's DT node.
-
-- opp-microvolt-<name>: Named opp-microvolt property. This is exactly similar to
-  the above opp-microvolt property, but allows multiple voltage ranges to be
-  provided for the same OPP. At runtime, the platform can pick a <name> and
-  matching opp-microvolt-<name> property will be enabled for all OPPs. If the
-  platform doesn't pick a specific <name> or the <name> doesn't match with any
-  opp-microvolt-<name> properties, then opp-microvolt property shall be used, if
-  present.
-
-- opp-microamp: The maximum current drawn by the device in microamperes
-  considering system specific parameters (such as transients, process, aging,
-  maximum operating temperature range etc.) as necessary. This may be used to
-  set the most efficient regulator operating mode.
-
-  Should only be set if opp-microvolt is set for the OPP.
-
-  Entries for multiple regulators must be present in the same order as
-  regulators are specified in device's DT node. If this property isn't required
-  for few regulators, then this should be marked as zero for them. If it isn't
-  required for any regulator, then this property need not be present.
-
-- opp-microamp-<name>: Named opp-microamp property. Similar to
-  opp-microvolt-<name> property, but for microamp instead.
-
-- clock-latency-ns: Specifies the maximum possible transition latency (in
-  nanoseconds) for switching to this OPP from any other OPP.
-
-- turbo-mode: Marks the OPP to be used only for turbo modes. Turbo mode is
-  available on some platforms, where the device can run over its operating
-  frequency for a short duration of time limited by the device's power, current
-  and thermal limits.
-
-- opp-suspend: Marks the OPP to be used during device suspend. Only one OPP in
-  the table should have this.
-
-- opp-supported-hw: This enables us to select only a subset of OPPs from the
-  larger OPP table, based on what version of the hardware we are running on. We
-  still can't have multiple nodes with the same opp-hz value in OPP table.
-
-  It's an user defined array containing a hierarchy of hardware version numbers,
-  supported by the OPP. For example: a platform with hierarchy of three levels
-  of versions (A, B and C), this field should be like <X Y Z>, where X
-  corresponds to Version hierarchy A, Y corresponds to version hierarchy B and Z
-  corresponds to version hierarchy C.
-
-  Each level of hierarchy is represented by a 32 bit value, and so there can be
-  only 32 different supported version per hierarchy. i.e. 1 bit per version. A
-  value of 0xFFFFFFFF will enable the OPP for all versions for that hierarchy
-  level. And a value of 0x00000000 will disable the OPP completely, and so we
-  never want that to happen.
-
-  If 32 values aren't sufficient for a version hierarchy, than that version
-  hierarchy can be contained in multiple 32 bit values. i.e. <X Y Z1 Z2> in the
-  above example, Z1 & Z2 refer to the version hierarchy Z.
-
-- status: Marks the node enabled/disabled.
-
-Example for a Juno with Mali T624:
-
-gpu_opp_table: opp_table0 {
-	compatible = "operating-points-v2", "operating-points-v2-mali";
-
-	opp@112500000 {
-		opp-hz = /bits/ 64 <112500000>;
-		opp-hz-real = /bits/ 64 <450000000>;
-		opp-microvolt = <820000>;
-		opp-core-mask = /bits/ 64 <0x1>;
-		opp-suspend;
-	};
-	opp@225000000 {
-		opp-hz = /bits/ 64 <225000000>;
-		opp-hz-real = /bits/ 64 <450000000>;
-		opp-microvolt = <820000>;
-		opp-core-count = <2>;
-	};
-	opp@450000000 {
-		opp-hz = /bits/ 64 <450000000>;
-		opp-hz-real = /bits/ 64 <450000000>;
-		opp-microvolt = <820000>;
-		opp-core-mask = /bits/ 64 <0xf>;
-	};
-	opp@487500000 {
-		opp-hz = /bits/ 64 <487500000>;
-		opp-microvolt = <825000>;
-	};
-	opp@525000000 {
-		opp-hz = /bits/ 64 <525000000>;
-		opp-microvolt = <850000>;
-	};
-	opp@562500000 {
-		opp-hz = /bits/ 64 <562500000>;
-		opp-microvolt = <875000>;
-	};
-	opp@600000000 {
-		opp-hz = /bits/ 64 <600000000>;
-		opp-microvolt = <900000>;
-	};
-};
-
diff --git a/drivers/gpu/arm/midgard/Kbuild b/drivers/gpu/arm/midgard/Kbuild
index 6e06db5..34725d6 100644
--- a/drivers/gpu/arm/midgard/Kbuild
+++ b/drivers/gpu/arm/midgard/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2012-2016, 2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012,2014 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -14,7 +14,7 @@
 
 
 # Driver version string which is returned to userspace via an ioctl
-MALI_RELEASE_NAME ?= "r18p0-01rel0"
+MALI_RELEASE_NAME ?= "r14p0-01rel0"
 
 # Paths required for build
 KBASE_PATH = $(src)
@@ -76,7 +76,7 @@ SRC := \
 	mali_kbase_cache_policy.c \
 	mali_kbase_mem.c \
 	mali_kbase_mmu.c \
-	mali_kbase_ctx_sched.c \
+	mali_kbase_ipa.c \
 	mali_kbase_jd.c \
 	mali_kbase_jd_debugfs.c \
 	mali_kbase_jm.c \
@@ -97,6 +97,8 @@ SRC := \
 	mali_kbase_gpu_memory_debugfs.c \
 	mali_kbase_mem_linux.c \
 	mali_kbase_core_linux.c \
+	mali_kbase_sync.c \
+	mali_kbase_sync_user.c \
 	mali_kbase_replay.c \
 	mali_kbase_mem_profile_debugfs.c \
 	mali_kbase_mmu_mode_lpae.c \
@@ -113,9 +115,6 @@ SRC := \
 	mali_kbase_as_fault_debugfs.c \
 	mali_kbase_regs_history_debugfs.c
 
-
-
-
 ifeq ($(MALI_UNIT_TEST),1)
 	SRC += mali_kbase_tlstream_test.c
 endif
@@ -125,6 +124,9 @@ ifeq ($(MALI_CUSTOMER_RELEASE),0)
 endif
 
 
+# Job Scheduler Policy: Completely Fair Scheduler
+SRC += mali_kbase_js_policy_cfs.c
+
 ccflags-y += -I$(KBASE_PATH)
 
 ifeq ($(CONFIG_MALI_PLATFORM_FAKE),y)
@@ -141,6 +143,16 @@ ifeq ($(CONFIG_MALI_PLATFORM_FAKE),y)
 		ccflags-y += -I$(src)/platform/rtsm_ve
 	endif
 
+	ifeq ($(CONFIG_MALI_PLATFORM_JUNO),y)
+		SRC += platform/juno/mali_kbase_config_vexpress.c
+		ccflags-y += -I$(src)/platform/juno
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_JUNO_SOC),y)
+		SRC += platform/juno_soc/mali_kbase_config_juno_soc.c
+		ccflags-y += -I$(src)/platform/juno_soc
+	endif
+
 	ifeq ($(CONFIG_MALI_PLATFORM_VEXPRESS_1XV7_A57),y)
 		SRC += platform/vexpress_1xv7_a57/mali_kbase_config_vexpress.c
 		ccflags-y += -I$(src)/platform/vexpress_1xv7_a57
@@ -151,38 +163,42 @@ ifeq ($(CONFIG_MALI_PLATFORM_FAKE),y)
 		platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.c
 		ccflags-y += -I$(src)/platform/vexpress_6xvirtex7_10mhz
 	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_A7_KIPLING),y)
+		SRC += platform/a7_kipling/mali_kbase_config_a7_kipling.c \
+		platform/a7_kipling/mali_kbase_cpu_a7_kipling.c
+		ccflags-y += -I$(src)/platform/a7_kipling
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_THIRDPARTY),y)
+	# remove begin and end quotes from the Kconfig string type
+	platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+	MALI_PLATFORM_THIRDPARTY_DIR := platform/$(platform_name)
+	ccflags-y += -I$(src)/$(MALI_PLATFORM_THIRDPARTY_DIR)
+	ifeq ($(CONFIG_MALI_MIDGARD),m)
+	include  $(src)/platform/$(platform_name)/Kbuild
+	else ifeq ($(CONFIG_MALI_MIDGARD),y)
+	obj-$(CONFIG_MALI_MIDGARD) += platform/
+	endif
+	endif
 endif # CONFIG_MALI_PLATFORM_FAKE=y
 
+ifeq ($(CONFIG_MALI_PLATFORM_THIRDPARTY),y)
+# remove begin and end quotes from the Kconfig string type
+platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+MALI_PLATFORM_THIRDPARTY_DIR := platform/$(platform_name)
+ccflags-y += -I$(src)/$(MALI_PLATFORM_THIRDPARTY_DIR)
+MALI_PLATFORM_DIR := platform/$(platform_name)
+include  $(src)/platform/$(platform_name)/Kbuild
+endif
+
 # Tell the Linux build system from which .o file to create the kernel module
 obj-$(CONFIG_MALI_MIDGARD) += midgard_kbase.o
 
 # Tell the Linux build system to enable building of our .c files
 midgard_kbase-y := $(SRC:.c=.o)
 
-ifeq ($(CONFIG_MALI_PLATFORM_THIRDPARTY),y)
-  # Kconfig passes in the name with quotes for in-tree builds - remove them.
-  platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
-  MALI_PLATFORM_THIRDPARTY_DIR := platform/$(platform_name)
-  ccflags-y += -I$(src)/$(MALI_PLATFORM_THIRDPARTY_DIR)
-  include $(src)/$(MALI_PLATFORM_THIRDPARTY_DIR)/Kbuild
-endif
-
-ifeq ($(CONFIG_MALI_DEVFREQ),y)
-  ifeq ($(CONFIG_DEVFREQ_THERMAL),y)
-    include $(src)/ipa/Kbuild
-  endif
-endif
-
-midgard_kbase-$(CONFIG_MALI_DMA_FENCE) += \
-	mali_kbase_dma_fence.o \
-	mali_kbase_fence.o
-midgard_kbase-$(CONFIG_SYNC) += \
-	mali_kbase_sync_android.o \
-	mali_kbase_sync_common.o
-midgard_kbase-$(CONFIG_SYNC_FILE) += \
-	mali_kbase_sync_file.o \
-	mali_kbase_sync_common.o \
-	mali_kbase_fence.o
+midgard_kbase-$(CONFIG_MALI_DMA_FENCE) += mali_kbase_dma_fence.o
 
 MALI_BACKEND_PATH ?= backend
 CONFIG_MALI_BACKEND ?= gpu
@@ -198,7 +214,6 @@ endif
 include  $(src)/$(MALI_BACKEND_PATH)/$(CONFIG_MALI_BACKEND_REAL)/Kbuild
 midgard_kbase-y += $(BACKEND:.c=.o)
 
-
 ccflags-y += -I$(src)/$(MALI_BACKEND_PATH)/$(CONFIG_MALI_BACKEND_REAL)
 subdir-ccflags-y += -I$(src)/$(MALI_BACKEND_PATH)/$(CONFIG_MALI_BACKEND_REAL)
 
@@ -212,6 +227,3 @@ midgard_kbase-$(CONFIG_MALI_PLATFORM_DEVICETREE) += \
 	platform/devicetree/mali_kbase_runtime_pm.o \
 	platform/devicetree/mali_kbase_config_devicetree.o
 ccflags-$(CONFIG_MALI_PLATFORM_DEVICETREE) += -I$(src)/platform/devicetree
-
-# For kutf and mali_kutf_irq_latency_test
-obj-$(CONFIG_MALI_KUTF) += tests/
diff --git a/drivers/gpu/arm/midgard/Kconfig b/drivers/gpu/arm/midgard/Kconfig
index 54565bb..201832b 100644
--- a/drivers/gpu/arm/midgard/Kconfig
+++ b/drivers/gpu/arm/midgard/Kconfig
@@ -79,19 +79,6 @@ menuconfig MALI_EXPERT
 	  Enabling this option and modifying the default settings may produce a driver with performance or
 	  other limitations.
 
-config MALI_CORESTACK
-	bool "Support controlling power to the GPU core stack"
-	depends on MALI_MIDGARD && MALI_EXPERT
-	default n
-	help
-	  Enabling this feature on supported GPUs will let the driver powering
-	  on/off the GPU core stack independently without involving the Power
-	  Domain Controller. This should only be enabled on platforms which
-	  integration of the PDC to the Mali GPU is known to be problematic.
-	  This feature is currently only supported on t-Six and t-HEx GPUs.
-
-	  If unsure, say N.
-
 config MALI_PRFCNT_SET_SECONDARY
 	bool "Use secondary set of performance counters"
 	depends on MALI_MIDGARD && MALI_EXPERT
@@ -169,7 +156,7 @@ config MALI_DEBUG
 
 config MALI_FENCE_DEBUG
 	bool "Debug sync fence usage"
-	depends on MALI_MIDGARD && MALI_EXPERT && (SYNC || SYNC_FILE)
+	depends on MALI_MIDGARD && MALI_EXPERT && SYNC
 	default y if MALI_DEBUG
 	help
 	  Select this option to enable additional checking and reporting on the
@@ -235,13 +222,4 @@ config MALI_GPU_MMU_AARCH64
 
 	  If in doubt, say N.
 
-config MALI_PWRSOFT_765
-	bool "PWRSOFT-765 ticket"
-	default n
-	help
-	  PWRSOFT-765 fixes devfreq cooling devices issues. However, they are
-	  not merged in mainline kernel yet. So this define helps to guard those
-	  parts of the code.
-
 source "drivers/gpu/arm/midgard/platform/Kconfig"
-source "drivers/gpu/arm/midgard/tests/Kconfig"
diff --git a/drivers/gpu/arm/midgard/Makefile b/drivers/gpu/arm/midgard/Makefile
index 9aa242c..e1625e6 100644
--- a/drivers/gpu/arm/midgard/Makefile
+++ b/drivers/gpu/arm/midgard/Makefile
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2010-2016, 2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/midgard/backend/gpu/Kbuild b/drivers/gpu/arm/midgard/backend/gpu/Kbuild
index 5f700e9..a39df41 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/Kbuild
+++ b/drivers/gpu/arm/midgard/backend/gpu/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2014,2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2014 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -46,9 +46,7 @@ BACKEND += \
 endif
 
 ifeq ($(CONFIG_MALI_DEVFREQ),y)
-BACKEND += \
-	backend/gpu/mali_kbase_devfreq.c \
-	backend/gpu/mali_kbase_pm_ca_devfreq.c
+BACKEND += backend/gpu/mali_kbase_devfreq.c
 endif
 
 ifeq ($(CONFIG_MALI_NO_MALI),y)
@@ -58,3 +56,7 @@ ifeq ($(CONFIG_MALI_NO_MALI),y)
 	# HW error simulation
 	BACKEND += backend/gpu/mali_kbase_model_error_generator.c
 endif
+
+ifeq ($(CONFIG_DEVFREQ_THERMAL),y)
+	BACKEND += backend/gpu/mali_kbase_power_model_simple.c
+endif
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_devfreq.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_devfreq.c
index 0127d2a..3510a98 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_devfreq.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_devfreq.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,8 +22,10 @@
 #include <mali_kbase_tlstream.h>
 #include <mali_kbase_config_defaults.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
+#ifdef CONFIG_DEVFREQ_THERMAL
+#include <backend/gpu/mali_kbase_power_model_simple.h>
+#endif
 
-#include <linux/of.h>
 #include <linux/clk.h>
 #include <linux/devfreq.h>
 #ifdef CONFIG_DEVFREQ_THERMAL
@@ -43,51 +45,18 @@
 #define dev_pm_opp_get_voltage opp_get_voltage
 #define dev_pm_opp_get_opp_count opp_get_opp_count
 #define dev_pm_opp_find_freq_ceil opp_find_freq_ceil
-#define dev_pm_opp_find_freq_floor opp_find_freq_floor
 #endif /* Linux >= 3.13 */
 
-/**
- * opp_translate - Translate nominal OPP frequency from devicetree into real
- *                 frequency and core mask
- * @kbdev:     Device pointer
- * @freq:      Nominal frequency
- * @core_mask: Pointer to u64 to store core mask to
- *
- * Return: Real target frequency
- *
- * This function will only perform translation if an operating-points-v2-mali
- * table is present in devicetree. If one is not present then it will return an
- * untranslated frequency and all cores enabled.
- */
-static unsigned long opp_translate(struct kbase_device *kbdev,
-		unsigned long freq, u64 *core_mask)
-{
-	int i;
-
-	for (i = 0; i < kbdev->num_opps; i++) {
-		if (kbdev->opp_table[i].opp_freq == freq) {
-			*core_mask = kbdev->opp_table[i].core_mask;
-			return kbdev->opp_table[i].real_freq;
-		}
-	}
-
-	/* Failed to find OPP - return all cores enabled & nominal frequency */
-	*core_mask = kbdev->gpu_props.props.raw_props.shader_present;
-
-	return freq;
-}
 
 static int
 kbase_devfreq_target(struct device *dev, unsigned long *target_freq, u32 flags)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
 	struct dev_pm_opp *opp;
-	unsigned long nominal_freq;
 	unsigned long freq = 0;
 	unsigned long old_freq = kbdev->current_freq;
 	unsigned long voltage;
 	int err;
-	u64 core_mask;
 
 	freq = *target_freq;
 
@@ -101,13 +70,11 @@ kbase_devfreq_target(struct device *dev, unsigned long *target_freq, u32 flags)
 	voltage = dev_pm_opp_get_voltage(opp);
 	rcu_read_unlock();
 
-	nominal_freq = freq;
-
 	/*
 	 * Only update if there is a change of frequency
 	 */
-	if (kbdev->current_nominal_freq == nominal_freq) {
-		*target_freq = nominal_freq;
+	if (old_freq == freq) {
+		*target_freq = freq;
 #ifdef CONFIG_REGULATOR
 		if (kbdev->current_voltage == voltage)
 			return 0;
@@ -116,12 +83,11 @@ kbase_devfreq_target(struct device *dev, unsigned long *target_freq, u32 flags)
 			dev_err(dev, "Failed to set voltage (%d)\n", err);
 			return err;
 		}
-		kbdev->current_voltage = voltage;
-#endif
+#else
 		return 0;
+#endif
 	}
 
-	freq = opp_translate(kbdev, nominal_freq, &core_mask);
 #ifdef CONFIG_REGULATOR
 	if (kbdev->regulator && kbdev->current_voltage != voltage &&
 	    old_freq < freq) {
@@ -154,17 +120,9 @@ kbase_devfreq_target(struct device *dev, unsigned long *target_freq, u32 flags)
 	}
 #endif
 
-	if (kbdev->pm.backend.ca_current_policy->id ==
-			KBASE_PM_CA_POLICY_ID_DEVFREQ)
-		kbase_devfreq_set_core_mask(kbdev, core_mask);
-
-	*target_freq = nominal_freq;
 	kbdev->current_voltage = voltage;
-	kbdev->current_nominal_freq = nominal_freq;
-	kbdev->current_freq = freq;
-	kbdev->current_core_mask = core_mask;
 
-	KBASE_TLSTREAM_AUX_DEVFREQ_TARGET((u64)nominal_freq);
+	kbase_tlstream_aux_devfreq_target((u64)freq);
 
 	kbase_pm_reset_dvfs_utilisation(kbdev);
 
@@ -176,7 +134,7 @@ kbase_devfreq_cur_freq(struct device *dev, unsigned long *freq)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
 
-	*freq = kbdev->current_nominal_freq;
+	*freq = kbdev->current_freq;
 
 	return 0;
 }
@@ -186,13 +144,21 @@ kbase_devfreq_status(struct device *dev, struct devfreq_dev_status *stat)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
 
-	stat->current_frequency = kbdev->current_nominal_freq;
+	stat->current_frequency = kbdev->current_freq;
 
 	kbase_pm_get_dvfs_utilisation(kbdev,
 			&stat->total_time, &stat->busy_time);
 
 	stat->private_data = NULL;
 
+#ifdef CONFIG_DEVFREQ_THERMAL
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 3, 0)
+	if (kbdev->devfreq_cooling)
+		memcpy(&kbdev->devfreq_cooling->last_status, stat,
+				sizeof(*stat));
+#endif
+#endif
+
 	return 0;
 }
 
@@ -201,7 +167,7 @@ static int kbase_devfreq_init_freq_table(struct kbase_device *kbdev,
 {
 	int count;
 	int i = 0;
-	unsigned long freq;
+	unsigned long freq = 0;
 	struct dev_pm_opp *opp;
 
 	rcu_read_lock();
@@ -218,8 +184,8 @@ static int kbase_devfreq_init_freq_table(struct kbase_device *kbdev,
 		return -ENOMEM;
 
 	rcu_read_lock();
-	for (i = 0, freq = ULONG_MAX; i < count; i++, freq--) {
-		opp = dev_pm_opp_find_freq_floor(kbdev->dev, &freq);
+	for (i = 0; i < count; i++, freq++) {
+		opp = dev_pm_opp_find_freq_ceil(kbdev->dev, &freq);
 		if (IS_ERR(opp))
 			break;
 
@@ -250,93 +216,21 @@ static void kbase_devfreq_exit(struct device *dev)
 	kbase_devfreq_term_freq_table(kbdev);
 }
 
-static int kbase_devfreq_init_core_mask_table(struct kbase_device *kbdev)
-{
-	struct device_node *opp_node = of_parse_phandle(kbdev->dev->of_node,
-			"operating-points-v2", 0);
-	struct device_node *node;
-	int i = 0;
-	int count;
-
-	if (!opp_node)
-		return 0;
-	if (!of_device_is_compatible(opp_node, "operating-points-v2-mali"))
-		return 0;
-
-	count = dev_pm_opp_get_opp_count(kbdev->dev);
-	kbdev->opp_table = kmalloc_array(count,
-			sizeof(struct kbase_devfreq_opp), GFP_KERNEL);
-	if (!kbdev->opp_table)
-		return -ENOMEM;
-
-	for_each_available_child_of_node(opp_node, node) {
-		u64 core_mask;
-		u64 opp_freq, real_freq;
-		const void *core_count_p;
-
-		if (of_property_read_u64(node, "opp-hz", &opp_freq)) {
-			dev_warn(kbdev->dev, "OPP is missing required opp-hz property\n");
-			continue;
-		}
-		if (of_property_read_u64(node, "opp-hz-real", &real_freq))
-			real_freq = opp_freq;
-		if (of_property_read_u64(node, "opp-core-mask", &core_mask))
-			core_mask =
-				kbdev->gpu_props.props.raw_props.shader_present;
-		core_count_p = of_get_property(node, "opp-core-count", NULL);
-		if (core_count_p) {
-			u64 remaining_core_mask =
-				kbdev->gpu_props.props.raw_props.shader_present;
-			int core_count = be32_to_cpup(core_count_p);
-
-			core_mask = 0;
-
-			for (; core_count > 0; core_count--) {
-				int core = ffs(remaining_core_mask);
-
-				if (!core) {
-					dev_err(kbdev->dev, "OPP has more cores than GPU\n");
-					return -ENODEV;
-				}
-
-				core_mask |= (1ull << (core-1));
-				remaining_core_mask &= ~(1ull << (core-1));
-			}
-		}
-
-		if (!core_mask) {
-			dev_err(kbdev->dev, "OPP has invalid core mask of 0\n");
-			return -ENODEV;
-		}
-
-		kbdev->opp_table[i].opp_freq = opp_freq;
-		kbdev->opp_table[i].real_freq = real_freq;
-		kbdev->opp_table[i].core_mask = core_mask;
-
-		dev_info(kbdev->dev, "OPP %d : opp_freq=%llu real_freq=%llu core_mask=%llx\n",
-				i, opp_freq, real_freq, core_mask);
-
-		i++;
-	}
-
-	kbdev->num_opps = i;
-
-	return 0;
-}
-
 int kbase_devfreq_init(struct kbase_device *kbdev)
 {
 	struct devfreq_dev_profile *dp;
 	unsigned long opp_rate;
 	int err;
 
-	if (!kbdev->clock) {
-		dev_err(kbdev->dev, "Clock not available for devfreq\n");
+	if (!kbdev->clock)
 		return -ENODEV;
-	}
 
 	kbdev->current_freq = clk_get_rate(kbdev->clock);
-	kbdev->current_nominal_freq = kbdev->current_freq;
+#ifdef CONFIG_REGULATOR
+	if (kbdev->regulator)
+		kbdev->current_voltage =
+			regulator_get_voltage(kbdev->regulator);
+#endif
 
 	dp = &kbdev->devfreq_profile;
 
@@ -351,10 +245,6 @@ int kbase_devfreq_init(struct kbase_device *kbdev)
 	if (kbase_devfreq_init_freq_table(kbdev, dp))
 		return -EFAULT;
 
-	err = kbase_devfreq_init_core_mask_table(kbdev);
-	if (err)
-		return err;
-
 	kbdev->devfreq = devfreq_add_device(kbdev->dev, dp,
 				"simple_ondemand", NULL);
 	if (IS_ERR(kbdev->devfreq)) {
@@ -362,10 +252,6 @@ int kbase_devfreq_init(struct kbase_device *kbdev)
 		return PTR_ERR(kbdev->devfreq);
 	}
 
-	/* devfreq_add_device only copies a few of kbdev->dev's fields, so
-	 * set drvdata explicitly so IPA models can access kbdev. */
-	dev_set_drvdata(&kbdev->devfreq->dev, kbdev);
-
 	err = devfreq_register_opp_notifier(kbdev->dev, kbdev->devfreq);
 	if (err) {
 		dev_err(kbdev->dev,
@@ -379,23 +265,30 @@ int kbase_devfreq_init(struct kbase_device *kbdev)
 	rcu_read_unlock();
 	kbdev->devfreq->last_status.current_frequency = opp_rate;
 #ifdef CONFIG_DEVFREQ_THERMAL
-	err = kbase_ipa_init(kbdev);
-	if (err) {
-		dev_err(kbdev->dev, "IPA initialization failed\n");
-		goto cooling_failed;
-	}
-
-	kbdev->devfreq_cooling = of_devfreq_cooling_register_power(
-			kbdev->dev->of_node,
-			kbdev->devfreq,
-			&kbase_ipa_power_model_ops);
-	if (IS_ERR_OR_NULL(kbdev->devfreq_cooling)) {
-		err = PTR_ERR(kbdev->devfreq_cooling);
+	err = kbase_power_model_simple_init(kbdev);
+	if (err && err != -ENODEV && err != -EPROBE_DEFER) {
 		dev_err(kbdev->dev,
-			"Failed to register cooling device (%d)\n",
+			"Failed to initialize simple power model (%d)\n",
 			err);
 		goto cooling_failed;
 	}
+	if (err == -EPROBE_DEFER)
+		goto cooling_failed;
+	if (err != -ENODEV) {
+		kbdev->devfreq_cooling = of_devfreq_cooling_register_power(
+				kbdev->dev->of_node,
+				kbdev->devfreq,
+				&power_model_simple_ops);
+		if (IS_ERR_OR_NULL(kbdev->devfreq_cooling)) {
+			err = PTR_ERR(kbdev->devfreq_cooling);
+			dev_err(kbdev->dev,
+				"Failed to register cooling device (%d)\n",
+				err);
+			goto cooling_failed;
+		}
+	} else {
+		err = 0;
+	}
 	I("success initing power_model_simple.");
 #endif
 
@@ -423,8 +316,6 @@ void kbase_devfreq_term(struct kbase_device *kbdev)
 #ifdef CONFIG_DEVFREQ_THERMAL
 	if (kbdev->devfreq_cooling)
 		devfreq_cooling_unregister(kbdev->devfreq_cooling);
-
-	kbase_ipa_term(kbdev);
 #endif
 
 	devfreq_unregister_opp_notifier(kbdev->dev, kbdev->devfreq);
@@ -434,6 +325,4 @@ void kbase_devfreq_term(struct kbase_device *kbdev)
 		dev_err(kbdev->dev, "Failed to terminate devfreq (%d)\n", err);
 	else
 		kbdev->devfreq = NULL;
-
-	kfree(kbdev->opp_table);
 }
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_gpuprops_backend.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_gpuprops_backend.c
index b395325..d410cd2 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_gpuprops_backend.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_gpuprops_backend.c
@@ -81,11 +81,6 @@ void kbase_backend_gpuprops_get(struct kbase_device *kbdev,
 				GPU_CONTROL_REG(L2_PRESENT_LO), NULL);
 	regdump->l2_present_hi = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(L2_PRESENT_HI), NULL);
-
-	regdump->stack_present_lo = kbase_reg_read(kbdev,
-				GPU_CONTROL_REG(STACK_PRESENT_LO), NULL);
-	regdump->stack_present_hi = kbase_reg_read(kbdev,
-				GPU_CONTROL_REG(STACK_PRESENT_HI), NULL);
 }
 
 void kbase_backend_gpuprops_get_features(struct kbase_device *kbdev,
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_as.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_as.c
index 92358f2..202dcfa 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_as.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_as.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,7 +22,6 @@
 
 #include <mali_kbase.h>
 #include <mali_kbase_hwaccess_jm.h>
-#include <mali_kbase_ctx_sched.h>
 
 /**
  * assign_and_activate_kctx_addr_space - Assign an AS to a context
@@ -48,20 +47,65 @@ static void assign_and_activate_kctx_addr_space(struct kbase_device *kbdev,
 						struct kbase_as *current_as)
 {
 	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
+	struct kbasep_js_per_as_data *js_per_as_data;
+	int as_nr = current_as->number;
 
 	lockdep_assert_held(&kctx->jctx.sched_info.ctx.jsctx_mutex);
 	lockdep_assert_held(&js_devdata->runpool_mutex);
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
 	/* Attribute handling */
 	kbasep_js_ctx_attr_runpool_retain_ctx(kbdev, kctx);
 
+	/* Assign addr space */
+	kctx->as_nr = as_nr;
+
+	/* If the GPU is currently powered, activate this address space on the
+	 * MMU */
+	if (kbdev->pm.backend.gpu_powered)
+		kbase_mmu_update(kctx);
+	/* If the GPU was not powered then the MMU will be reprogrammed on the
+	 * next pm_context_active() */
+
 	/* Allow it to run jobs */
 	kbasep_js_set_submit_allowed(js_devdata, kctx);
 
+	/* Book-keeping */
+	js_per_as_data->kctx = kctx;
+	js_per_as_data->as_busy_refcount = 0;
+
 	kbase_js_runpool_inc_context_count(kbdev, kctx);
 }
 
+/**
+ * release_addr_space - Release an address space
+ * @kbdev: Kbase device
+ * @kctx_as_nr: Address space of context to release
+ * @kctx: Context being released
+ *
+ * Context: kbasep_js_device_data.runpool_mutex must be held
+ *
+ * Release an address space, making it available for being picked again.
+ */
+static void release_addr_space(struct kbase_device *kbdev, int kctx_as_nr,
+						struct kbase_context *kctx)
+{
+	struct kbasep_js_device_data *js_devdata;
+	u16 as_bit = (1u << kctx_as_nr);
+
+	js_devdata = &kbdev->js_data;
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+
+	/* The address space must not already be free */
+	KBASE_DEBUG_ASSERT(!(js_devdata->as_free & as_bit));
+
+	js_devdata->as_free |= as_bit;
+
+	kbase_js_runpool_dec_context_count(kbdev, kctx);
+}
+
 bool kbase_backend_use_ctx_sched(struct kbase_device *kbdev,
 						struct kbase_context *kctx)
 {
@@ -73,7 +117,10 @@ bool kbase_backend_use_ctx_sched(struct kbase_device *kbdev,
 	}
 
 	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
-		if (kbdev->as_to_kctx[i] == kctx) {
+		struct kbasep_js_per_as_data *js_per_as_data =
+				&kbdev->js_data.runpool_irq.per_as_data[i];
+
+		if (js_per_as_data->kctx == kctx) {
 			/* Context already has ASID - mark as active */
 			return true;
 		}
@@ -86,6 +133,7 @@ bool kbase_backend_use_ctx_sched(struct kbase_device *kbdev,
 void kbase_backend_release_ctx_irq(struct kbase_device *kbdev,
 						struct kbase_context *kctx)
 {
+	struct kbasep_js_per_as_data *js_per_as_data;
 	int as_nr = kctx->as_nr;
 
 	if (as_nr == KBASEP_AS_NR_INVALID) {
@@ -95,15 +143,25 @@ void kbase_backend_release_ctx_irq(struct kbase_device *kbdev,
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	if (atomic_read(&kctx->refcount) != 1) {
+	js_per_as_data = &kbdev->js_data.runpool_irq.per_as_data[kctx->as_nr];
+	if (js_per_as_data->as_busy_refcount != 0) {
 		WARN(1, "Attempting to release active ASID\n");
 		return;
 	}
 
-	kbasep_js_clear_submit_allowed(&kbdev->js_data, kctx);
+	/* Release context from address space */
+	js_per_as_data->kctx = NULL;
 
-	kbase_ctx_sched_release_ctx(kctx);
-	kbase_js_runpool_dec_context_count(kbdev, kctx);
+	kbasep_js_clear_submit_allowed(&kbdev->js_data, kctx);
+	/* If the GPU is currently powered, de-activate this address space on
+	 * the MMU */
+	if (kbdev->pm.backend.gpu_powered)
+		kbase_mmu_disable(kctx);
+	/* If the GPU was not powered then the MMU will be reprogrammed on the
+	 * next pm_context_active() */
+
+	release_addr_space(kbdev, as_nr, kctx);
+	kctx->as_nr = KBASEP_AS_NR_INVALID;
 }
 
 void kbase_backend_release_ctx_noirq(struct kbase_device *kbdev,
@@ -111,8 +169,74 @@ void kbase_backend_release_ctx_noirq(struct kbase_device *kbdev,
 {
 }
 
-int kbase_backend_find_and_release_free_address_space(
-		struct kbase_device *kbdev, struct kbase_context *kctx)
+void kbase_backend_release_free_address_space(struct kbase_device *kbdev,
+								int as_nr)
+{
+	struct kbasep_js_device_data *js_devdata;
+
+	js_devdata = &kbdev->js_data;
+
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+
+	js_devdata->as_free |= (1 << as_nr);
+}
+
+/**
+ * check_is_runpool_full - check whether the runpool is full for a specified
+ * context
+ * @kbdev: Kbase device
+ * @kctx:  Kbase context
+ *
+ * If kctx == NULL, then this makes the least restrictive check on the
+ * runpool. A specific context that is supplied immediately after could fail
+ * the check, even under the same conditions.
+ *
+ * Therefore, once a context is obtained you \b must re-check it with this
+ * function, since the return value could change to false.
+ *
+ * Context:
+ *   In all cases, the caller must hold kbasep_js_device_data.runpool_mutex.
+ *   When kctx != NULL the caller must hold the
+ *   kbasep_js_kctx_info.ctx.jsctx_mutex.
+ *   When kctx == NULL, then the caller need not hold any jsctx_mutex locks (but
+ *   it doesn't do any harm to do so).
+ *
+ * Return: true if the runpool is full
+ */
+static bool check_is_runpool_full(struct kbase_device *kbdev,
+						struct kbase_context *kctx)
+{
+	struct kbasep_js_device_data *js_devdata;
+	bool is_runpool_full;
+
+	js_devdata = &kbdev->js_data;
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+
+	/* Regardless of whether a context is submitting or not, can't have more
+	 * than there are HW address spaces */
+	is_runpool_full = (bool) (js_devdata->nr_all_contexts_running >=
+						kbdev->nr_hw_address_spaces);
+
+	if (kctx && !kbase_ctx_flag(kctx, KCTX_SUBMIT_DISABLED)) {
+		lockdep_assert_held(&kctx->jctx.sched_info.ctx.jsctx_mutex);
+		/* Contexts that submit might use less of the address spaces
+		 * available, due to HW workarounds.  In which case, the runpool
+		 * is also full when the number of submitting contexts exceeds
+		 * the number of submittable address spaces.
+		 *
+		 * Both checks must be made: can have nr_user_address_spaces ==
+		 * nr_hw_address spaces, and at the same time can have
+		 * nr_user_contexts_running < nr_all_contexts_running. */
+		is_runpool_full |= (bool)
+					(js_devdata->nr_user_contexts_running >=
+						kbdev->nr_user_address_spaces);
+	}
+
+	return is_runpool_full;
+}
+
+int kbase_backend_find_free_address_space(struct kbase_device *kbdev,
+						struct kbase_context *kctx)
 {
 	struct kbasep_js_device_data *js_devdata;
 	struct kbasep_js_kctx_info *js_kctx_info;
@@ -125,23 +249,37 @@ int kbase_backend_find_and_release_free_address_space(
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
 	mutex_lock(&js_devdata->runpool_mutex);
 
+	/* First try to find a free address space */
+	if (check_is_runpool_full(kbdev, kctx))
+		i = -1;
+	else
+		i = ffs(js_devdata->as_free) - 1;
+
+	if (i >= 0 && i < kbdev->nr_hw_address_spaces) {
+		js_devdata->as_free &= ~(1 << i);
+
+		mutex_unlock(&js_devdata->runpool_mutex);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+		return i;
+	}
+
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
+	/* No address space currently free, see if we can release one */
 	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
+		struct kbasep_js_per_as_data *js_per_as_data;
 		struct kbasep_js_kctx_info *as_js_kctx_info;
 		struct kbase_context *as_kctx;
 
-		as_kctx = kbdev->as_to_kctx[i];
+		js_per_as_data = &kbdev->js_data.runpool_irq.per_as_data[i];
+		as_kctx = js_per_as_data->kctx;
 		as_js_kctx_info = &as_kctx->jctx.sched_info;
 
 		/* Don't release privileged or active contexts, or contexts with
-		 * jobs running.
-		 * Note that a context will have at least 1 reference (which
-		 * was previously taken by kbasep_js_schedule_ctx()) until
-		 * descheduled.
-		 */
+		 * jobs running */
 		if (as_kctx && !kbase_ctx_flag(as_kctx, KCTX_PRIVILEGED) &&
-			atomic_read(&as_kctx->refcount) == 1) {
+			js_per_as_data->as_busy_refcount == 0) {
 			if (!kbasep_js_runpool_retain_ctx_nolock(kbdev,
 								as_kctx)) {
 				WARN(1, "Failed to retain active context\n");
@@ -176,6 +314,8 @@ int kbase_backend_find_and_release_free_address_space(
 								as_kctx,
 								true);
 
+				js_devdata->as_free &= ~(1 << i);
+
 				mutex_unlock(&js_devdata->runpool_mutex);
 				mutex_unlock(&as_js_kctx_info->ctx.jsctx_mutex);
 
@@ -213,15 +353,16 @@ bool kbase_backend_use_ctx(struct kbase_device *kbdev,
 	js_devdata = &kbdev->js_data;
 	js_kctx_info = &kctx->jctx.sched_info;
 
-	if (kbdev->hwaccess.active_kctx == kctx) {
-		WARN(1, "Context is already scheduled in\n");
+	if (kbdev->hwaccess.active_kctx == kctx ||
+	    kctx->as_nr != KBASEP_AS_NR_INVALID ||
+	    as_nr == KBASEP_AS_NR_INVALID) {
+		WARN(1, "Invalid parameters to use_ctx()\n");
 		return false;
 	}
 
 	new_address_space = &kbdev->as[as_nr];
 
 	lockdep_assert_held(&js_devdata->runpool_mutex);
-	lockdep_assert_held(&kbdev->mmu_hw_mutex);
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
 	assign_and_activate_kctx_addr_space(kbdev, kctx, new_address_space);
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_hw.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_hw.c
index a6fb097..668258b 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_hw.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_hw.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,7 +29,6 @@
 #include <mali_kbase_vinstr.h>
 #include <mali_kbase_hw.h>
 #include <mali_kbase_hwaccess_jm.h>
-#include <mali_kbase_ctx_sched.h>
 #include <backend/gpu/mali_kbase_device_internal.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 #include <backend/gpu/mali_kbase_js_affinity.h>
@@ -99,7 +98,8 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
 		cfg |= JS_CONFIG_END_FLUSH_CLEAN_INVALIDATE;
 #endif /* CONFIG_MALI_COH_GPU */
 
-	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10649))
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10649) ||
+		!kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_T76X_3982))
 		cfg |= JS_CONFIG_START_MMU;
 
 	cfg |= JS_CONFIG_THREAD_PRI(8);
@@ -130,7 +130,8 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
 
 	/* Write an approximate start timestamp.
 	 * It's approximate because there might be a job in the HEAD register.
-	 */
+	 * In such cases, we'll try to make a better approximation in the IRQ
+	 * handler (up to the KBASE_JS_IRQ_THROTTLE_TIME_US). */
 	katom->start_timestamp = ktime_get();
 
 	/* GO ! */
@@ -145,14 +146,14 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
 				GATOR_MAKE_EVENT(GATOR_JOB_SLOT_START, js),
 				kctx, kbase_jd_atom_id(kctx, katom));
 #endif
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_CONFIG(katom, jc_head,
+	kbase_tlstream_tl_attrib_atom_config(katom, jc_head,
 			katom->affinity, cfg);
-	KBASE_TLSTREAM_TL_RET_CTX_LPU(
+	kbase_tlstream_tl_ret_ctx_lpu(
 		kctx,
 		&kbdev->gpu_props.props.raw_props.js_features[
 			katom->slot_nr]);
-	KBASE_TLSTREAM_TL_RET_ATOM_AS(katom, &kbdev->as[kctx->as_nr]);
-	KBASE_TLSTREAM_TL_RET_ATOM_LPU(
+	kbase_tlstream_tl_ret_atom_as(katom, &kbdev->as[kctx->as_nr]);
+	kbase_tlstream_tl_ret_atom_lpu(
 			katom,
 			&kbdev->gpu_props.props.raw_props.js_features[js],
 			"ctx_nr,atom_nr");
@@ -162,8 +163,7 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
 		char js_string[16];
 
 		trace_gpu_sched_switch(
-				kbasep_make_job_slot_string(js, js_string,
-						sizeof(js_string)),
+				kbasep_make_job_slot_string(js, js_string),
 				ktime_to_ns(katom->start_timestamp),
 				(u32)katom->kctx->id, 0, katom->work_id);
 		kbdev->hwaccess.backend.slot_rb[js].last_context = katom->kctx;
@@ -184,9 +184,9 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
  * Update the start_timestamp of the job currently in the HEAD, based on the
  * fact that we got an IRQ for the previous set of completed jobs.
  *
- * The estimate also takes into account the time the job was submitted, to
- * work out the best estimate (which might still result in an over-estimate to
- * the calculated time spent)
+ * The estimate also takes into account the %KBASE_JS_IRQ_THROTTLE_TIME_US and
+ * the time the job was submitted, to work out the best estimate (which might
+ * still result in an over-estimate to the calculated time spent)
  */
 static void kbasep_job_slot_update_head_start_timestamp(
 						struct kbase_device *kbdev,
@@ -195,25 +195,53 @@ static void kbasep_job_slot_update_head_start_timestamp(
 {
 	if (kbase_backend_nr_atoms_on_slot(kbdev, js) > 0) {
 		struct kbase_jd_atom *katom;
+		ktime_t new_timestamp;
 		ktime_t timestamp_diff;
 		/* The atom in the HEAD */
 		katom = kbase_gpu_inspect(kbdev, js, 0);
 
 		KBASE_DEBUG_ASSERT(katom != NULL);
 
-		timestamp_diff = ktime_sub(end_timestamp,
-				katom->start_timestamp);
+		/* Account for any IRQ Throttle time - makes an overestimate of
+		 * the time spent by the job */
+		new_timestamp = ktime_sub_ns(end_timestamp,
+					KBASE_JS_IRQ_THROTTLE_TIME_US * 1000);
+		timestamp_diff = ktime_sub(new_timestamp,
+							katom->start_timestamp);
 		if (ktime_to_ns(timestamp_diff) >= 0) {
 			/* Only update the timestamp if it's a better estimate
 			 * than what's currently stored. This is because our
 			 * estimate that accounts for the throttle time may be
 			 * too much of an overestimate */
-			katom->start_timestamp = end_timestamp;
+			katom->start_timestamp = new_timestamp;
 		}
 	}
 }
 
 /**
+ * kbasep_trace_tl_nret_atom_lpu - Call nret_atom_lpu timeline tracepoint
+ * @kbdev: kbase device
+ * @js: job slot
+ *
+ * Get kbase atom by calling kbase_gpu_inspect for given job slot.
+ * Then use obtained katom and name of slot associated with the given
+ * job slot number in tracepoint call to the instrumentation module
+ * informing that given atom is no longer executed on given lpu (job slot).
+ */
+static void kbasep_trace_tl_nret_atom_lpu(struct kbase_device *kbdev, int js)
+{
+	int i;
+	for (i = 0;
+	     i < kbase_backend_nr_atoms_submitted(kbdev, js);
+	     i++) {
+		struct kbase_jd_atom *katom = kbase_gpu_inspect(kbdev, js, i);
+
+		kbase_tlstream_tl_nret_atom_lpu(katom,
+			&kbdev->gpu_props.props.raw_props.js_features[js]);
+	}
+}
+
+/**
  * kbasep_trace_tl_event_lpu_softstop - Call event_lpu_softstop timeline
  * tracepoint
  * @kbdev: kbase device
@@ -225,7 +253,7 @@ static void kbasep_job_slot_update_head_start_timestamp(
 static void kbasep_trace_tl_event_lpu_softstop(struct kbase_device *kbdev,
 					int js)
 {
-	KBASE_TLSTREAM_TL_EVENT_LPU_SOFTSTOP(
+	kbase_tlstream_tl_event_lpu_softstop(
 		&kbdev->gpu_props.props.raw_props.js_features[js]);
 }
 
@@ -245,6 +273,16 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 	memset(&kbdev->slot_submit_count_irq[0], 0,
 					sizeof(kbdev->slot_submit_count_irq));
 
+	/* write irq throttle register, this will prevent irqs from occurring
+	 * until the given number of gpu clock cycles have passed */
+	{
+		int irq_throttle_cycles =
+				atomic_read(&kbdev->irq_throttle_cycles);
+
+		kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_THROTTLE),
+						irq_throttle_cycles, NULL);
+	}
+
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
 	while (done) {
@@ -283,6 +321,9 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 					kbasep_trace_tl_event_lpu_softstop(
 						kbdev, i);
 
+					kbasep_trace_tl_nret_atom_lpu(
+						kbdev, i);
+
 					/* Soft-stopped job - read the value of
 					 * JS<n>_TAIL so that the job chain can
 					 * be resumed */
@@ -520,7 +561,7 @@ void kbasep_job_slot_soft_or_hard_stop_do_action(struct kbase_device *kbdev,
 		target_katom->atom_flags |= KBASE_KATOM_FLAG_BEEN_SOFT_STOPPPED;
 
 		/* Mark the point where we issue the soft-stop command */
-		KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_ISSUE(target_katom);
+		kbase_tlstream_tl_event_atom_softstop_issue(target_katom);
 
 		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316)) {
 			int i;
@@ -749,7 +790,7 @@ void kbase_job_slot_ctx_priority_check_locked(struct kbase_context *kctx,
 
 		if (katom->sched_priority > priority) {
 			if (!stop_sent)
-				KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY_CHANGE(
+				kbase_tlstream_tl_attrib_atom_priority_change(
 						target_katom);
 
 			kbase_job_slot_softstop(kbdev, js, katom);
@@ -1124,9 +1165,10 @@ static void kbase_debug_dump_registers(struct kbase_device *kbdev)
 	dev_err(kbdev->dev, "  GPU_IRQ_RAWSTAT=0x%08x GPU_STATUS=0x%08x",
 		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT), NULL),
 		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_STATUS), NULL));
-	dev_err(kbdev->dev, "  JOB_IRQ_RAWSTAT=0x%08x JOB_IRQ_JS_STATE=0x%08x",
+	dev_err(kbdev->dev, "  JOB_IRQ_RAWSTAT=0x%08x JOB_IRQ_JS_STATE=0x%08x JOB_IRQ_THROTTLE=0x%08x",
 		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_RAWSTAT), NULL),
-		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_JS_STATE), NULL));
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_JS_STATE), NULL),
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_THROTTLE), NULL));
 	for (i = 0; i < 3; i++) {
 		dev_err(kbdev->dev, "  JS%d_STATUS=0x%08x      JS%d_HEAD_LO=0x%08x",
 			i, kbase_reg_read(kbdev, JOB_SLOT_REG(i, JS_STATUS),
@@ -1147,15 +1189,13 @@ static void kbase_debug_dump_registers(struct kbase_device *kbdev)
 	dev_err(kbdev->dev, "  SHADER_CONFIG=0x%08x   L2_MMU_CONFIG=0x%08x",
 		kbase_reg_read(kbdev, GPU_CONTROL_REG(SHADER_CONFIG), NULL),
 		kbase_reg_read(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG), NULL));
-	dev_err(kbdev->dev, "  TILER_CONFIG=0x%08x    JM_CONFIG=0x%08x",
-		kbase_reg_read(kbdev, GPU_CONTROL_REG(TILER_CONFIG), NULL),
-		kbase_reg_read(kbdev, GPU_CONTROL_REG(JM_CONFIG), NULL));
 }
 
 static void kbasep_reset_timeout_worker(struct work_struct *data)
 {
 	unsigned long flags;
 	struct kbase_device *kbdev;
+	int i;
 	ktime_t end_timestamp = ktime_get();
 	struct kbasep_js_device_data *js_devdata;
 	bool try_schedule = false;
@@ -1193,7 +1233,6 @@ static void kbasep_reset_timeout_worker(struct work_struct *data)
 						KBASE_RESET_GPU_NOT_PENDING);
 		kbase_disjoint_state_down(kbdev);
 		wake_up(&kbdev->hwaccess.backend.reset_wait);
-		kbase_vinstr_resume(kbdev->vinstr_ctx);
 		return;
 	}
 
@@ -1251,24 +1290,32 @@ static void kbasep_reset_timeout_worker(struct work_struct *data)
 	if (!silent)
 		kbase_debug_dump_registers(kbdev);
 
+	/* Reset the GPU */
+	kbase_pm_init_hw(kbdev, 0);
+
 	/* Complete any jobs that were still on the GPU */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbdev->protected_mode = false;
 	kbase_backend_reset(kbdev, &end_timestamp);
 	kbase_pm_metrics_update(kbdev, NULL);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	/* Reset the GPU */
-	kbase_pm_init_hw(kbdev, 0);
-
 	mutex_unlock(&kbdev->pm.lock);
 
 	mutex_lock(&js_devdata->runpool_mutex);
 
 	mutex_lock(&kbdev->mmu_hw_mutex);
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbase_ctx_sched_restore_all_as(kbdev);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	/* Reprogram the GPU's MMU */
+	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+		if (js_devdata->runpool_irq.per_as_data[i].kctx)
+			kbase_mmu_update(
+				js_devdata->runpool_irq.per_as_data[i].kctx);
+		else
+			kbase_mmu_disable_as(kbdev, i);
+
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	}
 	mutex_unlock(&kbdev->mmu_hw_mutex);
 
 	kbase_pm_enable_interrupts(kbdev);
@@ -1305,11 +1352,6 @@ static void kbasep_reset_timeout_worker(struct work_struct *data)
 		kbase_js_sched_all(kbdev);
 	}
 
-	/* Process any pending slot updates */
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbase_backend_slot_update(kbdev);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
 	kbase_pm_context_idle(kbdev);
 
 	/* Release vinstr */
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_internal.h b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_internal.h
index 1f382b3..89b1288 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_internal.h
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_internal.h
@@ -57,10 +57,9 @@ void kbase_job_done_slot(struct kbase_device *kbdev, int s, u32 completion_code,
 					u64 job_tail, ktime_t *end_timestamp);
 
 #ifdef CONFIG_GPU_TRACEPOINTS
-static inline char *kbasep_make_job_slot_string(int js, char *js_string,
-						size_t js_size)
+static inline char *kbasep_make_job_slot_string(int js, char *js_string)
 {
-	snprintf(js_string, js_size, "job_slot_%i", js);
+	sprintf(js_string, "job_slot_%i", js);
 	return js_string;
 }
 #endif
@@ -153,12 +152,4 @@ void kbase_job_slot_halt(struct kbase_device *kbdev);
  */
 void kbase_job_slot_term(struct kbase_device *kbdev);
 
-/**
- * kbase_gpu_cacheclean - Cause a GPU cache clean & flush
- * @kbdev: Device pointer
- *
- * Caller must not be in IRQ context
- */
-void kbase_gpu_cacheclean(struct kbase_device *kbdev);
-
 #endif /* _KBASE_JM_HWACCESS_H_ */
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_rb.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_rb.c
index 1e9b9e5..d7b4d3f 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_rb.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_jm_rb.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -623,8 +623,6 @@ static void kbase_gpu_release_atom(struct kbase_device *kbdev,
 					struct kbase_jd_atom *katom,
 					ktime_t *end_timestamp)
 {
-	struct kbase_context *kctx = katom->kctx;
-
 	switch (katom->gpu_rb_state) {
 	case KBASE_ATOM_GPU_RB_NOT_IN_SLOT_RB:
 		/* Should be impossible */
@@ -642,14 +640,6 @@ static void kbase_gpu_release_atom(struct kbase_device *kbdev,
 			kbase_pm_release_gpu_cycle_counter_nolock(kbdev);
 		/* ***FALLTHROUGH: TRANSITION TO LOWER STATE*** */
 
-		KBASE_TLSTREAM_TL_NRET_ATOM_LPU(katom,
-			&kbdev->gpu_props.props.raw_props.js_features
-				[katom->slot_nr]);
-		KBASE_TLSTREAM_TL_NRET_ATOM_AS(katom, &kbdev->as[kctx->as_nr]);
-		KBASE_TLSTREAM_TL_NRET_CTX_LPU(kctx,
-			&kbdev->gpu_props.props.raw_props.js_features
-				[katom->slot_nr]);
-
 	case KBASE_ATOM_GPU_RB_READY:
 		/* ***FALLTHROUGH: TRANSITION TO LOWER STATE*** */
 
@@ -670,14 +660,9 @@ static void kbase_gpu_release_atom(struct kbase_device *kbdev,
 
 		if (kbase_jd_katom_is_protected(katom) &&
 				(katom->protected_state.enter ==
-				KBASE_ATOM_ENTER_PROTECTED_IDLE_L2)) {
+				KBASE_ATOM_ENTER_PROTECTED_IDLE_L2))
 			kbase_vinstr_resume(kbdev->vinstr_ctx);
 
-			/* Go back to configured model for IPA */
-			kbase_ipa_model_use_configured_locked(kbdev);
-		}
-
-
 		/* ***FALLTHROUGH: TRANSITION TO LOWER STATE*** */
 
 	case KBASE_ATOM_GPU_RB_WAITING_PROTECTED_MODE_PREV:
@@ -691,7 +676,6 @@ static void kbase_gpu_release_atom(struct kbase_device *kbdev,
 	}
 
 	katom->gpu_rb_state = KBASE_ATOM_GPU_RB_WAITING_BLOCKED;
-	katom->protected_state.exit = KBASE_ATOM_EXIT_PROTECTED_CHECK;
 }
 
 static void kbase_gpu_mark_atom_for_return(struct kbase_device *kbdev,
@@ -735,30 +719,6 @@ static inline bool kbase_gpu_rmu_workaround(struct kbase_device *kbdev, int js)
 	return true;
 }
 
-/**
- * other_slots_busy - Determine if any job slots other than @js are currently
- *                    running atoms
- * @kbdev: Device pointer
- * @js:    Job slot
- *
- * Return: true if any slots other than @js are busy, false otherwise
- */
-static inline bool other_slots_busy(struct kbase_device *kbdev, int js)
-{
-	int slot;
-
-	for (slot = 0; slot < kbdev->gpu_props.num_job_slots; slot++) {
-		if (slot == js)
-			continue;
-
-		if (kbase_gpu_nr_atoms_on_slot_min(kbdev, slot,
-				KBASE_ATOM_GPU_RB_SUBMITTED))
-			return true;
-	}
-
-	return false;
-}
-
 static inline bool kbase_gpu_in_protected_mode(struct kbase_device *kbdev)
 {
 	return kbdev->protected_mode;
@@ -783,8 +743,7 @@ static int kbase_gpu_protected_mode_enter(struct kbase_device *kbdev)
 
 	if (kbdev->protected_ops) {
 		/* Switch GPU to protected mode */
-		err = kbdev->protected_ops->protected_mode_enable(
-				kbdev->protected_dev);
+		err = kbdev->protected_ops->protected_mode_enter(kbdev);
 
 		if (err)
 			dev_warn(kbdev->dev, "Failed to enable protected mode: %d\n",
@@ -806,8 +765,6 @@ static int kbase_gpu_protected_mode_reset(struct kbase_device *kbdev)
 	if (!kbdev->protected_ops)
 		return -EINVAL;
 
-	/* The protected mode disable callback will be called as part of reset
-	 */
 	kbase_reset_gpu_silent(kbdev);
 
 	return 0;
@@ -820,7 +777,6 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev,
 
 	switch (katom[idx]->protected_state.enter) {
 	case KBASE_ATOM_ENTER_PROTECTED_CHECK:
-		KBASE_TLSTREAM_AUX_PROTECTED_ENTER_START(kbdev);
 		/* The checks in KBASE_ATOM_GPU_RB_WAITING_PROTECTED_MODE_PREV
 		 * should ensure that we are not already transitiong, and that
 		 * there are no atoms currently on the GPU. */
@@ -843,9 +799,6 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev,
 			return -EAGAIN;
 		}
 
-		/* Use generic model for IPA in protected mode */
-		kbase_ipa_model_use_fallback_locked(kbdev);
-
 		/* Once reaching this point GPU must be
 		 * switched to protected mode or vinstr
 		 * re-enabled. */
@@ -890,7 +843,7 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev,
 		 * the GPU.
 		 */
 		kbdev->protected_mode_transition = false;
-		KBASE_TLSTREAM_AUX_PROTECTED_ENTER_END(kbdev);
+
 		if (err) {
 			/*
 			 * Failed to switch into protected mode, resume
@@ -907,10 +860,6 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev,
 				kbase_gpu_dequeue_atom(kbdev, js, NULL);
 				kbase_jm_return_atom_to_js(kbdev, katom[idx]);
 			}
-
-			/* Go back to configured model for IPA */
-			kbase_ipa_model_use_configured_locked(kbdev);
-
 			return -EINVAL;
 		}
 
@@ -936,7 +885,6 @@ static int kbase_jm_exit_protected_mode(struct kbase_device *kbdev,
 
 	switch (katom[idx]->protected_state.exit) {
 	case KBASE_ATOM_EXIT_PROTECTED_CHECK:
-		KBASE_TLSTREAM_AUX_PROTECTED_LEAVE_START(kbdev);
 		/* The checks in KBASE_ATOM_GPU_RB_WAITING_PROTECTED_MODE_PREV
 		 * should ensure that we are not already transitiong, and that
 		 * there are no atoms currently on the GPU. */
@@ -990,9 +938,6 @@ static int kbase_jm_exit_protected_mode(struct kbase_device *kbdev,
 
 			kbase_vinstr_resume(kbdev->vinstr_ctx);
 
-			/* Use generic model for IPA in protected mode */
-			kbase_ipa_model_use_fallback_locked(kbdev);
-
 			return -EINVAL;
 		}
 
@@ -1002,13 +947,21 @@ static int kbase_jm_exit_protected_mode(struct kbase_device *kbdev,
 		/* ***FALLTHROUGH: TRANSITION TO HIGHER STATE*** */
 
 	case KBASE_ATOM_EXIT_PROTECTED_RESET_WAIT:
-		/* A GPU reset is issued when exiting protected mode. Once the
-		 * reset is done all atoms' state will also be reset. For this
-		 * reason, if the atom is still in this state we can safely
-		 * say that the reset has not completed i.e., we have not
-		 * finished exiting protected mode yet.
-		 */
-		return -EAGAIN;
+		if (kbase_reset_gpu_active(kbdev))
+			return -EAGAIN;
+
+		kbdev->protected_mode_transition = false;
+		kbdev->protected_mode = false;
+
+		/* protected mode sanity checks */
+		KBASE_DEBUG_ASSERT_MSG(
+			kbase_jd_katom_is_protected(katom[idx]) == kbase_gpu_in_protected_mode(kbdev),
+			"Protected mode of atom (%d) doesn't match protected mode of GPU (%d)",
+			kbase_jd_katom_is_protected(katom[idx]), kbase_gpu_in_protected_mode(kbdev));
+		KBASE_DEBUG_ASSERT_MSG(
+			(kbase_jd_katom_is_protected(katom[idx]) && js == 0) ||
+			!kbase_jd_katom_is_protected(katom[idx]),
+			"Protected atom on JS%d not supported", js);
 	}
 
 	return 0;
@@ -1057,12 +1010,6 @@ void kbase_backend_slot_update(struct kbase_device *kbdev)
 						katom[idx])))
 					break;
 
-				if ((idx == 1) && (kbase_jd_katom_is_protected(
-								katom[0]) !=
-						kbase_jd_katom_is_protected(
-								katom[1])))
-					break;
-
 				if (kbdev->protected_mode_transition)
 					break;
 
@@ -1157,33 +1104,13 @@ void kbase_backend_slot_update(struct kbase_device *kbdev)
 
 			case KBASE_ATOM_GPU_RB_READY:
 
-				if (idx == 1) {
-					/* Only submit if head atom or previous
-					 * atom already submitted */
-					if ((katom[0]->gpu_rb_state !=
+				/* Only submit if head atom or previous atom
+				 * already submitted */
+				if (idx == 1 &&
+					(katom[0]->gpu_rb_state !=
 						KBASE_ATOM_GPU_RB_SUBMITTED &&
-						katom[0]->gpu_rb_state !=
+					katom[0]->gpu_rb_state !=
 					KBASE_ATOM_GPU_RB_NOT_IN_SLOT_RB))
-						break;
-
-					/* If intra-slot serialization in use
-					 * then don't submit atom to NEXT slot
-					 */
-					if (kbdev->serialize_jobs &
-						KBASE_SERIALIZE_INTRA_SLOT)
-						break;
-				}
-
-				/* If inter-slot serialization in use then don't
-				 * submit atom if any other slots are in use */
-				if ((kbdev->serialize_jobs &
-						KBASE_SERIALIZE_INTER_SLOT) &&
-						other_slots_busy(kbdev, js))
-					break;
-
-				if ((kbdev->serialize_jobs &
-						KBASE_SERIALIZE_RESET) &&
-						kbase_reset_gpu_active(kbdev))
 					break;
 
 				/* Check if this job needs the cycle counter
@@ -1239,9 +1166,6 @@ void kbase_backend_run_atom(struct kbase_device *kbdev,
 	kbase_backend_slot_update(kbdev);
 }
 
-#define HAS_DEP(katom) (katom->pre_dep || katom->atom_flags & \
-	(KBASE_KATOM_FLAG_X_DEP_BLOCKED | KBASE_KATOM_FLAG_FAIL_BLOCKER))
-
 bool kbase_gpu_irq_evict(struct kbase_device *kbdev, int js)
 {
 	struct kbase_jd_atom *katom;
@@ -1254,7 +1178,6 @@ bool kbase_gpu_irq_evict(struct kbase_device *kbdev, int js)
 
 	if (next_katom && katom->kctx == next_katom->kctx &&
 		next_katom->gpu_rb_state == KBASE_ATOM_GPU_RB_SUBMITTED &&
-		HAS_DEP(next_katom) &&
 		(kbase_reg_read(kbdev, JOB_SLOT_REG(js, JS_HEAD_NEXT_LO), NULL)
 									!= 0 ||
 		kbase_reg_read(kbdev, JOB_SLOT_REG(js, JS_HEAD_NEXT_HI), NULL)
@@ -1262,16 +1185,6 @@ bool kbase_gpu_irq_evict(struct kbase_device *kbdev, int js)
 		kbase_reg_write(kbdev, JOB_SLOT_REG(js, JS_COMMAND_NEXT),
 				JS_COMMAND_NOP, NULL);
 		next_katom->gpu_rb_state = KBASE_ATOM_GPU_RB_READY;
-
-		KBASE_TLSTREAM_TL_NRET_ATOM_LPU(katom,
-				&kbdev->gpu_props.props.raw_props.js_features
-					[katom->slot_nr]);
-		KBASE_TLSTREAM_TL_NRET_ATOM_AS(katom, &kbdev->as
-					[katom->kctx->as_nr]);
-		KBASE_TLSTREAM_TL_NRET_CTX_LPU(katom->kctx,
-				&kbdev->gpu_props.props.raw_props.js_features
-					[katom->slot_nr]);
-
 		return true;
 	}
 
@@ -1288,27 +1201,9 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	/*
-	 * When a hard-stop is followed close after a soft-stop, the completion
-	 * code may be set to STOPPED, even though the job is terminated
-	 */
-	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TMIX_8438)) {
-		if (completion_code == BASE_JD_EVENT_STOPPED &&
-				(katom->atom_flags &
-				KBASE_KATOM_FLAG_BEEN_HARD_STOPPED)) {
-			completion_code = BASE_JD_EVENT_TERMINATED;
-		}
-	}
-
-	if ((kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_6787) || (katom->core_req &
-					BASE_JD_REQ_SKIP_CACHE_END)) &&
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_6787) &&
 			completion_code != BASE_JD_EVENT_DONE &&
 			!(completion_code & BASE_JD_SW_EVENT)) {
-		/* When a job chain fails, on a T60x or when
-		 * BASE_JD_REQ_SKIP_CACHE_END is set, the GPU cache is not
-		 * flushed. To prevent future evictions causing possible memory
-		 * corruption we need to flush the cache manually before any
-		 * affected memory gets reused. */
 		katom->need_cache_flush_cores_retained = katom->affinity;
 		kbase_pm_request_cores(kbdev, false, katom->affinity);
 	} else if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10676)) {
@@ -1329,6 +1224,15 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 
 	katom = kbase_gpu_dequeue_atom(kbdev, js, end_timestamp);
 	kbase_timeline_job_slot_done(kbdev, katom->kctx, katom, js, 0);
+	kbase_tlstream_tl_nret_atom_lpu(
+			katom,
+			&kbdev->gpu_props.props.raw_props.js_features[
+				katom->slot_nr]);
+	kbase_tlstream_tl_nret_atom_as(katom, &kbdev->as[kctx->as_nr]);
+	kbase_tlstream_tl_nret_ctx_lpu(
+			kctx,
+			&kbdev->gpu_props.props.raw_props.js_features[
+				katom->slot_nr]);
 
 	if (completion_code == BASE_JD_EVENT_STOPPED) {
 		struct kbase_jd_atom *next_katom = kbase_gpu_inspect(kbdev, js,
@@ -1340,9 +1244,7 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 		 * registers by kbase_gpu_soft_hard_stop_slot(), to ensure that
 		 * the atoms on this slot are returned in the correct order.
 		 */
-		if (next_katom && katom->kctx == next_katom->kctx &&
-				next_katom->sched_priority ==
-				katom->sched_priority) {
+		if (next_katom && katom->kctx == next_katom->kctx) {
 			kbase_gpu_dequeue_atom(kbdev, js, end_timestamp);
 			kbase_jm_return_atom_to_js(kbdev, next_katom);
 		}
@@ -1368,16 +1270,14 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 						kbase_gpu_inspect(kbdev, i, 1);
 
 			if (katom_idx0 && katom_idx0->kctx == katom->kctx &&
-					HAS_DEP(katom_idx0) &&
-					katom_idx0->gpu_rb_state !=
-					KBASE_ATOM_GPU_RB_SUBMITTED) {
+				katom_idx0->gpu_rb_state !=
+				KBASE_ATOM_GPU_RB_SUBMITTED) {
 				/* Dequeue katom_idx0 from ringbuffer */
 				kbase_gpu_dequeue_atom(kbdev, i, end_timestamp);
 
 				if (katom_idx1 &&
-						katom_idx1->kctx == katom->kctx
-						&& HAS_DEP(katom_idx1) &&
-						katom_idx0->gpu_rb_state !=
+					katom_idx1->kctx == katom->kctx &&
+					katom_idx0->gpu_rb_state !=
 						KBASE_ATOM_GPU_RB_SUBMITTED) {
 					/* Dequeue katom_idx1 from ringbuffer */
 					kbase_gpu_dequeue_atom(kbdev, i,
@@ -1393,9 +1293,8 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 
 			} else if (katom_idx1 &&
 					katom_idx1->kctx == katom->kctx &&
-					HAS_DEP(katom_idx1) &&
 					katom_idx1->gpu_rb_state !=
-					KBASE_ATOM_GPU_RB_SUBMITTED) {
+						KBASE_ATOM_GPU_RB_SUBMITTED) {
 				/* Can not dequeue this atom yet - will be
 				 * dequeued when atom at idx0 completes */
 				katom_idx1->event_code = BASE_JD_EVENT_STOPPED;
@@ -1446,8 +1345,7 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 			char js_string[16];
 
 			trace_gpu_sched_switch(kbasep_make_job_slot_string(js,
-							js_string,
-							sizeof(js_string)),
+								js_string),
 						ktime_to_ns(*end_timestamp),
 						(u32)next_katom->kctx->id, 0,
 						next_katom->work_id);
@@ -1457,8 +1355,7 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 			char js_string[16];
 
 			trace_gpu_sched_switch(kbasep_make_job_slot_string(js,
-							js_string,
-							sizeof(js_string)),
+								js_string),
 						ktime_to_ns(ktime_get()), 0, 0,
 						0);
 			kbdev->hwaccess.backend.slot_rb[js].last_context = 0;
@@ -1466,9 +1363,6 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, int js,
 	}
 #endif
 
-	if (kbdev->serialize_jobs & KBASE_SERIALIZE_RESET)
-		kbase_reset_gpu_silent(kbdev);
-
 	if (completion_code == BASE_JD_EVENT_STOPPED)
 		katom = kbase_jm_return_atom_to_js(kbdev, katom);
 	else
@@ -1495,9 +1389,6 @@ void kbase_backend_reset(struct kbase_device *kbdev, ktime_t *end_timestamp)
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	/* Reset should always take the GPU out of protected mode */
-	WARN_ON(kbase_gpu_in_protected_mode(kbdev));
-
 	for (js = 0; js < kbdev->gpu_props.num_job_slots; js++) {
 		int atom_idx = 0;
 		int idx;
@@ -1509,23 +1400,7 @@ void kbase_backend_reset(struct kbase_device *kbdev, ktime_t *end_timestamp)
 
 			if (!katom)
 				break;
-			if (katom->protected_state.exit ==
-					KBASE_ATOM_EXIT_PROTECTED_RESET_WAIT)
-			{
-				KBASE_TLSTREAM_AUX_PROTECTED_LEAVE_END(kbdev);
-
-				kbase_vinstr_resume(kbdev->vinstr_ctx);
-
-				/* protected mode sanity checks */
-				KBASE_DEBUG_ASSERT_MSG(
-					kbase_jd_katom_is_protected(katom) == kbase_gpu_in_protected_mode(kbdev),
-					"Protected mode of atom (%d) doesn't match protected mode of GPU (%d)",
-					kbase_jd_katom_is_protected(katom), kbase_gpu_in_protected_mode(kbdev));
-				KBASE_DEBUG_ASSERT_MSG(
-					(kbase_jd_katom_is_protected(katom) && js == 0) ||
-					!kbase_jd_katom_is_protected(katom),
-					"Protected atom on JS%d not supported", js);
-			}
+
 			if (katom->gpu_rb_state < KBASE_ATOM_GPU_RB_SUBMITTED)
 				keep_in_jm_rb = true;
 
@@ -1537,7 +1412,6 @@ void kbase_backend_reset(struct kbase_device *kbdev, ktime_t *end_timestamp)
 			 * it will be processed again from the starting state.
 			 */
 			if (keep_in_jm_rb) {
-				kbasep_js_job_check_deref_cores(kbdev, katom);
 				katom->coreref_state = KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED;
 				katom->affinity = 0;
 				katom->protected_state.exit = KBASE_ATOM_EXIT_PROTECTED_CHECK;
@@ -1559,6 +1433,7 @@ void kbase_backend_reset(struct kbase_device *kbdev, ktime_t *end_timestamp)
 	}
 
 	kbdev->protected_mode_transition = false;
+	kbdev->protected_mode = false;
 }
 
 static inline void kbase_gpu_stop_atom(struct kbase_device *kbdev,
@@ -1566,12 +1441,13 @@ static inline void kbase_gpu_stop_atom(struct kbase_device *kbdev,
 					struct kbase_jd_atom *katom,
 					u32 action)
 {
+	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 	u32 hw_action = action & JS_COMMAND_MASK;
 
 	kbase_job_check_enter_disjoint(kbdev, action, katom->core_req, katom);
 	kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, hw_action,
 							katom->core_req, katom);
-	katom->kctx->blocked_js[js][katom->sched_priority] = true;
+	kbasep_js_clear_submit_allowed(js_devdata, katom->kctx);
 }
 
 static inline void kbase_gpu_remove_atom(struct kbase_device *kbdev,
@@ -1579,9 +1455,11 @@ static inline void kbase_gpu_remove_atom(struct kbase_device *kbdev,
 						u32 action,
 						bool disjoint)
 {
+	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
+
 	katom->event_code = BASE_JD_EVENT_REMOVED_FROM_NEXT;
 	kbase_gpu_mark_atom_for_return(kbdev, katom);
-	katom->kctx->blocked_js[katom->slot_nr][katom->sched_priority] = true;
+	kbasep_js_clear_submit_allowed(js_devdata, katom->kctx);
 
 	if (disjoint)
 		kbase_job_check_enter_disjoint(kbdev, action, katom->core_req,
@@ -1614,6 +1492,8 @@ bool kbase_backend_soft_hard_stop_slot(struct kbase_device *kbdev,
 					struct kbase_jd_atom *katom,
 					u32 action)
 {
+	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
+
 	struct kbase_jd_atom *katom_idx0;
 	struct kbase_jd_atom *katom_idx1;
 
@@ -1622,18 +1502,12 @@ bool kbase_backend_soft_hard_stop_slot(struct kbase_device *kbdev,
 	bool ret = false;
 
 	int stop_x_dep_idx0 = -1, stop_x_dep_idx1 = -1;
-	int prio_idx0 = 0, prio_idx1 = 0;
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
 	katom_idx0 = kbase_gpu_inspect(kbdev, js, 0);
 	katom_idx1 = kbase_gpu_inspect(kbdev, js, 1);
 
-	if (katom_idx0)
-		prio_idx0 = katom_idx0->sched_priority;
-	if (katom_idx1)
-		prio_idx1 = katom_idx1->sched_priority;
-
 	if (katom) {
 		katom_idx0_valid = (katom_idx0 == katom);
 		/* If idx0 is to be removed and idx1 is on the same context,
@@ -1648,10 +1522,9 @@ bool kbase_backend_soft_hard_stop_slot(struct kbase_device *kbdev,
 			katom_idx1_valid = false;
 	} else {
 		katom_idx0_valid = (katom_idx0 &&
-				(!kctx || katom_idx0->kctx == kctx));
+					(!kctx || katom_idx0->kctx == kctx));
 		katom_idx1_valid = (katom_idx1 &&
-				(!kctx || katom_idx1->kctx == kctx) &&
-				prio_idx0 == prio_idx1);
+					(!kctx || katom_idx1->kctx == kctx));
 	}
 
 	if (katom_idx0_valid)
@@ -1668,14 +1541,15 @@ bool kbase_backend_soft_hard_stop_slot(struct kbase_device *kbdev,
 				katom_idx1->event_code =
 						BASE_JD_EVENT_REMOVED_FROM_NEXT;
 				kbase_jm_return_atom_to_js(kbdev, katom_idx1);
-				katom_idx1->kctx->blocked_js[js][prio_idx1] =
-						true;
+				kbasep_js_clear_submit_allowed(js_devdata,
+							katom_idx1->kctx);
 			}
 
 			katom_idx0->event_code =
 						BASE_JD_EVENT_REMOVED_FROM_NEXT;
 			kbase_jm_return_atom_to_js(kbdev, katom_idx0);
-			katom_idx0->kctx->blocked_js[js][prio_idx0] = true;
+			kbasep_js_clear_submit_allowed(js_devdata,
+							katom_idx0->kctx);
 		} else {
 			/* katom_idx0 is on GPU */
 			if (katom_idx1 && katom_idx1->gpu_rb_state ==
@@ -1810,11 +1684,13 @@ bool kbase_backend_soft_hard_stop_slot(struct kbase_device *kbdev,
 	return ret;
 }
 
-void kbase_gpu_cacheclean(struct kbase_device *kbdev)
+void kbase_gpu_cacheclean(struct kbase_device *kbdev,
+					struct kbase_jd_atom *katom)
 {
 	/* Limit the number of loops to avoid a hang if the interrupt is missed
 	 */
 	u32 max_loops = KBASE_CLEAN_CACHE_MAX_LOOPS;
+	unsigned long flags;
 
 	mutex_lock(&kbdev->cacheclean_lock);
 
@@ -1840,22 +1716,11 @@ void kbase_gpu_cacheclean(struct kbase_device *kbdev)
 	    "Instrumentation code was cleaning caches, but Job Management code cleared their IRQ - Instrumentation code will now hang.");
 
 	mutex_unlock(&kbdev->cacheclean_lock);
-}
 
-void kbase_backend_cacheclean(struct kbase_device *kbdev,
-		struct kbase_jd_atom *katom)
-{
-	if (katom->need_cache_flush_cores_retained) {
-		unsigned long flags;
-
-		kbase_gpu_cacheclean(kbdev);
-
-		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-		kbase_pm_unrequest_cores(kbdev, false,
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_pm_unrequest_cores(kbdev, false,
 					katom->need_cache_flush_cores_retained);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-		katom->need_cache_flush_cores_retained = 0;
-	}
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 }
 
 void kbase_backend_complete_wq(struct kbase_device *kbdev,
@@ -1865,7 +1730,10 @@ void kbase_backend_complete_wq(struct kbase_device *kbdev,
 	 * If cache flush required due to HW workaround then perform the flush
 	 * now
 	 */
-	kbase_backend_cacheclean(kbdev, katom);
+	if (katom->need_cache_flush_cores_retained) {
+		kbase_gpu_cacheclean(kbdev, katom);
+		katom->need_cache_flush_cores_retained = 0;
+	}
 
 	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10969)            &&
 	    (katom->core_req & BASE_JD_REQ_FS)                        &&
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_js_backend.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_js_backend.c
index a8c1af2..b09d491 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_js_backend.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_js_backend.c
@@ -117,7 +117,7 @@ static enum hrtimer_restart timer_callback(struct hrtimer *timer)
 			/* The current version of the model doesn't support
 			 * Soft-Stop */
 			if (!kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_5736)) {
-				u32 ticks = atom->ticks++;
+				u32 ticks = atom->sched_info.cfs.ticks++;
 
 #if !CINSTR_DUMPING_ENABLED
 				u32 soft_stop_ticks, hard_stop_ticks,
@@ -146,7 +146,8 @@ static enum hrtimer_restart timer_callback(struct hrtimer *timer)
 				 * changing the timeouts. */
 				if (backend->timeouts_updated &&
 						ticks > soft_stop_ticks)
-					ticks = atom->ticks = soft_stop_ticks;
+					ticks = atom->sched_info.cfs.ticks =
+							soft_stop_ticks;
 
 				/* Job is Soft-Stoppable */
 				if (ticks == soft_stop_ticks) {
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_mmu_hw_direct.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_mmu_hw_direct.c
index aa1817c..08eea1c 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_mmu_hw_direct.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_mmu_hw_direct.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -200,15 +200,15 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 				KBASE_MMU_FAULT_TYPE_BUS :
 				KBASE_MMU_FAULT_TYPE_PAGE;
 
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU)) {
-			as->fault_extra_addr = kbase_reg_read(kbdev,
-					MMU_AS_REG(as_no, AS_FAULTEXTRA_HI),
-					kctx);
-			as->fault_extra_addr <<= 32;
-			as->fault_extra_addr |= kbase_reg_read(kbdev,
-					MMU_AS_REG(as_no, AS_FAULTEXTRA_LO),
-					kctx);
-		}
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+		as->fault_extra_addr = kbase_reg_read(kbdev,
+				MMU_AS_REG(as_no, AS_FAULTEXTRA_HI),
+				kctx);
+		as->fault_extra_addr <<= 32;
+		as->fault_extra_addr |= kbase_reg_read(kbdev,
+				MMU_AS_REG(as_no, AS_FAULTEXTRA_LO),
+				kctx);
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
 
 		if (kbase_as_has_bus_fault(as)) {
 			/* Mark bus fault as handled.
@@ -248,33 +248,35 @@ void kbase_mmu_hw_configure(struct kbase_device *kbdev, struct kbase_as *as,
 	struct kbase_mmu_setup *current_setup = &as->current_setup;
 	u32 transcfg = 0;
 
-	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU)) {
-		transcfg = current_setup->transcfg & 0xFFFFFFFFUL;
-
-		/* Set flag AS_TRANSCFG_PTW_MEMATTR_WRITE_BACK */
-		/* Clear PTW_MEMATTR bits */
-		transcfg &= ~AS_TRANSCFG_PTW_MEMATTR_MASK;
-		/* Enable correct PTW_MEMATTR bits */
-		transcfg |= AS_TRANSCFG_PTW_MEMATTR_WRITE_BACK;
-
-		if (kbdev->system_coherency == COHERENCY_ACE) {
-			/* Set flag AS_TRANSCFG_PTW_SH_OS (outer shareable) */
-			/* Clear PTW_SH bits */
-			transcfg = (transcfg & ~AS_TRANSCFG_PTW_SH_MASK);
-			/* Enable correct PTW_SH bits */
-			transcfg = (transcfg | AS_TRANSCFG_PTW_SH_OS);
-		}
-
-		kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_TRANSCFG_LO),
-				transcfg, kctx);
-		kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_TRANSCFG_HI),
-				(current_setup->transcfg >> 32) & 0xFFFFFFFFUL,
-				kctx);
-	} else {
-		if (kbdev->system_coherency == COHERENCY_ACE)
-			current_setup->transtab |= AS_TRANSTAB_LPAE_SHARE_OUTER;
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+	transcfg = current_setup->transcfg & 0xFFFFFFFFUL;
+
+	/* Set flag AS_TRANSCFG_PTW_MEMATTR_WRITE_BACK */
+	/* Clear PTW_MEMATTR bits */
+	transcfg &= ~AS_TRANSCFG_PTW_MEMATTR_MASK;
+	/* Enable correct PTW_MEMATTR bits */
+	transcfg |= AS_TRANSCFG_PTW_MEMATTR_WRITE_BACK;
+
+	if (kbdev->system_coherency == COHERENCY_ACE) {
+		/* Set flag AS_TRANSCFG_PTW_SH_OS (outer shareable) */
+		/* Clear PTW_SH bits */
+		transcfg = (transcfg & ~AS_TRANSCFG_PTW_SH_MASK);
+		/* Enable correct PTW_SH bits */
+		transcfg = (transcfg | AS_TRANSCFG_PTW_SH_OS);
 	}
 
+	kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_TRANSCFG_LO),
+			transcfg, kctx);
+	kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_TRANSCFG_HI),
+			(current_setup->transcfg >> 32) & 0xFFFFFFFFUL, kctx);
+
+#else /* CONFIG_MALI_GPU_MMU_AARCH64 */
+
+	if (kbdev->system_coherency == COHERENCY_ACE)
+		current_setup->transtab |= AS_TRANSTAB_LPAE_SHARE_OUTER;
+
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
+
 	kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_TRANSTAB_LO),
 			current_setup->transtab & 0xFFFFFFFFUL, kctx);
 	kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_TRANSTAB_HI),
@@ -285,7 +287,7 @@ void kbase_mmu_hw_configure(struct kbase_device *kbdev, struct kbase_as *as,
 	kbase_reg_write(kbdev, MMU_AS_REG(as->number, AS_MEMATTR_HI),
 			(current_setup->memattr >> 32) & 0xFFFFFFFFUL, kctx);
 
-	KBASE_TLSTREAM_TL_ATTRIB_AS_CONFIG(as,
+	kbase_tlstream_tl_attrib_as_config(as,
 			current_setup->transtab,
 			current_setup->memattr,
 			transcfg);
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_backend.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_backend.c
index 146fd48..93baf7c 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_backend.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_backend.c
@@ -31,8 +31,6 @@
 #include <mali_kbase_hwaccess_jm.h>
 #include <backend/gpu/mali_kbase_js_internal.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
-#include <backend/gpu/mali_kbase_jm_internal.h>
 
 static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data);
 
@@ -179,7 +177,11 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 	unsigned long flags;
 
-#if !PLATFORM_POWER_DOWN_ONLY
+/* rk_ext: adaption in DDK r14 for solution_1_for_glitch. */
+#define NOT_TO_WAIT_CORES_POWER_TRANSITIONS_BEFORE_POWER_OFF_GPU
+
+#ifdef NOT_TO_WAIT_CORES_POWER_TRANSITIONS_BEFORE_POWER_OFF_GPU
+#else
 	/* Wait for power transitions to complete. We do this with no locks held
 	 * so that we don't deadlock with any pending workqueues */
 	KBASE_TIMELINE_PM_CHECKTRANS(kbdev,
@@ -187,32 +189,15 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 	kbase_pm_check_transitions_sync(kbdev);
 	KBASE_TIMELINE_PM_CHECKTRANS(kbdev,
 				SW_FLOW_PM_CHECKTRANS_PM_DO_POWEROFF_END);
-#endif /* !PLATFORM_POWER_DOWN_ONLY */
+#endif
 
 	mutex_lock(&js_devdata->runpool_mutex);
 	mutex_lock(&kbdev->pm.lock);
 
-#if PLATFORM_POWER_DOWN_ONLY
-	if (kbdev->pm.backend.gpu_powered) {
-		if (kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_L2)) {
-			/* If L2 cache is powered then we must flush it before
-			 * we power off the GPU. Normally this would have been
-			 * handled when the L2 was powered off. */
-			kbase_gpu_cacheclean(kbdev);
-		}
-	}
-#endif /* PLATFORM_POWER_DOWN_ONLY */
-
 	if (!backend->poweron_required) {
-#if !PLATFORM_POWER_DOWN_ONLY
-		unsigned long flags;
-
-		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 		WARN_ON(kbdev->l2_available_bitmap ||
 				kbdev->shader_available_bitmap ||
 				kbdev->tiler_available_bitmap);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-#endif /* !PLATFORM_POWER_DOWN_ONLY */
 
 		/* Consume any change-state events */
 		kbase_timeline_pm_check_handle_event(kbdev,
@@ -247,7 +232,6 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 	if (backend->poweron_required) {
 		backend->poweron_required = false;
 		kbase_pm_update_cores_state_nolock(kbdev);
-		kbase_backend_slot_update(kbdev);
 	}
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
@@ -331,7 +315,7 @@ int kbase_hwaccess_pm_powerup(struct kbase_device *kbdev,
 		return ret;
 	}
 
-	kbasep_pm_init_core_use_bitmaps(kbdev);
+	kbasep_pm_read_present_cores(kbdev);
 
 	kbdev->pm.debug_core_mask_all = kbdev->pm.debug_core_mask[0] =
 			kbdev->pm.debug_core_mask[1] =
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca.c
index 85890f1..e8cd8cb 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2013-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -25,9 +25,6 @@
 
 static const struct kbase_pm_ca_policy *const policy_list[] = {
 	&kbase_pm_ca_fixed_policy_ops,
-#ifdef CONFIG_MALI_DEVFREQ
-	&kbase_pm_ca_devfreq_policy_ops,
-#endif
 #if !MALI_CUSTOMER_RELEASE
 	&kbase_pm_ca_random_policy_ops
 #endif
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.c
deleted file mode 100644
index 66bf660..0000000
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.c
+++ /dev/null
@@ -1,129 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/*
- * A core availability policy implementing core mask selection from devfreq OPPs
- *
- */
-
-#include <mali_kbase.h>
-#include <mali_kbase_pm.h>
-#include <backend/gpu/mali_kbase_pm_internal.h>
-#include <linux/version.h>
-
-void kbase_devfreq_set_core_mask(struct kbase_device *kbdev, u64 core_mask)
-{
-	struct kbasep_pm_ca_policy_devfreq *data =
-				&kbdev->pm.backend.ca_policy_data.devfreq;
-	unsigned long flags;
-
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-
-	data->cores_desired = core_mask;
-
-	/* Disable any cores that are now unwanted */
-	data->cores_enabled &= data->cores_desired;
-
-	kbdev->pm.backend.ca_in_transition = true;
-
-	/* If there are no cores to be powered off then power on desired cores
-	 */
-	if (!(data->cores_used & ~data->cores_desired)) {
-		data->cores_enabled = data->cores_desired;
-		kbdev->pm.backend.ca_in_transition = false;
-	}
-
-	kbase_pm_update_cores_state_nolock(kbdev);
-
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
-	dev_dbg(kbdev->dev, "Devfreq policy : new core mask=%llX %llX\n",
-				data->cores_desired, data->cores_enabled);
-}
-
-static void devfreq_init(struct kbase_device *kbdev)
-{
-	struct kbasep_pm_ca_policy_devfreq *data =
-				&kbdev->pm.backend.ca_policy_data.devfreq;
-
-	if (kbdev->current_core_mask) {
-		data->cores_enabled = kbdev->current_core_mask;
-		data->cores_desired = kbdev->current_core_mask;
-	} else {
-		data->cores_enabled =
-				kbdev->gpu_props.props.raw_props.shader_present;
-		data->cores_desired =
-				kbdev->gpu_props.props.raw_props.shader_present;
-	}
-	data->cores_used = 0;
-	kbdev->pm.backend.ca_in_transition = false;
-}
-
-static void devfreq_term(struct kbase_device *kbdev)
-{
-}
-
-static u64 devfreq_get_core_mask(struct kbase_device *kbdev)
-{
-	return kbdev->pm.backend.ca_policy_data.devfreq.cores_enabled;
-}
-
-static void devfreq_update_core_status(struct kbase_device *kbdev,
-							u64 cores_ready,
-							u64 cores_transitioning)
-{
-	struct kbasep_pm_ca_policy_devfreq *data =
-				&kbdev->pm.backend.ca_policy_data.devfreq;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	data->cores_used = cores_ready | cores_transitioning;
-
-	/* If in desired state then clear transition flag */
-	if (data->cores_enabled == data->cores_desired)
-		kbdev->pm.backend.ca_in_transition = false;
-
-	/* If all undesired cores are now off then power on desired cores.
-	 * The direct comparison against cores_enabled limits potential
-	 * recursion to one level */
-	if (!(data->cores_used & ~data->cores_desired) &&
-				data->cores_enabled != data->cores_desired) {
-		data->cores_enabled = data->cores_desired;
-
-		kbase_pm_update_cores_state_nolock(kbdev);
-
-		kbdev->pm.backend.ca_in_transition = false;
-	}
-}
-
-/*
- * The struct kbase_pm_ca_policy structure for the devfreq core availability
- * policy.
- *
- * This is the static structure that defines the devfreq core availability power
- * policy's callback and name.
- */
-const struct kbase_pm_ca_policy kbase_pm_ca_devfreq_policy_ops = {
-	"devfreq",			/* name */
-	devfreq_init,			/* init */
-	devfreq_term,			/* term */
-	devfreq_get_core_mask,		/* get_core_mask */
-	devfreq_update_core_status,	/* update_core_status */
-	0u,				/* flags */
-	KBASE_PM_CA_POLICY_ID_DEVFREQ,	/* id */
-};
-
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.h b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.h
deleted file mode 100644
index 7ab3cd4..0000000
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_ca_devfreq.h
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/*
- * A core availability policy for use with devfreq, where core masks are
- * associated with OPPs.
- */
-
-#ifndef MALI_KBASE_PM_CA_DEVFREQ_H
-#define MALI_KBASE_PM_CA_DEVFREQ_H
-
-/**
- * struct kbasep_pm_ca_policy_devfreq - Private structure for devfreq ca policy
- *
- * This contains data that is private to the devfreq core availability
- * policy.
- *
- * @cores_desired: Cores that the policy wants to be available
- * @cores_enabled: Cores that the policy is currently returning as available
- * @cores_used: Cores currently powered or transitioning
- */
-struct kbasep_pm_ca_policy_devfreq {
-	u64 cores_desired;
-	u64 cores_enabled;
-	u64 cores_used;
-};
-
-extern const struct kbase_pm_ca_policy kbase_pm_ca_devfreq_policy_ops;
-
-/**
- * kbase_devfreq_set_core_mask - Set core mask for policy to use
- * @kbdev: Device pointer
- * @core_mask: New core mask
- *
- * The new core mask will have immediate effect if the GPU is powered, or will
- * take effect when it is next powered on.
- */
-void kbase_devfreq_set_core_mask(struct kbase_device *kbdev, u64 core_mask);
-
-#endif /* MALI_KBASE_PM_CA_DEVFREQ_H */
-
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_defs.h b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_defs.h
index 352744e..99fb62d 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_defs.h
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_defs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,7 +23,6 @@
 #define _KBASE_PM_HWACCESS_DEFS_H_
 
 #include "mali_kbase_pm_ca_fixed.h"
-#include "mali_kbase_pm_ca_devfreq.h"
 #if !MALI_CUSTOMER_RELEASE
 #include "mali_kbase_pm_ca_random.h"
 #endif
@@ -56,13 +55,11 @@ struct kbase_jd_atom;
  * @KBASE_PM_CORE_L2: The L2 cache
  * @KBASE_PM_CORE_SHADER: Shader cores
  * @KBASE_PM_CORE_TILER: Tiler cores
- * @KBASE_PM_CORE_STACK: Core stacks
  */
 enum kbase_pm_core_type {
 	KBASE_PM_CORE_L2 = L2_PRESENT_LO,
 	KBASE_PM_CORE_SHADER = SHADER_PRESENT_LO,
-	KBASE_PM_CORE_TILER = TILER_PRESENT_LO,
-	KBASE_PM_CORE_STACK = STACK_PRESENT_LO
+	KBASE_PM_CORE_TILER = TILER_PRESENT_LO
 };
 
 /**
@@ -132,7 +129,6 @@ union kbase_pm_policy_data {
 
 union kbase_pm_ca_policy_data {
 	struct kbasep_pm_ca_policy_fixed fixed;
-	struct kbasep_pm_ca_policy_devfreq devfreq;
 #if !MALI_CUSTOMER_RELEASE
 	struct kbasep_pm_ca_policy_random random;
 #endif
@@ -178,8 +174,6 @@ union kbase_pm_ca_policy_data {
  *                           currently in a power-on transition
  * @powering_on_l2_state: A bit mask indicating which l2-caches are currently
  *                        in a power-on transition
- * @powering_on_stack_state: A bit mask indicating which core stacks are
- *                           currently in a power-on transition
  * @gpu_in_desired_state: This flag is set if the GPU is powered as requested
  *                        by the desired_xxx_state variables
  * @gpu_in_desired_state_wait: Wait queue set when @gpu_in_desired_state != 0
@@ -266,9 +260,6 @@ struct kbase_pm_backend_data {
 	u64 desired_tiler_state;
 	u64 powering_on_tiler_state;
 	u64 powering_on_l2_state;
-#ifdef CONFIG_MALI_CORESTACK
-	u64 powering_on_stack_state;
-#endif /* CONFIG_MALI_CORESTACK */
 
 	bool gpu_in_desired_state;
 	wait_queue_head_t gpu_in_desired_state_wait;
@@ -412,18 +403,12 @@ struct kbase_pm_policy {
 
 enum kbase_pm_ca_policy_id {
 	KBASE_PM_CA_POLICY_ID_FIXED = 1,
-	KBASE_PM_CA_POLICY_ID_DEVFREQ,
 	KBASE_PM_CA_POLICY_ID_RANDOM
 };
 
 typedef u32 kbase_pm_ca_policy_flags;
 
 /**
- * Maximum length of a CA policy names
- */
-#define KBASE_PM_CA_MAX_POLICY_NAME_LEN 15
-
-/**
  * struct kbase_pm_ca_policy - Core availability policy structure.
  *
  * Each core availability policy exposes a (static) instance of this structure
@@ -442,7 +427,7 @@ typedef u32 kbase_pm_ca_policy_flags;
  *                      It is used purely for debugging.
  */
 struct kbase_pm_ca_policy {
-	char name[KBASE_PM_CA_MAX_POLICY_NAME_LEN + 1];
+	char *name;
 
 	/**
 	 * Function called when the policy is selected
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_driver.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_driver.c
index 8272793..9271314 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_driver.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_driver.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -35,7 +35,6 @@
 #include <mali_kbase_config_defaults.h>
 #include <mali_kbase_smc.h>
 #include <mali_kbase_hwaccess_jm.h>
-#include <mali_kbase_ctx_sched.h>
 #include <backend/gpu/mali_kbase_cache_policy_backend.h>
 #include <backend/gpu/mali_kbase_device_internal.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
@@ -49,6 +48,9 @@
 #define MOCKABLE(function) function
 #endif				/* MALI_MOCK_TEST */
 
+/* Special value to indicate that the JM_CONFIG reg isn't currently used. */
+#define KBASE_JM_CONFIG_UNUSED (1<<31)
+
 /**
  * enum kbasep_pm_action - Actions that can be performed on a core.
  *
@@ -114,25 +116,6 @@ static u64 kbase_pm_get_state(
 static u32 core_type_to_reg(enum kbase_pm_core_type core_type,
 						enum kbasep_pm_action action)
 {
-#ifdef CONFIG_MALI_CORESTACK
-	if (core_type == KBASE_PM_CORE_STACK) {
-		switch (action) {
-		case ACTION_PRESENT:
-			return STACK_PRESENT_LO;
-		case ACTION_READY:
-			return STACK_READY_LO;
-		case ACTION_PWRON:
-			return STACK_PWRON_LO;
-		case ACTION_PWROFF:
-			return STACK_PWROFF_LO;
-		case ACTION_PWRTRANS:
-			return STACK_PWRTRANS_LO;
-		default:
-			BUG();
-		}
-	}
-#endif /* CONFIG_MALI_CORESTACK */
-
 	return (u32)core_type + (u32)action;
 }
 
@@ -231,7 +214,7 @@ static void kbase_pm_invoke(struct kbase_device *kbdev,
 			state |= cores;
 		else if (action == ACTION_PWROFF)
 			state &= ~cores;
-		KBASE_TLSTREAM_AUX_PM_STATE(core_type, state);
+		kbase_tlstream_aux_pm_state(core_type, state);
 	}
 
 	/* Tracing */
@@ -286,8 +269,8 @@ static void kbase_pm_invoke(struct kbase_device *kbdev,
  *
  * This function gets information (chosen by @action) about a set of cores of
  * a type given by @core_type. It is a static function used by
- * kbase_pm_get_active_cores(), kbase_pm_get_trans_cores() and
- * kbase_pm_get_ready_cores().
+ * kbase_pm_get_present_cores(), kbase_pm_get_active_cores(),
+ * kbase_pm_get_trans_cores() and kbase_pm_get_ready_cores().
  *
  * @kbdev:     The kbase device structure of the device
  * @core_type: The type of core that the should be queried
@@ -312,7 +295,7 @@ static u64 kbase_pm_get_state(struct kbase_device *kbdev,
 	return (((u64) hi) << 32) | ((u64) lo);
 }
 
-void kbasep_pm_init_core_use_bitmaps(struct kbase_device *kbdev)
+void kbasep_pm_read_present_cores(struct kbase_device *kbdev)
 {
 	kbdev->shader_inuse_bitmap = 0;
 	kbdev->shader_needed_bitmap = 0;
@@ -326,6 +309,8 @@ void kbasep_pm_init_core_use_bitmaps(struct kbase_device *kbdev)
 	memset(kbdev->shader_needed_cnt, 0, sizeof(kbdev->shader_needed_cnt));
 }
 
+KBASE_EXPORT_TEST_API(kbasep_pm_read_present_cores);
+
 /**
  * kbase_pm_get_present_cores - Get the cores that are present
  *
@@ -346,15 +331,8 @@ u64 kbase_pm_get_present_cores(struct kbase_device *kbdev,
 		return kbdev->gpu_props.props.raw_props.shader_present;
 	case KBASE_PM_CORE_TILER:
 		return kbdev->gpu_props.props.raw_props.tiler_present;
-#ifdef CONFIG_MALI_CORESTACK
-	case KBASE_PM_CORE_STACK:
-		return kbdev->gpu_props.props.raw_props.stack_present;
-#endif /* CONFIG_MALI_CORESTACK */
-	default:
-		break;
 	}
 	KBASE_DEBUG_ASSERT(0);
-
 	return 0;
 }
 
@@ -481,9 +459,6 @@ static bool kbase_pm_transition_core_type(struct kbase_device *kbdev,
 	 * register reads */
 	trans &= ~ready;
 
-	if (trans) /* Do not progress if any cores are transitioning */
-		return false;
-
 	powering_on_trans = trans & *powering_on;
 	*powering_on = powering_on_trans;
 
@@ -551,9 +526,7 @@ static bool kbase_pm_transition_core_type(struct kbase_device *kbdev,
 
 	/* Perform transitions if any */
 	kbase_pm_invoke(kbdev, type, powerup, ACTION_PWRON);
-#if !PLATFORM_POWER_DOWN_ONLY
 	kbase_pm_invoke(kbdev, type, powerdown, ACTION_PWROFF);
-#endif
 
 	/* Recalculate cores transitioning on, and re-evaluate our state */
 	powering_on_trans |= powerup;
@@ -615,49 +588,20 @@ static u64 get_desired_cache_status(u64 present, u64 cores_powered,
 
 KBASE_EXPORT_TEST_API(get_desired_cache_status);
 
-#ifdef CONFIG_MALI_CORESTACK
-u64 kbase_pm_core_stack_mask(u64 cores)
-{
-	u64 stack_mask = 0;
-	size_t const MAX_CORE_ID = 31;
-	size_t const NUM_CORES_PER_STACK = 4;
-	size_t i;
-
-	for (i = 0; i <= MAX_CORE_ID; ++i) {
-		if (test_bit(i, (unsigned long *)&cores)) {
-			/* Every core which ID >= 16 is filled to stacks 4-7
-			 * instead of 0-3 */
-			size_t const stack_num = (i > 16) ?
-				(i % NUM_CORES_PER_STACK) + 4 :
-				(i % NUM_CORES_PER_STACK);
-			set_bit(stack_num, (unsigned long *)&stack_mask);
-		}
-	}
-
-	return stack_mask;
-}
-#endif /* CONFIG_MALI_CORESTACK */
-
 bool
 MOCKABLE(kbase_pm_check_transitions_nolock) (struct kbase_device *kbdev)
 {
 	bool cores_are_available = false;
 	bool in_desired_state = true;
 	u64 desired_l2_state;
-#ifdef CONFIG_MALI_CORESTACK
-	u64 desired_stack_state;
-	u64 stacks_powered;
-#endif /* CONFIG_MALI_CORESTACK */
 	u64 cores_powered;
 	u64 tilers_powered;
 	u64 tiler_available_bitmap;
-	u64 tiler_transitioning_bitmap;
 	u64 shader_available_bitmap;
 	u64 shader_ready_bitmap;
 	u64 shader_transitioning_bitmap;
 	u64 l2_available_bitmap;
 	u64 prev_l2_available_bitmap;
-	u64 l2_inuse_bitmap;
 
 	KBASE_DEBUG_ASSERT(NULL != kbdev);
 	lockdep_assert_held(&kbdev->hwaccess_lock);
@@ -681,21 +625,11 @@ MOCKABLE(kbase_pm_check_transitions_nolock) (struct kbase_device *kbdev)
 				KBASE_TIMELINE_PM_EVENT_CHANGE_GPU_STATE);
 
 	/* If any cores are already powered then, we must keep the caches on */
-	shader_transitioning_bitmap = kbase_pm_get_trans_cores(kbdev,
-							KBASE_PM_CORE_SHADER);
 	cores_powered = kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_SHADER);
-	cores_powered |= kbdev->pm.backend.desired_shader_state;
 
-#ifdef CONFIG_MALI_CORESTACK
-	/* Work out which core stacks want to be powered */
-	desired_stack_state = kbase_pm_core_stack_mask(cores_powered);
-	stacks_powered = kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_STACK) |
-		desired_stack_state;
-#endif /* CONFIG_MALI_CORESTACK */
+	cores_powered |= kbdev->pm.backend.desired_shader_state;
 
 	/* Work out which tilers want to be powered */
-	tiler_transitioning_bitmap = kbase_pm_get_trans_cores(kbdev,
-							KBASE_PM_CORE_TILER);
 	tilers_powered = kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_TILER);
 	tilers_powered |= kbdev->pm.backend.desired_tiler_state;
 
@@ -708,41 +642,21 @@ MOCKABLE(kbase_pm_check_transitions_nolock) (struct kbase_device *kbdev)
 			kbdev->gpu_props.props.raw_props.l2_present,
 			cores_powered, tilers_powered);
 
-	l2_inuse_bitmap = get_desired_cache_status(
-			kbdev->gpu_props.props.raw_props.l2_present,
-			cores_powered | shader_transitioning_bitmap,
-			tilers_powered | tiler_transitioning_bitmap);
-
-#ifdef CONFIG_MALI_CORESTACK
-	if (stacks_powered)
-		desired_l2_state |= 1;
-#endif /* CONFIG_MALI_CORESTACK */
-
 	/* If any l2 cache is on, then enable l2 #0, for use by job manager */
 	if (0 != desired_l2_state)
 		desired_l2_state |= 1;
 
 	prev_l2_available_bitmap = kbdev->l2_available_bitmap;
 	in_desired_state &= kbase_pm_transition_core_type(kbdev,
-			KBASE_PM_CORE_L2, desired_l2_state, l2_inuse_bitmap,
-			&l2_available_bitmap,
-			&kbdev->pm.backend.powering_on_l2_state);
+				KBASE_PM_CORE_L2, desired_l2_state, 0,
+				&l2_available_bitmap,
+				&kbdev->pm.backend.powering_on_l2_state);
 
 	if (kbdev->l2_available_bitmap != l2_available_bitmap)
 		KBASE_TIMELINE_POWER_L2(kbdev, l2_available_bitmap);
 
 	kbdev->l2_available_bitmap = l2_available_bitmap;
 
-
-#ifdef CONFIG_MALI_CORESTACK
-	if (in_desired_state) {
-		in_desired_state &= kbase_pm_transition_core_type(kbdev,
-				KBASE_PM_CORE_STACK, desired_stack_state, 0,
-				&kbdev->stack_available_bitmap,
-				&kbdev->pm.backend.powering_on_stack_state);
-	}
-#endif /* CONFIG_MALI_CORESTACK */
-
 	if (in_desired_state) {
 		in_desired_state &= kbase_pm_transition_core_type(kbdev,
 				KBASE_PM_CORE_TILER,
@@ -835,33 +749,21 @@ MOCKABLE(kbase_pm_check_transitions_nolock) (struct kbase_device *kbdev)
 		kbase_trace_mali_pm_status(KBASE_PM_CORE_TILER,
 						kbase_pm_get_ready_cores(kbdev,
 							KBASE_PM_CORE_TILER));
-#ifdef CONFIG_MALI_CORESTACK
-		kbase_trace_mali_pm_status(KBASE_PM_CORE_STACK,
-						kbase_pm_get_ready_cores(kbdev,
-							KBASE_PM_CORE_STACK));
-#endif /* CONFIG_MALI_CORESTACK */
 #endif
 
-		KBASE_TLSTREAM_AUX_PM_STATE(
+		kbase_tlstream_aux_pm_state(
 				KBASE_PM_CORE_L2,
 				kbase_pm_get_ready_cores(
 					kbdev, KBASE_PM_CORE_L2));
-		KBASE_TLSTREAM_AUX_PM_STATE(
+		kbase_tlstream_aux_pm_state(
 				KBASE_PM_CORE_SHADER,
 				kbase_pm_get_ready_cores(
 					kbdev, KBASE_PM_CORE_SHADER));
-		KBASE_TLSTREAM_AUX_PM_STATE(
+		kbase_tlstream_aux_pm_state(
 				KBASE_PM_CORE_TILER,
 				kbase_pm_get_ready_cores(
 					kbdev,
 					KBASE_PM_CORE_TILER));
-#ifdef CONFIG_MALI_CORESTACK
-		KBASE_TLSTREAM_AUX_PM_STATE(
-				KBASE_PM_CORE_STACK,
-				kbase_pm_get_ready_cores(
-					kbdev,
-					KBASE_PM_CORE_STACK));
-#endif /* CONFIG_MALI_CORESTACK */
 
 		KBASE_TRACE_ADD(kbdev, PM_DESIRED_REACHED, NULL, NULL,
 				kbdev->pm.backend.gpu_in_desired_state,
@@ -1068,6 +970,7 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 	bool reset_required = is_resume;
 	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 	unsigned long flags;
+	int i;
 
 	KBASE_DEBUG_ASSERT(NULL != kbdev);
 	lockdep_assert_held(&js_devdata->runpool_mutex);
@@ -1109,9 +1012,18 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 	}
 
 	mutex_lock(&kbdev->mmu_hw_mutex);
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbase_ctx_sched_restore_all_as(kbdev);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	/* Reprogram the GPU's MMU */
+	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+		if (js_devdata->runpool_irq.per_as_data[i].kctx)
+			kbase_mmu_update(
+				js_devdata->runpool_irq.per_as_data[i].kctx);
+		else
+			kbase_mmu_disable_as(kbdev, i);
+
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	}
 	mutex_unlock(&kbdev->mmu_hw_mutex);
 
 	/* Lastly, enable the interrupts */
@@ -1256,10 +1168,6 @@ static void kbase_pm_hw_issues_detect(struct kbase_device *kbdev)
 			kbdev->hw_quirks_sc |= SC_LS_ALLOW_ATTR_TYPES;
 	}
 
-	if (!kbdev->hw_quirks_sc)
-		kbdev->hw_quirks_sc = kbase_reg_read(kbdev,
-				GPU_CONTROL_REG(SHADER_CONFIG), NULL);
-
 	kbdev->hw_quirks_tiler = kbase_reg_read(kbdev,
 			GPU_CONTROL_REG(TILER_CONFIG), NULL);
 
@@ -1288,9 +1196,19 @@ static void kbase_pm_hw_issues_detect(struct kbase_device *kbdev)
 		kbdev->hw_quirks_mmu |= L2_MMU_CONFIG_ALLOW_SNOOP_DISPARITY;
 	}
 
-	kbdev->hw_quirks_jm = 0;
 	/* Only for T86x/T88x-based products after r2p0 */
 	if (prod_id >= 0x860 && prod_id <= 0x880 && major >= 2) {
+		/* The JM_CONFIG register is specified as follows in the
+		 T86x/T88x Engineering Specification Supplement:
+		 The values are read from device tree in order.
+		*/
+#define TIMESTAMP_OVERRIDE  1
+#define CLOCK_GATE_OVERRIDE (1<<1)
+#define JOB_THROTTLE_ENABLE (1<<2)
+#define JOB_THROTTLE_LIMIT_SHIFT 3
+
+		/* 6 bits in the register */
+		const u32 jm_max_limit = 0x3F;
 
 		if (of_property_read_u32_array(np,
 					"jm_config",
@@ -1300,59 +1218,33 @@ static void kbase_pm_hw_issues_detect(struct kbase_device *kbdev)
 			jm_values[0] = 0;
 			jm_values[1] = 0;
 			jm_values[2] = 0;
-			jm_values[3] = JM_MAX_JOB_THROTTLE_LIMIT;
+			jm_values[3] = jm_max_limit; /* Max value */
 		}
 
 		/* Limit throttle limit to 6 bits*/
-		if (jm_values[3] > JM_MAX_JOB_THROTTLE_LIMIT) {
+		if (jm_values[3] > jm_max_limit) {
 			dev_dbg(kbdev->dev, "JOB_THROTTLE_LIMIT supplied in device tree is too large. Limiting to MAX (63).");
-			jm_values[3] = JM_MAX_JOB_THROTTLE_LIMIT;
+			jm_values[3] = jm_max_limit;
 		}
 
 		/* Aggregate to one integer. */
-		kbdev->hw_quirks_jm |= (jm_values[0] ?
-				JM_TIMESTAMP_OVERRIDE : 0);
-		kbdev->hw_quirks_jm |= (jm_values[1] ?
-				JM_CLOCK_GATE_OVERRIDE : 0);
-		kbdev->hw_quirks_jm |= (jm_values[2] ?
-				JM_JOB_THROTTLE_ENABLE : 0);
+		kbdev->hw_quirks_jm = (jm_values[0] ? TIMESTAMP_OVERRIDE : 0);
+		kbdev->hw_quirks_jm |= (jm_values[1] ? CLOCK_GATE_OVERRIDE : 0);
+		kbdev->hw_quirks_jm |= (jm_values[2] ? JOB_THROTTLE_ENABLE : 0);
 		kbdev->hw_quirks_jm |= (jm_values[3] <<
-				JM_JOB_THROTTLE_LIMIT_SHIFT);
-
-	} else if (GPU_ID_IS_NEW_FORMAT(prod_id) &&
-			   (GPU_ID2_MODEL_MATCH_VALUE(prod_id) ==
-					   GPU_ID2_PRODUCT_TMIX)) {
-		/* Only for tMIx */
-		u32 coherency_features;
-
-		coherency_features = kbase_reg_read(kbdev,
-				GPU_CONTROL_REG(COHERENCY_FEATURES), NULL);
-
-		/* (COHERENCY_ACE_LITE | COHERENCY_ACE) was incorrectly
-		 * documented for tMIx so force correct value here.
-		 */
-		if (coherency_features ==
-				COHERENCY_FEATURE_BIT(COHERENCY_ACE)) {
-			kbdev->hw_quirks_jm |=
-				(COHERENCY_ACE_LITE | COHERENCY_ACE) <<
-				JM_FORCE_COHERENCY_FEATURES_SHIFT;
-		}
+				JOB_THROTTLE_LIMIT_SHIFT);
+	} else {
+		kbdev->hw_quirks_jm = KBASE_JM_CONFIG_UNUSED;
 	}
 
-	if (!kbdev->hw_quirks_jm)
-		kbdev->hw_quirks_jm = kbase_reg_read(kbdev,
-				GPU_CONTROL_REG(JM_CONFIG), NULL);
 
-#ifdef CONFIG_MALI_CORESTACK
-#define MANUAL_POWER_CONTROL ((u32)(1 << 8))
-	kbdev->hw_quirks_jm |= MANUAL_POWER_CONTROL;
-#endif /* CONFIG_MALI_CORESTACK */
 }
 
 static void kbase_pm_hw_issues_apply(struct kbase_device *kbdev)
 {
-	kbase_reg_write(kbdev, GPU_CONTROL_REG(SHADER_CONFIG),
-			kbdev->hw_quirks_sc, NULL);
+	if (kbdev->hw_quirks_sc)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(SHADER_CONFIG),
+				kbdev->hw_quirks_sc, NULL);
 
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(TILER_CONFIG),
 			kbdev->hw_quirks_tiler, NULL);
@@ -1360,8 +1252,10 @@ static void kbase_pm_hw_issues_apply(struct kbase_device *kbdev)
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG),
 			kbdev->hw_quirks_mmu, NULL);
 
-	kbase_reg_write(kbdev, GPU_CONTROL_REG(JM_CONFIG),
-			kbdev->hw_quirks_jm, NULL);
+
+	if (kbdev->hw_quirks_jm != KBASE_JM_CONFIG_UNUSED)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(JM_CONFIG),
+				kbdev->hw_quirks_jm, NULL);
 
 }
 
@@ -1392,13 +1286,13 @@ void kbase_pm_cache_snoop_disable(struct kbase_device *kbdev)
 	}
 }
 
-static int kbase_pm_do_reset(struct kbase_device *kbdev)
+static int kbase_pm_reset_do_normal(struct kbase_device *kbdev)
 {
 	struct kbasep_reset_timeout_data rtdata;
 
 	KBASE_TRACE_ADD(kbdev, CORE_GPU_SOFT_RESET, NULL, NULL, 0u, 0);
 
-	KBASE_TLSTREAM_JD_GPU_SOFT_RESET(kbdev);
+	kbase_tlstream_jd_gpu_soft_reset(kbdev);
 
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
 						GPU_COMMAND_SOFT_RESET, NULL);
@@ -1472,29 +1366,14 @@ static int kbase_pm_do_reset(struct kbase_device *kbdev)
 	return -EINVAL;
 }
 
-static int kbasep_protected_mode_enable(struct protected_mode_device *pdev)
-{
-	struct kbase_device *kbdev = pdev->data;
-
-	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
-		GPU_COMMAND_SET_PROTECTED_MODE, NULL);
-	return 0;
-}
-
-static int kbasep_protected_mode_disable(struct protected_mode_device *pdev)
+static int kbase_pm_reset_do_protected(struct kbase_device *kbdev)
 {
-	struct kbase_device *kbdev = pdev->data;
-
-	lockdep_assert_held(&kbdev->pm.lock);
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_SOFT_RESET, NULL, NULL, 0u, 0);
+	kbase_tlstream_jd_gpu_soft_reset(kbdev);
 
-	return kbase_pm_do_reset(kbdev);
+	return kbdev->protected_ops->protected_mode_reset(kbdev);
 }
 
-struct protected_mode_ops kbase_native_protected_ops = {
-	.protected_mode_enable = kbasep_protected_mode_enable,
-	.protected_mode_disable = kbasep_protected_mode_disable
-};
-
 int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 {
 	unsigned long irq_flags;
@@ -1538,18 +1417,16 @@ int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
 
 	/* Soft reset the GPU */
-	if (kbdev->protected_mode_support)
-		err = kbdev->protected_ops->protected_mode_disable(
-				kbdev->protected_dev);
+	if (kbdev->protected_mode_support &&
+			kbdev->protected_ops->protected_mode_reset)
+		err = kbase_pm_reset_do_protected(kbdev);
 	else
-		err = kbase_pm_do_reset(kbdev);
+		err = kbase_pm_reset_do_normal(kbdev);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
 	if (kbdev->protected_mode)
 		resume_vinstr = true;
 	kbdev->protected_mode = false;
-	kbase_ipa_model_use_configured_locked(kbdev);
-
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
 
 	if (err)
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_internal.h b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_internal.h
index 6804f45..ad2667a 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_internal.h
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_internal.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -288,12 +288,14 @@ void kbase_pm_update_cores_state(struct kbase_device *kbdev);
 void kbase_pm_cancel_deferred_poweroff(struct kbase_device *kbdev);
 
 /**
- * kbasep_pm_init_core_use_bitmaps - Initialise data tracking the required
- *                                   and used cores.
+ * kbasep_pm_read_present_cores - Read the bitmasks of present cores.
+ *
+ * This information is cached to avoid having to perform register reads whenever
+ * the information is required.
  *
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
  */
-void kbasep_pm_init_core_use_bitmaps(struct kbase_device *kbdev);
+void kbasep_pm_read_present_cores(struct kbase_device *kbdev);
 
 /**
  * kbasep_pm_metrics_init - Initialize the metrics gathering framework.
@@ -486,11 +488,11 @@ void kbase_pm_do_poweron(struct kbase_device *kbdev, bool is_resume);
  */
 void kbase_pm_do_poweroff(struct kbase_device *kbdev, bool is_suspend);
 
-#if defined(CONFIG_MALI_DEVFREQ) || defined(CONFIG_MALI_MIDGARD_DVFS)
+#ifdef CONFIG_PM_DEVFREQ
 void kbase_pm_get_dvfs_utilisation(struct kbase_device *kbdev,
 		unsigned long *total, unsigned long *busy);
 void kbase_pm_reset_dvfs_utilisation(struct kbase_device *kbdev);
-#endif /* defined(CONFIG_MALI_DEVFREQ) || defined(CONFIG_MALI_MIDGARD_DVFS) */
+#endif
 
 #ifdef CONFIG_MALI_MIDGARD_DVFS
 
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_metrics.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_metrics.c
index 024248c..7613e1d 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_metrics.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_metrics.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -155,7 +155,7 @@ static void kbase_pm_get_dvfs_utilisation_calc(struct kbase_device *kbdev,
 	kbdev->pm.backend.metrics.time_period_start = now;
 }
 
-#if defined(CONFIG_MALI_DEVFREQ) || defined(CONFIG_MALI_MIDGARD_DVFS)
+#if defined(CONFIG_PM_DEVFREQ) || defined(CONFIG_MALI_MIDGARD_DVFS)
 /* Caller needs to hold kbdev->pm.backend.metrics.lock before calling this
  * function.
  */
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_policy.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_policy.c
index 075f020..92457e8 100644
--- a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_policy.c
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_pm_policy.c
@@ -35,16 +35,12 @@ static const struct kbase_pm_policy *const policy_list[] = {
 	&kbase_pm_fast_start_policy_ops,
 #endif
 #else				/* CONFIG_MALI_NO_MALI */
-#if !PLATFORM_POWER_DOWN_ONLY
 	&kbase_pm_demand_policy_ops,
-#endif /* !PLATFORM_POWER_DOWN_ONLY */
-	&kbase_pm_coarse_demand_policy_ops,
 	&kbase_pm_always_on_policy_ops,
+	&kbase_pm_coarse_demand_policy_ops,
 #if !MALI_CUSTOMER_RELEASE
-#if !PLATFORM_POWER_DOWN_ONLY
 	&kbase_pm_demand_always_powered_policy_ops,
 	&kbase_pm_fast_start_policy_ops,
-#endif /* !PLATFORM_POWER_DOWN_ONLY */
 #endif
 #endif /* CONFIG_MALI_NO_MALI */
 };
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.c b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.c
new file mode 100644
index 0000000..fee077a
--- /dev/null
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.c
@@ -0,0 +1,171 @@
+/*
+ *
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <linux/devfreq_cooling.h>
+#include <linux/thermal.h>
+#include <linux/of.h>
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <backend/gpu/mali_kbase_power_model_simple.h>
+
+/*
+ * This model is primarily designed for the Juno platform. It may not be
+ * suitable for other platforms.
+ */
+
+#define FALLBACK_STATIC_TEMPERATURE 55000
+
+static u32 dynamic_coefficient;
+static u32 static_coefficient;
+static s32 ts[4];
+static struct thermal_zone_device *gpu_tz;
+
+static unsigned long model_static_power(struct devfreq *devfreq,
+					unsigned long voltage)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 3, 0)
+	unsigned long temperature;
+#else
+	int temperature;
+#endif
+	unsigned long temp;
+	unsigned long temp_squared, temp_cubed, temp_scaling_factor;
+	const unsigned long voltage_cubed = (voltage * voltage * voltage) >> 10;
+
+	if (!IS_ERR_OR_NULL(gpu_tz) && gpu_tz->ops->get_temp) {
+		int ret;
+
+		ret = gpu_tz->ops->get_temp(gpu_tz, &temperature);
+		if (ret) {
+			pr_warn_ratelimited("Error reading temperature for gpu thermal zone: %d\n",
+					ret);
+			temperature = FALLBACK_STATIC_TEMPERATURE;
+		}
+	} else {
+		temperature = FALLBACK_STATIC_TEMPERATURE;
+	}
+
+	/* Calculate the temperature scaling factor. To be applied to the
+	 * voltage scaled power.
+	 */
+	temp = temperature / 1000;
+	temp_squared = temp * temp;
+	temp_cubed = temp_squared * temp;
+	temp_scaling_factor =
+			(ts[3] * temp_cubed)
+			+ (ts[2] * temp_squared)
+			+ (ts[1] * temp)
+			+ ts[0];
+
+	return (((static_coefficient * voltage_cubed) >> 20)
+			* temp_scaling_factor)
+				/ 1000000;
+}
+
+static unsigned long model_dynamic_power(struct devfreq *devfreq,
+		unsigned long freq,
+		unsigned long voltage)
+{
+	/* The inputs: freq (f) is in Hz, and voltage (v) in mV.
+	 * The coefficient (c) is in mW/(MHz mV mV).
+	 *
+	 * This function calculates the dynamic power after this formula:
+	 * Pdyn (mW) = c (mW/(MHz*mV*mV)) * v (mV) * v (mV) * f (MHz)
+	 */
+	const unsigned long v2 = (voltage * voltage) / 1000; /* m*(V*V) */
+	const unsigned long f_mhz = freq / 1000000; /* MHz */
+
+	return (dynamic_coefficient * v2 * f_mhz) / 1000000; /* mW */
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0)
+struct devfreq_cooling_ops power_model_simple_ops = {
+#else
+struct devfreq_cooling_power power_model_simple_ops = {
+#endif
+	.get_static_power = model_static_power,
+	.get_dynamic_power = model_dynamic_power,
+};
+
+int kbase_power_model_simple_init(struct kbase_device *kbdev)
+{
+	struct device_node *power_model_node;
+	const char *tz_name;
+	u32 static_power, dynamic_power;
+	u32 voltage, voltage_squared, voltage_cubed, frequency;
+
+	power_model_node = of_get_child_by_name(kbdev->dev->of_node,
+			"power_model");
+	if (!power_model_node) {
+		dev_err(kbdev->dev, "could not find power_model node\n");
+		return -ENODEV;
+	}
+	if (!of_device_is_compatible(power_model_node,
+			"arm,mali-simple-power-model")) {
+		dev_err(kbdev->dev, "power_model incompatible with simple power model\n");
+		return -ENODEV;
+	}
+
+	if (of_property_read_string(power_model_node, "thermal-zone",
+			&tz_name)) {
+		dev_err(kbdev->dev, "ts in power_model not available\n");
+		return -EINVAL;
+	}
+
+	gpu_tz = thermal_zone_get_zone_by_name(tz_name);
+	if (IS_ERR(gpu_tz)) {
+		pr_warn_ratelimited("Error getting gpu thermal zone (%ld), not yet ready?\n",
+				PTR_ERR(gpu_tz));
+		gpu_tz = NULL;
+
+		return -EPROBE_DEFER;
+	}
+
+	if (of_property_read_u32(power_model_node, "static-power",
+			&static_power)) {
+		dev_err(kbdev->dev, "static-power in power_model not available\n");
+		return -EINVAL;
+	}
+	if (of_property_read_u32(power_model_node, "dynamic-power",
+			&dynamic_power)) {
+		dev_err(kbdev->dev, "dynamic-power in power_model not available\n");
+		return -EINVAL;
+	}
+	if (of_property_read_u32(power_model_node, "voltage",
+			&voltage)) {
+		dev_err(kbdev->dev, "voltage in power_model not available\n");
+		return -EINVAL;
+	}
+	if (of_property_read_u32(power_model_node, "frequency",
+			&frequency)) {
+		dev_err(kbdev->dev, "frequency in power_model not available\n");
+		return -EINVAL;
+	}
+	voltage_squared = (voltage * voltage) / 1000;
+	voltage_cubed = voltage * voltage * voltage;
+	static_coefficient = (static_power << 20) / (voltage_cubed >> 10);
+	dynamic_coefficient = (((dynamic_power * 1000) / voltage_squared)
+			* 1000) / frequency;
+
+	if (of_property_read_u32_array(power_model_node, "ts", (u32 *)ts, 4)) {
+		dev_err(kbdev->dev, "ts in power_model not available\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
diff --git a/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.h b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.h
new file mode 100644
index 0000000..9b5e69a
--- /dev/null
+++ b/drivers/gpu/arm/midgard/backend/gpu/mali_kbase_power_model_simple.h
@@ -0,0 +1,47 @@
+/*
+ *
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#ifndef _BASE_POWER_MODEL_SIMPLE_H_
+#define _BASE_POWER_MODEL_SIMPLE_H_
+
+/**
+ * kbase_power_model_simple_init - Initialise the simple power model
+ * @kbdev: Device pointer
+ *
+ * The simple power model estimates power based on current voltage, temperature,
+ * and coefficients read from device tree. It does not take utilization into
+ * account.
+ *
+ * The power model requires coefficients from the power_model node in device
+ * tree. The absence of this node will prevent the model from functioning, but
+ * should not prevent the rest of the driver from running.
+ *
+ * Return: 0 on success
+ *         -ENOSYS if the power_model node is not present in device tree
+ *         -EPROBE_DEFER if the thermal zone specified in device tree is not
+ *         currently available
+ *         Any other negative value on failure
+ */
+int kbase_power_model_simple_init(struct kbase_device *kbdev);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0)
+extern struct devfreq_cooling_ops power_model_simple_ops;
+#else
+extern struct devfreq_cooling_power power_model_simple_ops;
+#endif
+
+#endif /* _BASE_POWER_MODEL_SIMPLE_H_ */
diff --git a/drivers/gpu/arm/midgard/ipa/Kbuild b/drivers/gpu/arm/midgard/ipa/Kbuild
deleted file mode 100644
index 602b15f..0000000
--- a/drivers/gpu/arm/midgard/ipa/Kbuild
+++ /dev/null
@@ -1,24 +0,0 @@
-#
-# (C) COPYRIGHT 2016-2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-midgard_kbase-y += \
-	ipa/mali_kbase_ipa_simple.o \
-	ipa/mali_kbase_ipa.o
-
-midgard_kbase-$(CONFIG_DEBUG_FS) += ipa/mali_kbase_ipa_debugfs.o
-
-ifneq ($(wildcard $(src)/ipa/mali_kbase_ipa_tmix.c),)
-  midgard_kbase-y += ipa/mali_kbase_ipa_tmix.o
-endif
diff --git a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.c b/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.c
deleted file mode 100644
index 01bdbb4..0000000
--- a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.c
+++ /dev/null
@@ -1,585 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2016-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-#include <linux/thermal.h>
-#include <linux/devfreq_cooling.h>
-#include <linux/of.h>
-#include "mali_kbase.h"
-#include "mali_kbase_ipa.h"
-#include "mali_kbase_ipa_debugfs.h"
-
-#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0))
-#include <linux/pm_opp.h>
-#else
-#include <linux/opp.h>
-#define dev_pm_opp_find_freq_exact opp_find_freq_exact
-#define dev_pm_opp_get_voltage opp_get_voltage
-#define dev_pm_opp opp
-#endif
-#include <linux/math64.h>
-
-#define KBASE_IPA_FALLBACK_MODEL_NAME "mali-simple-power-model"
-
-static struct kbase_ipa_model_ops *kbase_ipa_all_model_ops[] = {
-	&kbase_simple_ipa_model_ops,
-};
-
-int kbase_ipa_model_recalculate(struct kbase_ipa_model *model)
-{
-	int err = 0;
-
-	lockdep_assert_held(&model->kbdev->ipa.lock);
-
-	if (model->ops->recalculate) {
-		err = model->ops->recalculate(model);
-		if (err) {
-			dev_err(model->kbdev->dev,
-				"recalculation of power model %s returned error %d\n",
-				model->ops->name, err);
-		}
-	}
-
-	return err;
-}
-
-static struct kbase_ipa_model_ops *kbase_ipa_model_ops_find(struct kbase_device *kbdev,
-							    const char *name)
-{
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(kbase_ipa_all_model_ops); ++i) {
-		struct kbase_ipa_model_ops *ops = kbase_ipa_all_model_ops[i];
-
-		if (!strcmp(ops->name, name))
-			return ops;
-	}
-
-	dev_err(kbdev->dev, "power model \'%s\' not found\n", name);
-
-	return NULL;
-}
-
-void kbase_ipa_model_use_fallback_locked(struct kbase_device *kbdev)
-{
-	atomic_set(&kbdev->ipa_use_configured_model, false);
-}
-
-void kbase_ipa_model_use_configured_locked(struct kbase_device *kbdev)
-{
-	atomic_set(&kbdev->ipa_use_configured_model, true);
-}
-
-const char *kbase_ipa_model_name_from_id(u32 gpu_id)
-{
-	const u32 prod_id = (gpu_id & GPU_ID_VERSION_PRODUCT_ID) >>
-			GPU_ID_VERSION_PRODUCT_ID_SHIFT;
-
-	if (GPU_ID_IS_NEW_FORMAT(prod_id)) {
-		switch (GPU_ID2_MODEL_MATCH_VALUE(prod_id)) {
-		case GPU_ID2_PRODUCT_TMIX:
-			return KBASE_IPA_FALLBACK_MODEL_NAME;
-		default:
-			return KBASE_IPA_FALLBACK_MODEL_NAME;
-		}
-	}
-
-	return KBASE_IPA_FALLBACK_MODEL_NAME;
-}
-
-static struct device_node *get_model_dt_node(struct kbase_ipa_model *model)
-{
-	struct device_node *model_dt_node;
-	char compat_string[64];
-
-	snprintf(compat_string, sizeof(compat_string), "arm,%s",
-		 model->ops->name);
-
-	model_dt_node = of_find_compatible_node(model->kbdev->dev->of_node,
-						NULL, compat_string);
-	if (!model_dt_node && !model->missing_dt_node_warning) {
-		dev_warn(model->kbdev->dev,
-			 "Couldn't find power_model DT node matching \'%s\'\n",
-			 compat_string);
-		model->missing_dt_node_warning = true;
-	}
-
-	return model_dt_node;
-}
-
-int kbase_ipa_model_add_param_s32(struct kbase_ipa_model *model,
-				  const char *name, s32 *addr,
-				  size_t num_elems, bool dt_required)
-{
-	int err, i;
-	struct device_node *model_dt_node = get_model_dt_node(model);
-	char *origin;
-
-	err = of_property_read_u32_array(model_dt_node, name, addr, num_elems);
-
-	if (err && dt_required) {
-		memset(addr, 0, sizeof(s32) * num_elems);
-		dev_warn(model->kbdev->dev,
-			 "Error %d, no DT entry: %s.%s = %zu*[0]\n",
-			 err, model->ops->name, name, num_elems);
-		origin = "zero";
-	} else if (err && !dt_required) {
-		origin = "default";
-	} else /* !err */ {
-		origin = "DT";
-	}
-
-	/* Create a unique debugfs entry for each element */
-	for (i = 0; i < num_elems; ++i) {
-		char elem_name[32];
-
-		if (num_elems == 1)
-			snprintf(elem_name, sizeof(elem_name), "%s", name);
-		else
-			snprintf(elem_name, sizeof(elem_name), "%s.%d",
-				name, i);
-
-		dev_dbg(model->kbdev->dev, "%s.%s = %d (%s)\n",
-			model->ops->name, elem_name, addr[i], origin);
-
-		err = kbase_ipa_model_param_add(model, elem_name,
-						&addr[i], sizeof(s32),
-						PARAM_TYPE_S32);
-		if (err)
-			goto exit;
-	}
-exit:
-	return err;
-}
-
-int kbase_ipa_model_add_param_string(struct kbase_ipa_model *model,
-				     const char *name, char *addr,
-				     size_t size, bool dt_required)
-{
-	int err;
-	struct device_node *model_dt_node = get_model_dt_node(model);
-	const char *string_prop_value;
-	char *origin;
-
-	err = of_property_read_string(model_dt_node, name,
-				      &string_prop_value);
-	if (err && dt_required) {
-		strncpy(addr, "", size - 1);
-		dev_warn(model->kbdev->dev,
-			 "Error %d, no DT entry: %s.%s = \'%s\'\n",
-			 err, model->ops->name, name, addr);
-		err = 0;
-		origin = "zero";
-	} else if (err && !dt_required) {
-		origin = "default";
-	} else /* !err */ {
-		strncpy(addr, string_prop_value, size - 1);
-		origin = "DT";
-	}
-
-	addr[size - 1] = '\0';
-
-	dev_dbg(model->kbdev->dev, "%s.%s = \'%s\' (%s)\n",
-		model->ops->name, name, string_prop_value, origin);
-
-	err = kbase_ipa_model_param_add(model, name, addr, size,
-					PARAM_TYPE_STRING);
-
-	return err;
-}
-
-void kbase_ipa_term_model(struct kbase_ipa_model *model)
-{
-	if (!model)
-		return;
-
-	lockdep_assert_held(&model->kbdev->ipa.lock);
-
-	if (model->ops->term)
-		model->ops->term(model);
-
-	kbase_ipa_model_param_free_all(model);
-
-	kfree(model);
-}
-KBASE_EXPORT_TEST_API(kbase_ipa_term_model);
-
-struct kbase_ipa_model *kbase_ipa_init_model(struct kbase_device *kbdev,
-					     struct kbase_ipa_model_ops *ops)
-{
-	struct kbase_ipa_model *model;
-	int err;
-
-	lockdep_assert_held(&kbdev->ipa.lock);
-
-	if (!ops || !ops->name)
-		return NULL;
-
-	model = kzalloc(sizeof(struct kbase_ipa_model), GFP_KERNEL);
-	if (!model)
-		return NULL;
-
-	model->kbdev = kbdev;
-	model->ops = ops;
-	INIT_LIST_HEAD(&model->params);
-
-	err = model->ops->init(model);
-	if (err) {
-		dev_err(kbdev->dev,
-			"init of power model \'%s\' returned error %d\n",
-			ops->name, err);
-		goto term_model;
-	}
-
-	err = kbase_ipa_model_recalculate(model);
-	if (err)
-		goto term_model;
-
-	return model;
-
-term_model:
-	kbase_ipa_term_model(model);
-	return NULL;
-}
-KBASE_EXPORT_TEST_API(kbase_ipa_init_model);
-
-static void kbase_ipa_term_locked(struct kbase_device *kbdev)
-{
-	lockdep_assert_held(&kbdev->ipa.lock);
-
-	/* Clean up the models */
-	if (kbdev->ipa.configured_model != kbdev->ipa.fallback_model)
-		kbase_ipa_term_model(kbdev->ipa.configured_model);
-	kbase_ipa_term_model(kbdev->ipa.fallback_model);
-
-	kbdev->ipa.configured_model = NULL;
-	kbdev->ipa.fallback_model = NULL;
-}
-
-int kbase_ipa_init(struct kbase_device *kbdev)
-{
-
-	const char *model_name;
-	struct kbase_ipa_model_ops *ops;
-	struct kbase_ipa_model *default_model = NULL;
-	int err;
-
-	mutex_init(&kbdev->ipa.lock);
-	/*
-	 * Lock during init to avoid warnings from lockdep_assert_held (there
-	 * shouldn't be any concurrent access yet).
-	 */
-	mutex_lock(&kbdev->ipa.lock);
-
-	/* The simple IPA model must *always* be present.*/
-	ops = kbase_ipa_model_ops_find(kbdev, KBASE_IPA_FALLBACK_MODEL_NAME);
-
-	if (!ops->do_utilization_scaling_in_framework) {
-		dev_err(kbdev->dev,
-			"Fallback IPA model %s should not account for utilization\n",
-			ops->name);
-		err = -EINVAL;
-		goto end;
-	}
-
-	default_model = kbase_ipa_init_model(kbdev, ops);
-	if (!default_model) {
-		err = -EINVAL;
-		goto end;
-	}
-
-	kbdev->ipa.fallback_model = default_model;
-	err = of_property_read_string(kbdev->dev->of_node,
-				      "ipa-model",
-				      &model_name);
-	if (err) {
-		/* Attempt to load a match from GPU-ID */
-		u32 gpu_id;
-
-		gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
-		model_name = kbase_ipa_model_name_from_id(gpu_id);
-		dev_dbg(kbdev->dev,
-			"Inferring model from GPU ID 0x%x: \'%s\'\n",
-			gpu_id, model_name);
-	} else {
-		dev_dbg(kbdev->dev,
-			"Using ipa-model parameter from DT: \'%s\'\n",
-			model_name);
-	}
-
-	if (strcmp(KBASE_IPA_FALLBACK_MODEL_NAME, model_name) != 0) {
-		ops = kbase_ipa_model_ops_find(kbdev, model_name);
-		kbdev->ipa.configured_model = kbase_ipa_init_model(kbdev, ops);
-		if (!kbdev->ipa.configured_model) {
-			err = -EINVAL;
-			goto end;
-		}
-	} else {
-		kbdev->ipa.configured_model = default_model;
-		err = 0;
-	}
-
-	kbase_ipa_model_use_configured_locked(kbdev);
-
-end:
-	if (err)
-		kbase_ipa_term_locked(kbdev);
-	else
-		dev_info(kbdev->dev,
-			 "Using configured power model %s, and fallback %s\n",
-			 kbdev->ipa.configured_model->ops->name,
-			 kbdev->ipa.fallback_model->ops->name);
-
-	mutex_unlock(&kbdev->ipa.lock);
-	return err;
-}
-KBASE_EXPORT_TEST_API(kbase_ipa_init);
-
-void kbase_ipa_term(struct kbase_device *kbdev)
-{
-	mutex_lock(&kbdev->ipa.lock);
-	kbase_ipa_term_locked(kbdev);
-	mutex_unlock(&kbdev->ipa.lock);
-}
-KBASE_EXPORT_TEST_API(kbase_ipa_term);
-
-/**
- * kbase_scale_dynamic_power() - Scale a dynamic power coefficient to an OPP
- * @c:		Dynamic model coefficient, in pW/(Hz V^2). Should be in range
- *		0 < c < 2^26 to prevent overflow.
- * @freq:	Frequency, in Hz. Range: 2^23 < freq < 2^30 (~8MHz to ~1GHz)
- * @voltage:	Voltage, in mV. Range: 2^9 < voltage < 2^13 (~0.5V to ~8V)
- *
- * Keep a record of the approximate range of each value at every stage of the
- * calculation, to ensure we don't overflow. This makes heavy use of the
- * approximations 1000 = 2^10 and 1000000 = 2^20, but does the actual
- * calculations in decimal for increased accuracy.
- *
- * Return: Power consumption, in mW. Range: 0 < p < 2^13 (0W to ~8W)
- */
-static u32 kbase_scale_dynamic_power(const u32 c, const u32 freq,
-				     const u32 voltage)
-{
-	/* Range: 2^8 < v2 < 2^16 m(V^2) */
-	const u32 v2 = (voltage * voltage) / 1000;
-
-	/* Range: 2^3 < f_MHz < 2^10 MHz */
-	const u32 f_MHz = freq / 1000000;
-
-	/* Range: 2^11 < v2f_big < 2^26 kHz V^2 */
-	const u32 v2f_big = v2 * f_MHz;
-
-	/* Range: 2^1 < v2f < 2^16 MHz V^2 */
-	const u32 v2f = v2f_big / 1000;
-
-	/* Range (working backwards from next line): 0 < v2fc < 2^23 uW.
-	 * Must be < 2^42 to avoid overflowing the return value. */
-	const u64 v2fc = (u64) c * (u64) v2f;
-	u32 remainder;
-
-	/* Range: 0 < v2fc / 1000 < 2^13 mW */
-	// static inline u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
-	return div_u64_rem(v2fc, 1000, &remainder);
-}
-
-/**
- * kbase_scale_static_power() - Scale a static power coefficient to an OPP
- * @c:		Static model coefficient, in uW/V^3. Should be in range
- *		0 < c < 2^32 to prevent overflow.
- * @voltage:	Voltage, in mV. Range: 2^9 < voltage < 2^13 (~0.5V to ~8V)
- *
- * Return: Power consumption, in mW. Range: 0 < p < 2^13 (0W to ~8W)
- */
-u32 kbase_scale_static_power(const u32 c, const u32 voltage)
-{
-	/* Range: 2^8 < v2 < 2^16 m(V^2) */
-	const u32 v2 = (voltage * voltage) / 1000;
-
-	/* Range: 2^17 < v3_big < 2^29 m(V^2) mV */
-	const u32 v3_big = v2 * voltage;
-
-	/* Range: 2^7 < v3 < 2^19 m(V^3) */
-	const u32 v3 = v3_big / 1000;
-
-	/*
-	 * Range (working backwards from next line): 0 < v3c_big < 2^33 nW.
-	 * The result should be < 2^52 to avoid overflowing the return value.
-	 */
-	const u64 v3c_big = (u64) c * (u64) v3;
-	u32 remainder;
-
-	/* Range: 0 < v3c_big / 1000000 < 2^13 mW */
-	// return v3c_big / 1000000;
-	return div_u64_rem(v3c_big, 1000000, &remainder);
-}
-
-static struct kbase_ipa_model *get_current_model(struct kbase_device *kbdev)
-{
-	lockdep_assert_held(&kbdev->ipa.lock);
-
-	if (atomic_read(&kbdev->ipa_use_configured_model))
-		return kbdev->ipa.configured_model;
-	else
-		return kbdev->ipa.fallback_model;
-}
-
-static u32 get_static_power_locked(struct kbase_device *kbdev,
-				   struct kbase_ipa_model *model,
-				   unsigned long voltage)
-{
-	u32 power = 0;
-	int err;
-	u32 power_coeff;
-
-	lockdep_assert_held(&model->kbdev->ipa.lock);
-
-	if (!model->ops->get_static_coeff)
-		model = kbdev->ipa.fallback_model;
-
-	if (model->ops->get_static_coeff) {
-		err = model->ops->get_static_coeff(model, &power_coeff);
-		if (!err)
-			power = kbase_scale_static_power(power_coeff,
-							 (u32) voltage);
-	}
-
-	return power;
-}
-
-#ifdef CONFIG_MALI_PWRSOFT_765
-static unsigned long kbase_get_static_power(struct devfreq *df,
-					    unsigned long voltage)
-#else
-static unsigned long kbase_get_static_power(unsigned long voltage)
-#endif
-{
-	struct kbase_ipa_model *model;
-	u32 power = 0;
-#ifdef CONFIG_MALI_PWRSOFT_765
-	struct kbase_device *kbdev = dev_get_drvdata(&df->dev);
-#else
-	struct kbase_device *kbdev = kbase_find_device(-1);
-#endif
-
-	mutex_lock(&kbdev->ipa.lock);
-
-	model = get_current_model(kbdev);
-	power = get_static_power_locked(kbdev, model, voltage);
-
-	mutex_unlock(&kbdev->ipa.lock);
-
-#ifndef CONFIG_MALI_PWRSOFT_765
-	kbase_release_device(kbdev);
-#endif
-
-	return power;
-}
-
-#ifdef CONFIG_MALI_PWRSOFT_765
-static unsigned long kbase_get_dynamic_power(struct devfreq *df,
-					     unsigned long freq,
-					     unsigned long voltage)
-#else
-static unsigned long kbase_get_dynamic_power(unsigned long freq,
-					     unsigned long voltage)
-#endif
-{
-	struct kbase_ipa_model *model;
-	u32 power_coeff = 0, power = 0;
-	int err = 0;
-#ifdef CONFIG_MALI_PWRSOFT_765
-	struct kbase_device *kbdev = dev_get_drvdata(&df->dev);
-#else
-	struct kbase_device *kbdev = kbase_find_device(-1);
-#endif
-
-	mutex_lock(&kbdev->ipa.lock);
-
-	model = kbdev->ipa.fallback_model;
-
-	err = model->ops->get_dynamic_coeff(model, &power_coeff, freq);
-
-	if (!err)
-		power = kbase_scale_dynamic_power(power_coeff, freq, voltage);
-	else
-		dev_err_ratelimited(kbdev->dev,
-				    "Model %s returned error code %d\n",
-				    model->ops->name, err);
-
-	mutex_unlock(&kbdev->ipa.lock);
-
-#ifndef CONFIG_MALI_PWRSOFT_765
-	kbase_release_device(kbdev);
-#endif
-
-	return power;
-}
-
-int kbase_get_real_power(struct devfreq *df, u32 *power,
-				unsigned long freq,
-				unsigned long voltage)
-{
-	struct kbase_ipa_model *model;
-	u32 power_coeff = 0;
-	int err = 0;
-	struct kbase_device *kbdev = dev_get_drvdata(&df->dev);
-
-	mutex_lock(&kbdev->ipa.lock);
-
-	model = get_current_model(kbdev);
-
-	err = model->ops->get_dynamic_coeff(model, &power_coeff, freq);
-
-	/* If we switch to protected model between get_current_model() and
-	 * get_dynamic_coeff(), counter reading could fail. If that happens
-	 * (unlikely, but possible), revert to the fallback model. */
-	if (err && model != kbdev->ipa.fallback_model) {
-		model = kbdev->ipa.fallback_model;
-		err = model->ops->get_dynamic_coeff(model, &power_coeff, freq);
-	}
-
-	if (err)
-		goto exit_unlock;
-
-	*power = kbase_scale_dynamic_power(power_coeff, freq, voltage);
-
-	if (model->ops->do_utilization_scaling_in_framework) {
-		struct devfreq_dev_status *status = &df->last_status;
-		unsigned long total_time = max(status->total_time, 1ul);
-		u64 busy_time = min(status->busy_time, total_time);
-		u32 remainder;
-
-		// *power = ((u64) *power * (u64) busy_time) / total_time;
-		*power = div_u64_rem(((u64) *power * (u64) busy_time), total_time, &remainder);
-	}
-
-	*power += get_static_power_locked(kbdev, model, voltage);
-
-exit_unlock:
-	mutex_unlock(&kbdev->ipa.lock);
-
-	return err;
-}
-KBASE_EXPORT_TEST_API(kbase_get_real_power);
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0)
-struct devfreq_cooling_ops kbase_ipa_power_model_ops = {
-#else
-struct devfreq_cooling_power kbase_ipa_power_model_ops = {
-#endif
-	.get_static_power = &kbase_get_static_power,
-	.get_dynamic_power = &kbase_get_dynamic_power,
-};
-KBASE_EXPORT_TEST_API(kbase_ipa_power_model_ops);
diff --git a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.h b/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.h
deleted file mode 100644
index b2d3db1..0000000
--- a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa.h
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2016-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KBASE_IPA_H_
-#define _KBASE_IPA_H_
-
-#if defined(CONFIG_MALI_DEVFREQ) && defined(CONFIG_DEVFREQ_THERMAL)
-
-struct devfreq;
-
-struct kbase_ipa_model {
-	struct list_head link;
-	struct kbase_device *kbdev;
-	void *model_data;
-	struct kbase_ipa_model_ops *ops;
-	struct list_head params;
-	bool missing_dt_node_warning;
-};
-
-/**
- * kbase_ipa_model_add_param_s32 - Add an integer model parameter
- * @model:	pointer to IPA model
- * @name:	name of corresponding debugfs entry
- * @addr:	address where the value is stored
- * @num_elems:	number of elements (1 if not an array)
- * @dt_required: if false, a corresponding devicetree entry is not required,
- *		 and the current value will be used. If true, a warning is
- *		 output and the data is zeroed
- *
- * Return: 0 on success, or an error code
- */
-int kbase_ipa_model_add_param_s32(struct kbase_ipa_model *model,
-				  const char *name, s32 *addr,
-				  size_t num_elems, bool dt_required);
-
-/**
- * kbase_ipa_model_add_param_string - Add a string model parameter
- * @model:	pointer to IPA model
- * @name:	name of corresponding debugfs entry
- * @addr:	address where the value is stored
- * @size:	size, in bytes, of the value storage (so the maximum string
- *		length is size - 1)
- * @dt_required: if false, a corresponding devicetree entry is not required,
- *		 and the current value will be used. If true, a warning is
- *		 output and the data is zeroed
- *
- * Return: 0 on success, or an error code
- */
-int kbase_ipa_model_add_param_string(struct kbase_ipa_model *model,
-				     const char *name, char *addr,
-				     size_t size, bool dt_required);
-
-struct kbase_ipa_model_ops {
-	char *name;
-	/* The init, recalculate and term ops on the default model are always
-	 * called.  However, all the other models are only invoked if the model
-	 * is selected in the device tree. Otherwise they are never
-	 * initialized. Additional resources can be acquired by models in
-	 * init(), however they must be terminated in the term().
-	 */
-	int (*init)(struct kbase_ipa_model *model);
-	/* Called immediately after init(), or when a parameter is changed, so
-	 * that any coefficients derived from model parameters can be
-	 * recalculated. */
-	int (*recalculate)(struct kbase_ipa_model *model);
-	void (*term)(struct kbase_ipa_model *model);
-	/*
-	 * get_dynamic_coeff() - calculate dynamic power coefficient
-	 * @model:		pointer to model
-	 * @coeffp:		pointer to return value location
-	 * @current_freq:	frequency the GPU has been running at for the
-	 *			previous sampling period.
-	 *
-	 * Calculate a dynamic power coefficient, with units pW/(Hz V^2), which
-	 * is then scaled by the IPA framework according to the current OPP's
-	 * frequency and voltage.
-	 *
-	 * Return: 0 on success, or an error code.
-	 */
-	int (*get_dynamic_coeff)(struct kbase_ipa_model *model, u32 *coeffp,
-				 u32 current_freq);
-	/*
-	 * get_static_coeff() - calculate static power coefficient
-	 * @model:		pointer to model
-	 * @coeffp:		pointer to return value location
-	 *
-	 * Calculate a static power coefficient, with units uW/(V^3), which is
-	 * scaled by the IPA framework according to the current OPP's voltage.
-	 *
-	 * Return: 0 on success, or an error code.
-	 */
-	int (*get_static_coeff)(struct kbase_ipa_model *model, u32 *coeffp);
-	/* If false, the model's get_dynamic_coeff() method accounts for how
-	 * long the GPU was active over the sample period. If true, the
-	 * framework will scale the calculated power according to the
-	 * utilization stats recorded by devfreq in get_real_power(). */
-	bool do_utilization_scaling_in_framework;
-};
-
-/* Models can be registered only in the platform's platform_init_func call */
-int kbase_ipa_model_ops_register(struct kbase_device *kbdev,
-			     struct kbase_ipa_model_ops *new_model_ops);
-struct kbase_ipa_model *kbase_ipa_get_model(struct kbase_device *kbdev,
-					    const char *name);
-
-int kbase_ipa_init(struct kbase_device *kbdev);
-void kbase_ipa_term(struct kbase_device *kbdev);
-void kbase_ipa_model_use_fallback_locked(struct kbase_device *kbdev);
-void kbase_ipa_model_use_configured_locked(struct kbase_device *kbdev);
-int kbase_ipa_model_recalculate(struct kbase_ipa_model *model);
-struct kbase_ipa_model *kbase_ipa_init_model(struct kbase_device *kbdev,
-					     struct kbase_ipa_model_ops *ops);
-void kbase_ipa_term_model(struct kbase_ipa_model *model);
-
-extern struct kbase_ipa_model_ops kbase_simple_ipa_model_ops;
-
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0)
-extern struct devfreq_cooling_ops kbase_ipa_power_model_ops;
-#else
-extern struct devfreq_cooling_power kbase_ipa_power_model_ops;
-#endif
-
-#else /* !(defined(CONFIG_MALI_DEVFREQ) && defined(CONFIG_DEVFREQ_THERMAL)) */
-
-static inline void kbase_ipa_model_use_fallback_locked(struct kbase_device *kbdev)
-{ }
-
-static inline void kbase_ipa_model_use_configured_locked(struct kbase_device *kbdev)
-{ }
-
-#endif /* (defined(CONFIG_MALI_DEVFREQ) && defined(CONFIG_DEVFREQ_THERMAL)) */
-
-#endif
diff --git a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.c b/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.c
deleted file mode 100644
index eafc140..0000000
--- a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.c
+++ /dev/null
@@ -1,219 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#include <linux/debugfs.h>
-#include <linux/list.h>
-#include <linux/mutex.h>
-
-#include "mali_kbase.h"
-#include "mali_kbase_ipa.h"
-#include "mali_kbase_ipa_debugfs.h"
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0))
-#define DEFINE_DEBUGFS_ATTRIBUTE DEFINE_SIMPLE_ATTRIBUTE
-#endif
-
-struct kbase_ipa_model_param {
-	char *name;
-	union {
-		void *voidp;
-		s32 *s32p;
-		char *str;
-	} addr;
-	size_t size;
-	enum kbase_ipa_model_param_type type;
-	struct kbase_ipa_model *model;
-	struct list_head link;
-};
-
-static int param_int_get(void *data, u64 *val)
-{
-	struct kbase_ipa_model_param *param = data;
-
-	mutex_lock(&param->model->kbdev->ipa.lock);
-	*(s64 *) val = *param->addr.s32p;
-	mutex_unlock(&param->model->kbdev->ipa.lock);
-
-	return 0;
-}
-
-static int param_int_set(void *data, u64 val)
-{
-	struct kbase_ipa_model_param *param = data;
-	struct kbase_ipa_model *model = param->model;
-	s64 sval = (s64) val;
-	int err = 0;
-
-	if (sval < S32_MIN || sval > S32_MAX)
-		return -ERANGE;
-
-	mutex_lock(&param->model->kbdev->ipa.lock);
-	*param->addr.s32p = val;
-	err = kbase_ipa_model_recalculate(model);
-	mutex_unlock(&param->model->kbdev->ipa.lock);
-
-	return err;
-}
-
-DEFINE_DEBUGFS_ATTRIBUTE(fops_s32, param_int_get, param_int_set, "%lld\n");
-
-static ssize_t param_string_get(struct file *file, char __user *user_buf,
-				size_t count, loff_t *ppos)
-{
-	struct kbase_ipa_model_param *param = file->private_data;
-	ssize_t ret;
-	size_t len;
-
-	mutex_lock(&param->model->kbdev->ipa.lock);
-	len = strnlen(param->addr.str, param->size - 1) + 1;
-	ret = simple_read_from_buffer(user_buf, count, ppos,
-				      param->addr.str, len);
-	mutex_unlock(&param->model->kbdev->ipa.lock);
-
-	return ret;
-}
-
-static ssize_t param_string_set(struct file *file, const char __user *user_buf,
-				size_t count, loff_t *ppos)
-{
-	struct kbase_ipa_model_param *param = file->private_data;
-	struct kbase_ipa_model *model = param->model;
-	ssize_t ret = count;
-	size_t buf_size;
-	int err;
-
-	mutex_lock(&model->kbdev->ipa.lock);
-
-	if (count > param->size) {
-		ret = -EINVAL;
-		goto end;
-	}
-
-	buf_size = min(param->size - 1, count);
-	if (copy_from_user(param->addr.str, user_buf, buf_size)) {
-		ret = -EFAULT;
-		goto end;
-	}
-
-	param->addr.str[buf_size] = '\0';
-
-	err = kbase_ipa_model_recalculate(model);
-	if (err < 0)
-		ret = err;
-
-end:
-	mutex_unlock(&model->kbdev->ipa.lock);
-
-	return ret;
-}
-
-static const struct file_operations fops_string = {
-	.read = param_string_get,
-	.write = param_string_set,
-	.open = simple_open,
-	.llseek = default_llseek,
-};
-
-int kbase_ipa_model_param_add(struct kbase_ipa_model *model, const char *name,
-			      void *addr, size_t size,
-			      enum kbase_ipa_model_param_type type)
-{
-	struct kbase_ipa_model_param *param;
-
-	param = kzalloc(sizeof(*param), GFP_KERNEL);
-
-	if (!param)
-		return -ENOMEM;
-
-	/* 'name' is stack-allocated for array elements, so copy it into
-	 * heap-allocated storage */
-	param->name = kstrdup(name, GFP_KERNEL);
-	param->addr.voidp = addr;
-	param->size = size;
-	param->type = type;
-	param->model = model;
-
-	list_add(&param->link, &model->params);
-
-	return 0;
-}
-
-void kbase_ipa_model_param_free_all(struct kbase_ipa_model *model)
-{
-	struct kbase_ipa_model_param *param_p, *param_n;
-
-	list_for_each_entry_safe(param_p, param_n, &model->params, link) {
-		list_del(&param_p->link);
-		kfree(param_p->name);
-		kfree(param_p);
-	}
-}
-
-static void kbase_ipa_model_debugfs_init(struct kbase_ipa_model *model)
-{
-	struct list_head *it;
-	struct dentry *dir;
-
-	lockdep_assert_held(&model->kbdev->ipa.lock);
-
-	dir = debugfs_create_dir(model->ops->name,
-				 model->kbdev->mali_debugfs_directory);
-
-	if (!dir) {
-		dev_err(model->kbdev->dev,
-			"Couldn't create mali debugfs %s directory",
-			model->ops->name);
-		return;
-	}
-
-	list_for_each(it, &model->params) {
-		struct kbase_ipa_model_param *param =
-				list_entry(it,
-					   struct kbase_ipa_model_param,
-					   link);
-		const struct file_operations *fops = NULL;
-
-		switch (param->type) {
-		case PARAM_TYPE_S32:
-			fops = &fops_s32;
-			break;
-		case PARAM_TYPE_STRING:
-			fops = &fops_string;
-			break;
-		}
-
-		if (unlikely(!fops)) {
-			dev_err(model->kbdev->dev,
-				"Type not set for %s parameter %s\n",
-				model->ops->name, param->name);
-		} else {
-			debugfs_create_file(param->name, S_IRUGO | S_IWUSR,
-					    dir, param, fops);
-		}
-	}
-}
-
-void kbase_ipa_debugfs_init(struct kbase_device *kbdev)
-{
-	mutex_lock(&kbdev->ipa.lock);
-
-	if (kbdev->ipa.configured_model != kbdev->ipa.fallback_model)
-		kbase_ipa_model_debugfs_init(kbdev->ipa.configured_model);
-	kbase_ipa_model_debugfs_init(kbdev->ipa.fallback_model);
-
-	mutex_unlock(&kbdev->ipa.lock);
-}
diff --git a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.h b/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.h
deleted file mode 100644
index ec06e20..0000000
--- a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_debugfs.h
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KBASE_IPA_DEBUGFS_H_
-#define _KBASE_IPA_DEBUGFS_H_
-
-enum kbase_ipa_model_param_type {
-	PARAM_TYPE_S32 = 1,
-	PARAM_TYPE_STRING,
-};
-
-#ifdef CONFIG_DEBUG_FS
-
-void kbase_ipa_debugfs_init(struct kbase_device *kbdev);
-int kbase_ipa_model_param_add(struct kbase_ipa_model *model, const char *name,
-			      void *addr, size_t size,
-			      enum kbase_ipa_model_param_type type);
-void kbase_ipa_model_param_free_all(struct kbase_ipa_model *model);
-
-#else /* CONFIG_DEBUG_FS */
-
-static inline int kbase_ipa_model_param_add(struct kbase_ipa_model *model,
-					    const char *name, void *addr,
-					    size_t size,
-					    enum kbase_ipa_model_param_type type)
-{
-	return 0;
-}
-
-static inline void kbase_ipa_model_param_free_all(struct kbase_ipa_model *model)
-{ }
-
-#endif /* CONFIG_DEBUG_FS */
-
-#endif /* _KBASE_IPA_DEBUGFS_H_ */
diff --git a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_simple.c b/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_simple.c
deleted file mode 100644
index e11bfb0..0000000
--- a/drivers/gpu/arm/midgard/ipa/mali_kbase_ipa_simple.c
+++ /dev/null
@@ -1,222 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2016-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#include <linux/thermal.h>
-#ifdef CONFIG_DEVFREQ_THERMAL
-#include <linux/devfreq_cooling.h>
-#endif
-#include <linux/of.h>
-#include <linux/math64.h>
-
-#include "mali_kbase.h"
-#include "mali_kbase_defs.h"
-
-/*
- * This model is primarily designed for the Juno platform. It may not be
- * suitable for other platforms. The additional resources in this model
- * should preferably be minimal, as this model is rarely used when a dynamic
- * model is available.
- */
-
-/**
- * struct kbase_ipa_model_simple_data - IPA context per device
- * @dynamic_coefficient: dynamic coefficient of the model
- * @static_coefficient:  static coefficient of the model
- * @ts:                  Thermal scaling coefficients of the model
- * @tz_name:             Thermal zone name
- * @gpu_tz:              thermal zone device
- */
-
-struct kbase_ipa_model_simple_data {
-	u32 dynamic_coefficient;
-	u32 static_coefficient;
-	s32 ts[4];
-	char tz_name[16];
-	struct thermal_zone_device *gpu_tz;
-};
-#define FALLBACK_STATIC_TEMPERATURE 55000
-
-/**
- * calculate_temp_scaling_factor() - Calculate temperature scaling coefficient
- * @ts:		Signed coefficients, in order t^0 to t^3, with units Deg^-N
- * @t:		Temperature, in mDeg C. Range: -2^17 < t < 2^17
- *
- * Scale the temperature according to a cubic polynomial whose coefficients are
- * provided in the device tree. The result is used to scale the static power
- * coefficient, where 1000000 means no change.
- *
- * Return: Temperature scaling factor. Approx range 0 < ret < 10,000,000.
- */
-static u32 calculate_temp_scaling_factor(s32 ts[4], s64 t)
-{
-	/* Range: -2^24 < t2 < 2^24 m(Deg^2) */
-	u32 remainder;
-	// static inline s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
-	const s64 t2 = div_s64_rem((t * t), 1000, &remainder);
-
-	/* Range: -2^31 < t3 < 2^31 m(Deg^3) */
-	const s64 t3 = div_s64_rem((t * t2), 1000, &remainder);
-
-	/*
-	 * Sum the parts. t^[1-3] are in m(Deg^N), but the coefficients are in
-	 * Deg^-N, so we need to multiply the last coefficient by 1000.
-	 * Range: -2^63 < res_big < 2^63
-	 */
-	const s64 res_big = ts[3] * t3    /* +/- 2^62 */
-			  + ts[2] * t2    /* +/- 2^55 */
-			  + ts[1] * t     /* +/- 2^48 */
-			  + ts[0] * 1000; /* +/- 2^41 */
-
-	/* Range: -2^60 < res_unclamped < 2^60 */
-	s64 res_unclamped = div_s64_rem(res_big, 1000, &remainder);
-
-	/* Clamp to range of 0x to 10x the static power */
-	return clamp(res_unclamped, (s64) 0, (s64) 10000000);
-}
-
-static int model_static_coeff(struct kbase_ipa_model *model, u32 *coeffp)
-{
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 3, 0)
-	unsigned long temp;
-#else
-	int temp;
-#endif
-	u32 temp_scaling_factor;
-	struct kbase_ipa_model_simple_data *model_data =
-		(struct kbase_ipa_model_simple_data *) model->model_data;
-	struct thermal_zone_device *gpu_tz = model_data->gpu_tz;
-
-	if (gpu_tz) {
-		int ret;
-
-		ret = gpu_tz->ops->get_temp(gpu_tz, &temp);
-		if (ret) {
-			pr_warn_ratelimited("Error reading temperature for gpu thermal zone: %d\n",
-					ret);
-			temp = FALLBACK_STATIC_TEMPERATURE;
-		}
-	} else {
-		temp = FALLBACK_STATIC_TEMPERATURE;
-	}
-
-	temp_scaling_factor = calculate_temp_scaling_factor(model_data->ts,
-							    temp);
-
-	*coeffp = model_data->static_coefficient * temp_scaling_factor;
-	*coeffp /= 1000000;
-
-	return 0;
-}
-
-static int model_dynamic_coeff(struct kbase_ipa_model *model, u32 *coeffp,
-			       u32 current_freq)
-{
-	struct kbase_ipa_model_simple_data *model_data =
-		(struct kbase_ipa_model_simple_data *) model->model_data;
-
-	*coeffp = model_data->dynamic_coefficient;
-
-	return 0;
-}
-
-static int add_params(struct kbase_ipa_model *model)
-{
-	int err = 0;
-	struct kbase_ipa_model_simple_data *model_data =
-			(struct kbase_ipa_model_simple_data *)model->model_data;
-
-	err = kbase_ipa_model_add_param_s32(model, "static-coefficient",
-					    &model_data->static_coefficient,
-					    1, true);
-	if (err)
-		goto end;
-
-	err = kbase_ipa_model_add_param_s32(model, "dynamic-coefficient",
-					    &model_data->dynamic_coefficient,
-					    1, true);
-	if (err)
-		goto end;
-
-	err = kbase_ipa_model_add_param_s32(model, "ts",
-					    model_data->ts, 4, true);
-	if (err)
-		goto end;
-
-	err = kbase_ipa_model_add_param_string(model, "thermal-zone",
-					       model_data->tz_name,
-					       sizeof(model_data->tz_name), true);
-
-end:
-	return err;
-}
-
-static int kbase_simple_power_model_init(struct kbase_ipa_model *model)
-{
-	int err;
-	struct kbase_ipa_model_simple_data *model_data;
-
-	model_data = kzalloc(sizeof(struct kbase_ipa_model_simple_data),
-			     GFP_KERNEL);
-	if (!model_data)
-		return -ENOMEM;
-
-	model->model_data = (void *) model_data;
-
-	err = add_params(model);
-
-	return err;
-}
-
-static int kbase_simple_power_model_recalculate(struct kbase_ipa_model *model)
-{
-	struct kbase_ipa_model_simple_data *model_data =
-			(struct kbase_ipa_model_simple_data *)model->model_data;
-
-	if (!strnlen(model_data->tz_name, sizeof(model_data->tz_name))) {
-		model_data->gpu_tz = NULL;
-	} else {
-		model_data->gpu_tz = thermal_zone_get_zone_by_name(model_data->tz_name);
-
-		if (IS_ERR(model_data->gpu_tz)) {
-			pr_warn_ratelimited("Error %ld getting thermal zone \'%s\', not yet ready?\n",
-					    PTR_ERR(model_data->gpu_tz),
-					    model_data->tz_name);
-			model_data->gpu_tz = NULL;
-			return -EPROBE_DEFER;
-		}
-	}
-
-	return 0;
-}
-
-static void kbase_simple_power_model_term(struct kbase_ipa_model *model)
-{
-	struct kbase_ipa_model_simple_data *model_data =
-			(struct kbase_ipa_model_simple_data *)model->model_data;
-
-	kfree(model_data);
-}
-
-struct kbase_ipa_model_ops kbase_simple_ipa_model_ops = {
-		.name = "mali-simple-power-model",
-		.init = &kbase_simple_power_model_init,
-		.recalculate = &kbase_simple_power_model_recalculate,
-		.term = &kbase_simple_power_model_term,
-		.get_dynamic_coeff = &model_dynamic_coeff,
-		.get_static_coeff = &model_static_coeff,
-		.do_utilization_scaling_in_framework = true,
-};
diff --git a/drivers/gpu/arm/midgard/mali_base_hwconfig_features.h b/drivers/gpu/arm/midgard/mali_base_hwconfig_features.h
index 6be0a33..8b07cbc 100644
--- a/drivers/gpu/arm/midgard/mali_base_hwconfig_features.h
+++ b/drivers/gpu/arm/midgard/mali_base_hwconfig_features.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -51,7 +51,6 @@ enum base_hw_feature {
 	BASE_HW_FEATURE_PROTECTED_MODE,
 	BASE_HW_FEATURE_COHERENCY_REG,
 	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	BASE_HW_FEATURE_AARCH64_MMU,
 	BASE_HW_FEATURE_END
 };
 
@@ -220,92 +219,5 @@ static const enum base_hw_feature base_hw_features_tHEx[] = {
 	BASE_HW_FEATURE_END
 };
 
-static const enum base_hw_feature base_hw_features_tSIx[] = {
-	BASE_HW_FEATURE_33BIT_VA,
-	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
-	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
-	BASE_HW_FEATURE_XAFFINITY,
-	BASE_HW_FEATURE_WARPING,
-	BASE_HW_FEATURE_INTERPIPE_REG_ALIASING,
-	BASE_HW_FEATURE_32_BIT_UNIFORM_ADDRESS,
-	BASE_HW_FEATURE_ATTR_AUTO_TYPE_INFERRAL,
-	BASE_HW_FEATURE_BRNDOUT_CC,
-	BASE_HW_FEATURE_BRNDOUT_KILL,
-	BASE_HW_FEATURE_LD_ST_LEA_TEX,
-	BASE_HW_FEATURE_LD_ST_TILEBUFFER,
-	BASE_HW_FEATURE_LINEAR_FILTER_FLOAT,
-	BASE_HW_FEATURE_MRT,
-	BASE_HW_FEATURE_MSAA_16X,
-	BASE_HW_FEATURE_NEXT_INSTRUCTION_TYPE,
-	BASE_HW_FEATURE_OUT_OF_ORDER_EXEC,
-	BASE_HW_FEATURE_T7XX_PAIRING_RULES,
-	BASE_HW_FEATURE_TEST4_DATUM_MODE,
-	BASE_HW_FEATURE_FLUSH_REDUCTION,
-	BASE_HW_FEATURE_PROTECTED_MODE,
-	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	BASE_HW_FEATURE_COHERENCY_REG,
-	BASE_HW_FEATURE_END
-};
-
-
-#ifdef MALI_INCLUDE_TKAX
-static const enum base_hw_feature base_hw_features_tKAx[] = {
-	BASE_HW_FEATURE_33BIT_VA,
-	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
-	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
-	BASE_HW_FEATURE_XAFFINITY,
-	BASE_HW_FEATURE_WARPING,
-	BASE_HW_FEATURE_INTERPIPE_REG_ALIASING,
-	BASE_HW_FEATURE_32_BIT_UNIFORM_ADDRESS,
-	BASE_HW_FEATURE_ATTR_AUTO_TYPE_INFERRAL,
-	BASE_HW_FEATURE_BRNDOUT_CC,
-	BASE_HW_FEATURE_BRNDOUT_KILL,
-	BASE_HW_FEATURE_LD_ST_LEA_TEX,
-	BASE_HW_FEATURE_LD_ST_TILEBUFFER,
-	BASE_HW_FEATURE_LINEAR_FILTER_FLOAT,
-	BASE_HW_FEATURE_MRT,
-	BASE_HW_FEATURE_MSAA_16X,
-	BASE_HW_FEATURE_NEXT_INSTRUCTION_TYPE,
-	BASE_HW_FEATURE_OUT_OF_ORDER_EXEC,
-	BASE_HW_FEATURE_T7XX_PAIRING_RULES,
-	BASE_HW_FEATURE_TEST4_DATUM_MODE,
-	BASE_HW_FEATURE_FLUSH_REDUCTION,
-	BASE_HW_FEATURE_PROTECTED_MODE,
-	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	BASE_HW_FEATURE_COHERENCY_REG,
-	BASE_HW_FEATURE_END
-};
-
-#endif /* MALI_INCLUDE_TKAX */
-
-#ifdef MALI_INCLUDE_TTRX
-static const enum base_hw_feature base_hw_features_tTRx[] = {
-	BASE_HW_FEATURE_33BIT_VA,
-	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
-	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
-	BASE_HW_FEATURE_XAFFINITY,
-	BASE_HW_FEATURE_WARPING,
-	BASE_HW_FEATURE_INTERPIPE_REG_ALIASING,
-	BASE_HW_FEATURE_32_BIT_UNIFORM_ADDRESS,
-	BASE_HW_FEATURE_ATTR_AUTO_TYPE_INFERRAL,
-	BASE_HW_FEATURE_BRNDOUT_CC,
-	BASE_HW_FEATURE_BRNDOUT_KILL,
-	BASE_HW_FEATURE_LD_ST_LEA_TEX,
-	BASE_HW_FEATURE_LD_ST_TILEBUFFER,
-	BASE_HW_FEATURE_LINEAR_FILTER_FLOAT,
-	BASE_HW_FEATURE_MRT,
-	BASE_HW_FEATURE_MSAA_16X,
-	BASE_HW_FEATURE_NEXT_INSTRUCTION_TYPE,
-	BASE_HW_FEATURE_OUT_OF_ORDER_EXEC,
-	BASE_HW_FEATURE_T7XX_PAIRING_RULES,
-	BASE_HW_FEATURE_TEST4_DATUM_MODE,
-	BASE_HW_FEATURE_FLUSH_REDUCTION,
-	BASE_HW_FEATURE_PROTECTED_MODE,
-	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	BASE_HW_FEATURE_COHERENCY_REG,
-	BASE_HW_FEATURE_END
-};
-
-#endif /* MALI_INCLUDE_TTRX */
 
 #endif /* _BASE_HWCONFIG_FEATURES_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_base_hwconfig_issues.h b/drivers/gpu/arm/midgard/mali_base_hwconfig_issues.h
index 6d7e5c5..4d95b4f 100644
--- a/drivers/gpu/arm/midgard/mali_base_hwconfig_issues.h
+++ b/drivers/gpu/arm/midgard/mali_base_hwconfig_issues.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -92,7 +92,6 @@ enum base_hw_issue {
 	BASE_HW_ISSUE_11042,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_26,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
@@ -106,6 +105,7 @@ enum base_hw_issue {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	BASE_HW_ISSUE_TMIX_7940,
 	BASE_HW_ISSUE_TMIX_8042,
@@ -116,8 +116,6 @@ enum base_hw_issue {
 	BASE_HW_ISSUE_TMIX_8463,
 	BASE_HW_ISSUE_TMIX_8456,
 	GPUCORE_1619,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -187,7 +185,6 @@ static const enum base_hw_issue base_hw_issues_t60x_r0p0_15dev0[] = {
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_3964,
 	GPUCORE_1619,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -227,7 +224,6 @@ static const enum base_hw_issue base_hw_issues_t60x_r0p0_eac[] = {
 	BASE_HW_ISSUE_11054,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_3964,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -265,7 +261,6 @@ static const enum base_hw_issue base_hw_issues_t60x_r0p1[] = {
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3964,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -299,7 +294,6 @@ static const enum base_hw_issue base_hw_issues_t62x_r0p1[] = {
 	BASE_HW_ISSUE_11054,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -324,7 +318,6 @@ static const enum base_hw_issue base_hw_issues_t62x_r1p0[] = {
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3964,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -346,7 +339,6 @@ static const enum base_hw_issue base_hw_issues_t62x_r1p1[] = {
 	BASE_HW_ISSUE_11054,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -373,8 +365,8 @@ static const enum base_hw_issue base_hw_issues_t76x_r0p0[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -401,8 +393,8 @@ static const enum base_hw_issue base_hw_issues_t76x_r0p1[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -427,8 +419,8 @@ static const enum base_hw_issue base_hw_issues_t76x_r0p1_50rel0[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -455,8 +447,8 @@ static const enum base_hw_issue base_hw_issues_t76x_r0p2[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -481,8 +473,8 @@ static const enum base_hw_issue base_hw_issues_t76x_r0p3[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -504,8 +496,8 @@ static const enum base_hw_issue base_hw_issues_t76x_r1p0[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -525,7 +517,6 @@ static const enum base_hw_issue base_hw_issues_t72x_r0p0[] = {
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3964,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -542,11 +533,9 @@ static const enum base_hw_issue base_hw_issues_t72x_r1p0[] = {
 	BASE_HW_ISSUE_11042,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3964,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -563,11 +552,9 @@ static const enum base_hw_issue base_hw_issues_t72x_r1p1[] = {
 	BASE_HW_ISSUE_11042,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3964,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -601,6 +588,7 @@ static const enum base_hw_issue base_hw_issues_model_t76x[] = {
 	BASE_HW_ISSUE_T76X_3793,
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	GPUCORE_1619,
 	BASE_HW_ISSUE_END
@@ -661,8 +649,8 @@ static const enum base_hw_issue base_hw_issues_tFRx_r0p1[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -682,8 +670,8 @@ static const enum base_hw_issue base_hw_issues_tFRx_r0p2[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -701,8 +689,8 @@ static const enum base_hw_issue base_hw_issues_tFRx_r1p0[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -720,8 +708,8 @@ static const enum base_hw_issue base_hw_issues_tFRx_r2p0[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -735,6 +723,7 @@ static const enum base_hw_issue base_hw_issues_model_tFRx[] = {
 	BASE_HW_ISSUE_T76X_3793,
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	GPUCORE_1619,
 	BASE_HW_ISSUE_END
@@ -756,8 +745,8 @@ static const enum base_hw_issue base_hw_issues_t86x_r0p2[] = {
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -775,8 +764,8 @@ static const enum base_hw_issue base_hw_issues_t86x_r1p0[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -794,8 +783,8 @@ static const enum base_hw_issue base_hw_issues_t86x_r2p0[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3966,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -809,6 +798,7 @@ static const enum base_hw_issue base_hw_issues_model_t86x[] = {
 	BASE_HW_ISSUE_T76X_3793,
 	BASE_HW_ISSUE_T76X_3979,
 	BASE_HW_ISSUE_TMIX_7891,
+	BASE_HW_ISSUE_T76X_3982,
 	GPUCORE_1619,
 	BASE_HW_ISSUE_END
 };
@@ -820,7 +810,6 @@ static const enum base_hw_issue base_hw_issues_t83x_r0p1[] = {
 	BASE_HW_ISSUE_10946,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3086,
@@ -829,8 +818,8 @@ static const enum base_hw_issue base_hw_issues_t83x_r0p1[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3960,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -841,7 +830,6 @@ static const enum base_hw_issue base_hw_issues_t83x_r1p0[] = {
 	BASE_HW_ISSUE_10946,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3086,
 	BASE_HW_ISSUE_T76X_3700,
@@ -849,8 +837,8 @@ static const enum base_hw_issue base_hw_issues_t83x_r1p0[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3960,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -864,9 +852,9 @@ static const enum base_hw_issue base_hw_issues_model_t83x[] = {
 	BASE_HW_ISSUE_T76X_3793,
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	GPUCORE_1619,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -877,7 +865,6 @@ static const enum base_hw_issue base_hw_issues_t82x_r0p0[] = {
 	BASE_HW_ISSUE_10946,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3086,
@@ -887,8 +874,8 @@ static const enum base_hw_issue base_hw_issues_t82x_r0p0[] = {
 	BASE_HW_ISSUE_T76X_3960,
 	BASE_HW_ISSUE_T76X_3964,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -899,7 +886,6 @@ static const enum base_hw_issue base_hw_issues_t82x_r0p1[] = {
 	BASE_HW_ISSUE_10946,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1909,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3086,
@@ -908,8 +894,8 @@ static const enum base_hw_issue base_hw_issues_t82x_r0p1[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3960,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -920,7 +906,6 @@ static const enum base_hw_issue base_hw_issues_t82x_r1p0[] = {
 	BASE_HW_ISSUE_10946,
 	BASE_HW_ISSUE_11051,
 	BASE_HW_ISSUE_11054,
-	BASE_HW_ISSUE_T720_1386,
 	BASE_HW_ISSUE_T76X_1963,
 	BASE_HW_ISSUE_T76X_3086,
 	BASE_HW_ISSUE_T76X_3700,
@@ -928,8 +913,8 @@ static const enum base_hw_issue base_hw_issues_t82x_r1p0[] = {
 	BASE_HW_ISSUE_T76X_3953,
 	BASE_HW_ISSUE_T76X_3960,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
@@ -942,6 +927,7 @@ static const enum base_hw_issue base_hw_issues_model_t82x[] = {
 	BASE_HW_ISSUE_T76X_3700,
 	BASE_HW_ISSUE_T76X_3793,
 	BASE_HW_ISSUE_T76X_3979,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	GPUCORE_1619,
 	BASE_HW_ISSUE_END
@@ -950,24 +936,28 @@ static const enum base_hw_issue base_hw_issues_model_t82x[] = {
 static const enum base_hw_issue base_hw_issues_tMIx_r0p0_05dev0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10821,
 	BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_T76X_3700,
 	BASE_HW_ISSUE_T76X_3953,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	BASE_HW_ISSUE_TMIX_8042,
 	BASE_HW_ISSUE_TMIX_8133,
 	BASE_HW_ISSUE_TMIX_8138,
-	BASE_HW_ISSUE_TMIX_8206,
 	BASE_HW_ISSUE_TMIX_8343,
 	BASE_HW_ISSUE_TMIX_8463,
 	BASE_HW_ISSUE_TMIX_8456,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
 static const enum base_hw_issue base_hw_issues_tMIx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10821,
 	BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	BASE_HW_ISSUE_TMIX_7940,
 	BASE_HW_ISSUE_TMIX_8042,
@@ -977,13 +967,14 @@ static const enum base_hw_issue base_hw_issues_tMIx_r0p0[] = {
 	BASE_HW_ISSUE_TMIX_8343,
 	BASE_HW_ISSUE_TMIX_8463,
 	BASE_HW_ISSUE_TMIX_8456,
-	BASE_HW_ISSUE_TMIX_8438,
 	BASE_HW_ISSUE_END
 };
 
 static const enum base_hw_issue base_hw_issues_model_tMIx[] = {
 	BASE_HW_ISSUE_5736,
 	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3982,
 	BASE_HW_ISSUE_TMIX_7891,
 	BASE_HW_ISSUE_TMIX_7940,
 	BASE_HW_ISSUE_TMIX_8042,
@@ -992,21 +983,15 @@ static const enum base_hw_issue base_hw_issues_model_tMIx[] = {
 	BASE_HW_ISSUE_TMIX_8206,
 	BASE_HW_ISSUE_TMIX_8343,
 	BASE_HW_ISSUE_TMIX_8456,
+	GPUCORE_1619,
 	BASE_HW_ISSUE_END
 };
 
 static const enum base_hw_issue base_hw_issues_tHEx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_10682,
-	BASE_HW_ISSUE_TMIX_7891,
-	BASE_HW_ISSUE_TMIX_8042,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_END
-};
-
-static const enum base_hw_issue base_hw_issues_tHEx_r0p1[] = {
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_T76X_3700,
 	BASE_HW_ISSUE_TMIX_7891,
 	BASE_HW_ISSUE_TMIX_8042,
 	BASE_HW_ISSUE_TMIX_8133,
@@ -1016,83 +1001,14 @@ static const enum base_hw_issue base_hw_issues_tHEx_r0p1[] = {
 static const enum base_hw_issue base_hw_issues_model_tHEx[] = {
 	BASE_HW_ISSUE_5736,
 	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_T76X_3700,
 	BASE_HW_ISSUE_TMIX_7891,
 	BASE_HW_ISSUE_TMIX_8042,
 	BASE_HW_ISSUE_TMIX_8133,
+	GPUCORE_1619,
 	BASE_HW_ISSUE_END
 };
 
-static const enum base_hw_issue base_hw_issues_tSIx_r0p0[] = {
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-static const enum base_hw_issue base_hw_issues_tSIx_r0p1[] = {
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-static const enum base_hw_issue base_hw_issues_tSIx_r1p0[] = {
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-static const enum base_hw_issue base_hw_issues_model_tSIx[] = {
-	BASE_HW_ISSUE_5736,
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-
-
-#ifdef MALI_INCLUDE_TKAX
-static const enum base_hw_issue base_hw_issues_tKAx_r0p0[] = {
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-#endif /* MALI_INCLUDE_TKAX */
-
-#ifdef MALI_INCLUDE_TKAX
-static const enum base_hw_issue base_hw_issues_model_tKAx[] = {
-	BASE_HW_ISSUE_5736,
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-#endif /* MALI_INCLUDE_TKAX */
-
-#ifdef MALI_INCLUDE_TTRX
-static const enum base_hw_issue base_hw_issues_tTRx_r0p0[] = {
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
-
-#endif /* MALI_INCLUDE_TTRX */
-
-#ifdef MALI_INCLUDE_TTRX
-static const enum base_hw_issue base_hw_issues_model_tTRx[] = {
-	BASE_HW_ISSUE_5736,
-	BASE_HW_ISSUE_9435,
-	BASE_HW_ISSUE_TMIX_8133,
-	BASE_HW_ISSUE_TSIX_1116,
-	BASE_HW_ISSUE_END
-};
 
-#endif /* MALI_INCLUDE_TTRX */
 
 #endif /* _BASE_HWCONFIG_ISSUES_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_base_kernel.h b/drivers/gpu/arm/midgard/mali_base_kernel.h
index d5c8cbc..bcb05e4 100644
--- a/drivers/gpu/arm/midgard/mali_base_kernel.h
+++ b/drivers/gpu/arm/midgard/mali_base_kernel.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -118,44 +118,33 @@ typedef union kbase_pointer {
  */
 
 /**
- * typedef base_mem_alloc_flags - Memory allocation, access/hint flags.
+ * @brief Memory allocation, access/hint flags
  *
  * A combination of MEM_PROT/MEM_HINT flags must be passed to each allocator
  * in order to determine the best cache policy. Some combinations are
- * of course invalid (e.g. MEM_PROT_CPU_WR | MEM_HINT_CPU_RD),
- * which defines a write-only region on the CPU side, which is
+ * of course invalid (eg @c MEM_PROT_CPU_WR | @c MEM_HINT_CPU_RD),
+ * which defines a @a write-only region on the CPU side, which is
  * heavily read by the CPU...
  * Other flags are only meaningful to a particular allocator.
  * More flags can be added to this list, as long as they don't clash
- * (see BASE_MEM_FLAGS_NR_BITS for the number of the first free bit).
+ * (see ::BASE_MEM_FLAGS_NR_BITS for the number of the first free bit).
  */
 typedef u32 base_mem_alloc_flags;
 
-/* Memory allocation, access/hint flags.
+/**
+ * @brief Memory allocation, access/hint flags
+ *
+ * See ::base_mem_alloc_flags.
  *
- * See base_mem_alloc_flags.
  */
-
+enum {
 /* IN */
-/* Read access CPU side
- */
-#define BASE_MEM_PROT_CPU_RD ((base_mem_alloc_flags)1 << 0)
-
-/* Write access CPU side
- */
-#define BASE_MEM_PROT_CPU_WR ((base_mem_alloc_flags)1 << 1)
-
-/* Read access GPU side
- */
-#define BASE_MEM_PROT_GPU_RD ((base_mem_alloc_flags)1 << 2)
-
-/* Write access GPU side
- */
-#define BASE_MEM_PROT_GPU_WR ((base_mem_alloc_flags)1 << 3)
-
-/* Execute allowed on the GPU side
- */
-#define BASE_MEM_PROT_GPU_EX ((base_mem_alloc_flags)1 << 4)
+	BASE_MEM_PROT_CPU_RD = (1U << 0),      /**< Read access CPU side */
+	BASE_MEM_PROT_CPU_WR = (1U << 1),      /**< Write access CPU side */
+	BASE_MEM_PROT_GPU_RD = (1U << 2),      /**< Read access GPU side */
+	BASE_MEM_PROT_GPU_WR = (1U << 3),      /**< Write access GPU side */
+	BASE_MEM_PROT_GPU_EX = (1U << 4),      /**< Execute allowed on the GPU
+						    side */
 
 	/* BASE_MEM_HINT flags have been removed, but their values are reserved
 	 * for backwards compatibility with older user-space drivers. The values
@@ -168,66 +157,54 @@ typedef u32 base_mem_alloc_flags;
 	 * RESERVED: (1U << 8)
 	 */
 
-/* Grow backing store on GPU Page Fault
- */
-#define BASE_MEM_GROW_ON_GPF ((base_mem_alloc_flags)1 << 9)
-
-/* Page coherence Outer shareable, if available
- */
-#define BASE_MEM_COHERENT_SYSTEM ((base_mem_alloc_flags)1 << 10)
-
-/* Page coherence Inner shareable
- */
-#define BASE_MEM_COHERENT_LOCAL ((base_mem_alloc_flags)1 << 11)
+	BASE_MEM_GROW_ON_GPF = (1U << 9),      /**< Grow backing store on GPU
+						    Page Fault */
 
-/* Should be cached on the CPU
- */
-#define BASE_MEM_CACHED_CPU ((base_mem_alloc_flags)1 << 12)
+	BASE_MEM_COHERENT_SYSTEM = (1U << 10), /**< Page coherence Outer
+						    shareable, if available */
+	BASE_MEM_COHERENT_LOCAL = (1U << 11),  /**< Page coherence Inner
+						    shareable */
+	BASE_MEM_CACHED_CPU = (1U << 12),      /**< Should be cached on the
+						    CPU */
 
 /* IN/OUT */
-/* Must have same VA on both the GPU and the CPU
- */
-#define BASE_MEM_SAME_VA ((base_mem_alloc_flags)1 << 13)
-
+	BASE_MEM_SAME_VA = (1U << 13), /**< Must have same VA on both the GPU
+					    and the CPU */
 /* OUT */
-/* Must call mmap to acquire a GPU address for the alloc
- */
-#define BASE_MEM_NEED_MMAP ((base_mem_alloc_flags)1 << 14)
-
+	BASE_MEM_NEED_MMAP = (1U << 14), /**< Must call mmap to aquire a GPU
+					     address for the alloc */
 /* IN */
-/* Page coherence Outer shareable, required.
- */
-#define BASE_MEM_COHERENT_SYSTEM_REQUIRED ((base_mem_alloc_flags)1 << 15)
-
-/* Secure memory
- */
-#define BASE_MEM_SECURE ((base_mem_alloc_flags)1 << 16)
-
-/* Not needed physical memory
- */
-#define BASE_MEM_DONT_NEED ((base_mem_alloc_flags)1 << 17)
-
-/* Must use shared CPU/GPU zone (SAME_VA zone) but doesn't require the
- * addresses to be the same
- */
-#define BASE_MEM_IMPORT_SHARED ((base_mem_alloc_flags)1 << 18)
+	BASE_MEM_COHERENT_SYSTEM_REQUIRED = (1U << 15), /**< Page coherence
+					     Outer shareable, required. */
+	BASE_MEM_SECURE = (1U << 16),          /**< Secure memory */
+	BASE_MEM_DONT_NEED = (1U << 17),       /**< Not needed physical
+						    memory */
+	BASE_MEM_IMPORT_SHARED = (1U << 18),   /**< Must use shared CPU/GPU zone
+						    (SAME_VA zone) but doesn't
+						    require the addresses to
+						    be the same */
+};
 
-/* Number of bits used as flags for base memory management
+/**
+ * @brief Number of bits used as flags for base memory management
  *
- * Must be kept in sync with the base_mem_alloc_flags flags
+ * Must be kept in sync with the ::base_mem_alloc_flags flags
  */
 #define BASE_MEM_FLAGS_NR_BITS 19
 
-/* A mask for all output bits, excluding IN/OUT bits.
- */
+/**
+  * A mask for all output bits, excluding IN/OUT bits.
+  */
 #define BASE_MEM_FLAGS_OUTPUT_MASK BASE_MEM_NEED_MMAP
 
-/* A mask for all input bits, including IN/OUT bits.
- */
+/**
+  * A mask for all input bits, including IN/OUT bits.
+  */
 #define BASE_MEM_FLAGS_INPUT_MASK \
 	(((1 << BASE_MEM_FLAGS_NR_BITS) - 1) & ~BASE_MEM_FLAGS_OUTPUT_MASK)
 
-/* A mask for all the flags which are modifiable via the base_mem_set_flags
+/**
+ * A mask for all the flags which are modifiable via the base_mem_set_flags
  * interface.
  */
 #define BASE_MEM_FLAGS_MODIFIABLE \
@@ -317,6 +294,7 @@ struct base_mem_import_user_buffer {
  */
 typedef enum base_backing_threshold_status {
 	BASE_BACKING_THRESHOLD_OK = 0,			    /**< Resize successful */
+	BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE = -1,	    /**< Not a growable tmem object */
 	BASE_BACKING_THRESHOLD_ERROR_OOM = -2,		    /**< Increase failed due to an out-of-memory condition */
 	BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS = -4 /**< Invalid arguments (not tmem, illegal size request, etc.) */
 } base_backing_threshold_status;
@@ -385,7 +363,7 @@ typedef struct base_fence {
 /**
  * @brief Per-job data
  *
- * This structure is used to store per-job data, and is completely unused
+ * This structure is used to store per-job data, and is completly unused
  * by the Base driver. It can be used to store things such as callback
  * function pointer, data to handle job completion. It is guaranteed to be
  * untouched by the Base driver.
@@ -551,7 +529,7 @@ typedef u32 base_jd_core_req;
 /**
  * SW Only requirement : Replay job.
  *
- * If the preceding job fails, the replay job will cause the jobs specified in
+ * If the preceeding job fails, the replay job will cause the jobs specified in
  * the list of base_jd_replay_payload pointed to by the jc pointer to be
  * replayed.
  *
@@ -562,10 +540,10 @@ typedef u32 base_jd_core_req;
  * The replayed jobs will require a number of atom IDs. If there are not enough
  * free atom IDs then the replay job will fail.
  *
- * If the preceding job does not fail, then the replay job is returned as
+ * If the preceeding job does not fail, then the replay job is returned as
  * completed.
  *
- * The replayed jobs will never be returned to userspace. The preceding failed
+ * The replayed jobs will never be returned to userspace. The preceeding failed
  * job will be returned to userspace as failed; the status of this job should
  * be ignored. Completion should be determined by the status of the replay soft
  * job.
@@ -728,14 +706,6 @@ typedef u32 base_jd_core_req;
  */
 #define BASE_JD_REQ_SOFT_JOB_TYPE (BASE_JD_REQ_SOFT_JOB | 0x1f)
 
-/*
- * Returns non-zero value if core requirements passed define a soft job or
- * a dependency only job.
- */
-#define BASE_JD_REQ_SOFT_JOB_OR_DEP(core_req) \
-	((core_req & BASE_JD_REQ_SOFT_JOB) || \
-	(core_req & BASE_JD_REQ_ATOM_TYPE) == BASE_JD_REQ_DEP)
-
 /**
  * @brief States to model state machine processed by kbasep_js_job_check_ref_cores(), which
  * handles retaining cores for power management and affinity management.
@@ -752,7 +722,7 @@ typedef u32 base_jd_core_req;
  * -# the currently running atoms (which are causing the violation) to
  * finish
  * -# and, the atoms that had their affinity chosen during powerup to
- * finish. These are run preferentially because they don't cause a
+ * finish. These are run preferrentially because they don't cause a
  * violation, but instead continue to cause the violation in others.
  * -# or, the attacker is scheduled out (which might not happen for just 2
  * contexts)
@@ -958,7 +928,7 @@ static inline void base_jd_atom_dep_copy(struct base_dependency *dep,
  * be set to trigger when a GPU job has finished.
  *
  * The base fence object must not be terminated until the atom
- * has been submitted to @a base_jd_submit and @a base_jd_submit has returned.
+ * has been submitted to @a base_jd_submit_bag and @a base_jd_submit_bag has returned.
  *
  * @a fence must be a valid fence set up with @a base_fence_init.
  * Calling this function with a uninitialized fence results in undefined behavior.
@@ -987,7 +957,7 @@ static inline void base_jd_fence_trigger_setup_v2(struct base_jd_atom_v2 *atom,
  * be set to block a GPU job until it has been triggered.
  *
  * The base fence object must not be terminated until the atom
- * has been submitted to @a base_jd_submit and @a base_jd_submit has returned.
+ * has been submitted to @a base_jd_submit_bag and @a base_jd_submit_bag has returned.
  *
  * @a fence must be a valid fence set up with @a base_fence_init or @a base_fence_import.
  * Calling this function with a uninitialized fence results in undefined behavior.
@@ -1258,7 +1228,7 @@ typedef struct base_dump_cpu_gpu_counters {
  * has been configured correctly with the right set of Platform Config
  * Compile-time Properties.
  *
- * As a consistent guide across the entire DDK, the choice for dynamic or
+ * As a consistant guide across the entire DDK, the choice for dynamic or
  * compile-time should consider the following, in order:
  * -# Can the code be written so that it doesn't need to know the
  * implementation limits at all?
@@ -1423,9 +1393,9 @@ typedef struct base_dump_cpu_gpu_counters {
  * population count, since faulty cores may be disabled during production,
  * producing a non-contiguous mask.
  *
- * The memory requirements for this algorithm can be determined either by a u64
+ * The memory requirements for this algoirthm can be determined either by a u64
  * population count on the L2_PRESENT mask (a LUT helper already is
- * required for the above), or simple assumption that there can be no more than
+ * requried for the above), or simple assumption that there can be no more than
  * 16 coherent groups, since core groups are typically 4 cores.
  */
 
@@ -1451,9 +1421,9 @@ struct mali_base_gpu_core_props {
 
 	/**
 	 * Status of the GPU release.
-	 * No defined values, but starts at 0 and increases by one for each
-	 * release status (alpha, beta, EAC, etc.).
-	 * 4 bit values (0-15).
+     * No defined values, but starts at 0 and increases by one for each release
+     * status (alpha, beta, EAC, etc.).
+     * 4 bit values (0-15).
 	 */
 	u16 version_status;
 
@@ -1472,11 +1442,8 @@ struct mali_base_gpu_core_props {
 	u16 padding;
 
 	/**
-	 * This property is deprecated since it has not contained the real current
-	 * value of GPU clock speed. It is kept here only for backwards compatibility.
-	 * For the new ioctl interface, it is ignored and is treated as a padding
-	 * to keep the structure of the same size and retain the placement of its
-	 * members.
+	 * @usecase GPU clock speed is not specified in the Midgard Architecture, but is
+	 * <b>necessary for OpenCL's clGetDeviceInfo() function</b>.
 	 */
 	u32 gpu_speed_mhz;
 
@@ -1484,7 +1451,6 @@ struct mali_base_gpu_core_props {
 	 * @usecase GPU clock max/min speed is required for computing best/worst case
 	 * in tasks as job scheduling ant irq_throttling. (It is not specified in the
 	 *  Midgard Architecture).
-	 * Also, GPU clock max speed is used for OpenCL's clGetDeviceInfo() function.
 	 */
 	u32 gpu_freq_khz_max;
 	u32 gpu_freq_khz_min;
@@ -1623,7 +1589,7 @@ struct gpu_raw_gpu_props {
 	u64 shader_present;
 	u64 tiler_present;
 	u64 l2_present;
-	u64 stack_present;
+	u64 unused_1; /* keep for backward compatibility */
 
 	u32 l2_features;
 	u32 suspend_size; /* API 8.2+ */
@@ -1654,7 +1620,7 @@ struct gpu_raw_gpu_props {
 /**
  * Return structure for _mali_base_get_gpu_props().
  *
- * NOTE: the raw_props member in this data structure contains the register
+ * NOTE: the raw_props member in this datastructure contains the register
  * values from which the value of the other members are derived. The derived
  * members exist to allow for efficient access and/or shielding the details
  * of the layout of the registers.
@@ -1848,11 +1814,6 @@ typedef struct base_profiling_controls {
  * TL_ATOM_DONE, TL_ATOM_PRIO_CHANGE, TL_ATOM_EVENT_POST) */
 #define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1 << 0)
 
-/* Indicate that job dumping is enabled. This could affect certain timers
- * to account for the performance impact. */
-#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1 << 1)
-
-#define BASE_TLSTREAM_FLAGS_MASK (BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS | \
-		BASE_TLSTREAM_JOB_DUMPING_ENABLED)
+#define BASE_TLSTREAM_FLAGS_MASK (BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS)
 
 #endif				/* _BASE_KERNEL_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_base_kernel_sync.h b/drivers/gpu/arm/midgard/mali_base_kernel_sync.h
new file mode 100644
index 0000000..a24791f
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_base_kernel_sync.h
@@ -0,0 +1,47 @@
+/*
+ *
+ * (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file
+ * Base cross-proccess sync API.
+ */
+
+#ifndef _BASE_KERNEL_SYNC_H_
+#define _BASE_KERNEL_SYNC_H_
+
+#include <linux/ioctl.h>
+
+#define STREAM_IOC_MAGIC '~'
+
+/* Fence insert.
+ *
+ * Inserts a fence on the stream operated on.
+ * Fence can be waited via a base fence wait soft-job
+ * or triggered via a base fence trigger soft-job.
+ *
+ * Fences must be cleaned up with close when no longer needed.
+ *
+ * No input/output arguments.
+ * Returns
+ * >=0 fd
+ * <0  error code
+ */
+#define STREAM_IOC_FENCE_INSERT _IO(STREAM_IOC_MAGIC, 0)
+
+#endif				/* _BASE_KERNEL_SYNC_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase.h b/drivers/gpu/arm/midgard/mali_kbase.h
index baefe8a..443d4b1 100644
--- a/drivers/gpu/arm/midgard/mali_kbase.h
+++ b/drivers/gpu/arm/midgard/mali_kbase.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -65,14 +65,15 @@
 #include "mali_kbase_gpuprops.h"
 #include "mali_kbase_jm.h"
 #include "mali_kbase_vinstr.h"
-
-#include "ipa/mali_kbase_ipa.h"
-
+#include "mali_kbase_ipa.h"
 #ifdef CONFIG_GPU_TRACEPOINTS
 #include <trace/events/gpu.h>
 #endif
 /**
  * @page page_base_kernel_main Kernel-side Base (KBase) APIs
+ *
+ * The Kernel-side Base (KBase) APIs are divided up as follows:
+ * - @subpage page_kbase_js_policy
  */
 
 /**
@@ -104,27 +105,22 @@ void kbase_release_device(struct kbase_device *kbdev);
 
 void kbase_set_profiling_control(struct kbase_device *kbdev, u32 control, u32 value);
 
+u32 kbase_get_profiling_control(struct kbase_device *kbdev, u32 control);
+
 struct kbase_context *
 kbase_create_context(struct kbase_device *kbdev, bool is_compat);
 void kbase_destroy_context(struct kbase_context *kctx);
 
 int kbase_jd_init(struct kbase_context *kctx);
 void kbase_jd_exit(struct kbase_context *kctx);
-
-/**
- * kbase_jd_submit - Submit atoms to the job dispatcher
- *
- * @kctx: The kbase context to submit to
- * @user_addr: The address in user space of the struct base_jd_atom_v2 array
- * @nr_atoms: The number of atoms in the array
- * @stride: sizeof(struct base_jd_atom_v2)
- * @uk6_atom: true if the atoms are legacy atoms (struct base_jd_atom_v2_uk6)
- *
- * Return: 0 on success or error code
- */
+#ifdef BASE_LEGACY_UK6_SUPPORT
+int kbase_jd_submit(struct kbase_context *kctx,
+		const struct kbase_uk_job_submit *submit_data,
+		int uk6_atom);
+#else
 int kbase_jd_submit(struct kbase_context *kctx,
-		void __user *user_addr, u32 nr_atoms, u32 stride,
-		bool uk6_atom);
+		const struct kbase_uk_job_submit *submit_data);
+#endif
 
 /**
  * kbase_jd_done_worker - Handle a job completion
@@ -159,6 +155,8 @@ void kbase_jd_dep_clear_locked(struct kbase_jd_atom *katom);
 
 void kbase_job_done(struct kbase_device *kbdev, u32 done);
 
+void kbase_gpu_cacheclean(struct kbase_device *kbdev,
+					struct kbase_jd_atom *katom);
 /**
  * kbase_job_slot_ctx_priority_check_locked(): - Check for lower priority atoms
  *                                               and soft stop them
@@ -198,10 +196,8 @@ int kbase_prepare_soft_job(struct kbase_jd_atom *katom);
 void kbase_finish_soft_job(struct kbase_jd_atom *katom);
 void kbase_cancel_soft_job(struct kbase_jd_atom *katom);
 void kbase_resume_suspended_soft_jobs(struct kbase_device *kbdev);
+void kbasep_add_waiting_soft_job(struct kbase_jd_atom *katom);
 void kbasep_remove_waiting_soft_job(struct kbase_jd_atom *katom);
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-void kbase_soft_event_wait_callback(struct kbase_jd_atom *katom);
-#endif
 int kbase_soft_event_update(struct kbase_context *kctx,
 			    u64 event,
 			    unsigned char new_status);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_10969_workaround.c b/drivers/gpu/arm/midgard/mali_kbase_10969_workaround.c
index fde0f8f..933aa28 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_10969_workaround.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_10969_workaround.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2013-2015, 2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2015 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/midgard/mali_kbase_config_defaults.h b/drivers/gpu/arm/midgard/mali_kbase_config_defaults.h
index 1cf44b3..e674cc2 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_config_defaults.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_config_defaults.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2013-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,6 +29,54 @@
 #include <mali_kbase_config_platform.h>
 
 /**
+ * Irq throttle. It is the minimum desired time in between two
+ * consecutive gpu interrupts (given in 'us'). The irq throttle
+ * gpu register will be configured after this, taking into
+ * account the configured max frequency.
+ *
+ * Attached value: number in micro seconds
+ */
+#define DEFAULT_IRQ_THROTTLE_TIME_US 20
+
+/**
+ *  Default Job Scheduler initial runtime of a context for the CFS Policy,
+ *  in time-slices.
+ *
+ * This value is relative to that of the least-run context, and defines
+ * where in the CFS queue a new context is added. A value of 1 means 'after
+ * the least-run context has used its timeslice'. Therefore, when all
+ * contexts consistently use the same amount of time, a value of 1 models a
+ * FIFO. A value of 0 would model a LIFO.
+ *
+ * The value is represented in "numbers of time slices". Multiply this
+ * value by that defined in @ref DEFAULT_JS_CTX_TIMESLICE_NS to get
+ * the time value for this in nanoseconds.
+ */
+#define DEFAULT_JS_CFS_CTX_RUNTIME_INIT_SLICES 1
+
+/**
+ * Default Job Scheduler minimum runtime value of a context for CFS, in
+ * time_slices relative to that of the least-run context.
+ *
+ * This is a measure of how much preferrential treatment is given to a
+ * context that is not run very often.
+ *
+ * Specficially, this value defines how many timeslices such a context is
+ * (initially) allowed to use at once. Such contexts (e.g. 'interactive'
+ * processes) will appear near the front of the CFS queue, and can initially
+ * use more time than contexts that run continuously (e.g. 'batch'
+ * processes).
+ *
+ * This limit \b prevents a "stored-up timeslices" DoS attack, where a ctx
+ * not run for a long time attacks the system by using a very large initial
+ * number of timeslices when it finally does run.
+ *
+ * @note A value of zero allows not-run-often contexts to get scheduled in
+ * quickly, but to only use a single timeslice when they get scheduled in.
+ */
+#define DEFAULT_JS_CFS_CTX_RUNTIME_MIN_SLICES 2
+
+/**
 * Boolean indicating whether the driver is configured to be secure at
 * a potential loss of performance.
 *
@@ -153,13 +201,13 @@ enum {
 /*
  * Default minimum number of scheduling ticks before jobs are hard-stopped
  */
-#define DEFAULT_JS_HARD_STOP_TICKS_SS    (50) /* 5s */
+#define DEFAULT_JS_HARD_STOP_TICKS_SS    (100) /* 10s */
 #define DEFAULT_JS_HARD_STOP_TICKS_SS_8408  (300) /* 30s */
 
 /*
  * Default minimum number of scheduling ticks before CL jobs are hard-stopped.
  */
-#define DEFAULT_JS_HARD_STOP_TICKS_CL    (50) /* 5s */
+#define DEFAULT_JS_HARD_STOP_TICKS_CL    (100) /* 10s */
 
 /*
  * Default minimum number of scheduling ticks before jobs are hard-stopped
@@ -171,20 +219,20 @@ enum {
  * Default timeout for some software jobs, after which the software event wait
  * jobs will be cancelled.
  */
-#define DEFAULT_JS_SOFT_JOB_TIMEOUT (3000) /* 3s */
+#define DEFAULT_JS_SOFT_JOB_TIMEOUT ((u32)3000) /* 3s */
 
 /*
  * Default minimum number of scheduling ticks before the GPU is reset to clear a
  * "stuck" job
  */
-#define DEFAULT_JS_RESET_TICKS_SS           (55) /* 5.5s */
+#define DEFAULT_JS_RESET_TICKS_SS           (105) /* 10.5s */
 #define DEFAULT_JS_RESET_TICKS_SS_8408     (450) /* 45s */
 
 /*
  * Default minimum number of scheduling ticks before the GPU is reset to clear a
  * "stuck" CL job.
  */
-#define DEFAULT_JS_RESET_TICKS_CL        (55) /* 5.5s */
+#define DEFAULT_JS_RESET_TICKS_CL        (105) /* 10.5s */
 
 /*
  * Default minimum number of scheduling ticks before the GPU is reset to clear a
@@ -209,19 +257,5 @@ enum {
  */
 #define DEFAULT_JS_CTX_TIMESLICE_NS (50000000) /* 50ms */
 
-/*
- * Perform GPU power down using only platform specific code, skipping DDK power
- * management.
- *
- * If this is non-zero then kbase will avoid powering down shader cores, the
- * tiler, and the L2 cache, instead just powering down the entire GPU through
- * platform specific code. This may be required for certain platform
- * integrations.
- *
- * Note that as this prevents kbase from powering down shader cores, this limits
- * the available power policies to coarse_demand and always_on.
- */
-#define PLATFORM_POWER_DOWN_ONLY (1)
-
 #endif /* _KBASE_CONFIG_DEFAULTS_H_ */
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_context.c b/drivers/gpu/arm/midgard/mali_kbase_context.c
index 4e802bc..55c5ef6 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_context.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_context.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -24,8 +24,6 @@
 #include <mali_kbase.h>
 #include <mali_midg_regmap.h>
 #include <mali_kbase_mem_linux.h>
-#include <mali_kbase_dma_fence.h>
-#include <mali_kbase_ctx_sched.h>
 
 /**
  * kbase_create_context() - Create a kernel base context.
@@ -55,7 +53,6 @@ kbase_create_context(struct kbase_device *kbdev, bool is_compat)
 
 	kctx->kbdev = kbdev;
 	kctx->as_nr = KBASEP_AS_NR_INVALID;
-	atomic_set(&kctx->refcount, 0);
 	if (is_compat)
 		kbase_ctx_flag_set(kctx, KCTX_COMPAT);
 #ifdef CONFIG_MALI_TRACE_TIMELINE
@@ -116,10 +113,7 @@ kbase_create_context(struct kbase_device *kbdev, bool is_compat)
 				MIDGARD_MMU_BOTTOMLEVEL);
 		if (err)
 			goto pgd_no_mem;
-
-		mutex_lock(&kctx->mmu_lock);
 		kctx->pgd = kbase_mmu_alloc_pgd(kctx);
-		mutex_unlock(&kctx->mmu_lock);
 	} while (!kctx->pgd);
 
 	kctx->aliasing_sink_page = kbase_mem_alloc_page(kctx->kbdev);
@@ -213,7 +207,6 @@ void kbase_destroy_context(struct kbase_context *kctx)
 	struct kbase_device *kbdev;
 	int pages;
 	unsigned long pending_regions_to_clean;
-	unsigned long flags;
 
 	KBASE_DEBUG_ASSERT(NULL != kctx);
 
@@ -228,14 +221,6 @@ void kbase_destroy_context(struct kbase_context *kctx)
 	kbase_pm_context_active(kbdev);
 
 	kbase_jd_zap_context(kctx);
-
-#ifdef CONFIG_DEBUG_FS
-	/* Removing the rest of the debugfs entries here as we want to keep the
-	 * atom debugfs interface alive until all atoms have completed. This
-	 * is useful for debugging hung contexts. */
-	debugfs_remove_recursive(kctx->kctx_dentry);
-#endif
-
 	kbase_event_cleanup(kctx);
 
 	/*
@@ -281,12 +266,6 @@ void kbase_destroy_context(struct kbase_context *kctx)
 
 	kbase_dma_fence_term(kctx);
 
-	mutex_lock(&kbdev->mmu_hw_mutex);
-	spin_lock_irqsave(&kctx->kbdev->hwaccess_lock, flags);
-	kbase_ctx_sched_remove_ctx(kctx);
-	spin_unlock_irqrestore(&kctx->kbdev->hwaccess_lock, flags);
-	mutex_unlock(&kbdev->mmu_hw_mutex);
-
 	kbase_mmu_term(kctx);
 
 	pages = atomic_read(&kctx->used_pages);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_core_linux.c b/drivers/gpu/arm/midgard/mali_kbase_core_linux.c
index 65b2045..582bed3 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_core_linux.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_core_linux.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,11 +23,7 @@
 #include <mali_kbase_gator.h>
 #include <mali_kbase_mem_linux.h>
 #ifdef CONFIG_MALI_DEVFREQ
-#include <linux/devfreq.h>
 #include <backend/gpu/mali_kbase_devfreq.h>
-#ifdef CONFIG_DEVFREQ_THERMAL
-#include <ipa/mali_kbase_ipa_debugfs.h>
-#endif /* CONFIG_DEVFREQ_THERMAL */
 #endif /* CONFIG_MALI_DEVFREQ */
 #ifdef CONFIG_MALI_NO_MALI
 #include "mali_kbase_model_linux.h"
@@ -42,9 +38,7 @@
 #include "mali_kbase_regs_history_debugfs.h"
 #include <mali_kbase_hwaccess_backend.h>
 #include <mali_kbase_hwaccess_jm.h>
-#include <mali_kbase_ctx_sched.h>
 #include <backend/gpu/mali_kbase_device_internal.h>
-#include "mali_kbase_ioctl.h"
 
 #ifdef CONFIG_KDS
 #include <linux/kds.h>
@@ -60,7 +54,6 @@
 #include <linux/errno.h>
 #include <linux/of.h>
 #include <linux/platform_device.h>
-#include <linux/of_platform.h>
 #include <linux/miscdevice.h>
 #include <linux/list.h>
 #include <linux/semaphore.h>
@@ -79,9 +72,12 @@
 #ifdef CONFIG_MALI_PLATFORM_FAKE
 #include <platform/mali_kbase_platform_fake.h>
 #endif /*CONFIG_MALI_PLATFORM_FAKE */
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 #include <mali_kbase_sync.h>
-#endif /* CONFIG_SYNC || CONFIG_SYNC_FILE */
+#endif /* CONFIG_SYNC */
+#ifdef CONFIG_PM_DEVFREQ
+#include <linux/devfreq.h>
+#endif /* CONFIG_PM_DEVFREQ */
 #include <linux/clk.h>
 #include <linux/delay.h>
 
@@ -122,8 +118,60 @@ static inline void __compile_time_asserts(void)
 	CSTD_COMPILE_TIME_ASSERT(sizeof(KERNEL_SIDE_DDK_VERSION_STRING) <= KBASE_GET_VERSION_BUFFER_SIZE);
 }
 
-static int kbase_api_handshake(struct kbase_context *kctx,
-		struct kbase_ioctl_version_check *version)
+static void kbase_create_timeline_objects(struct kbase_context *kctx)
+{
+	struct kbase_device             *kbdev = kctx->kbdev;
+	unsigned int                    lpu_id;
+	unsigned int                    as_nr;
+	struct kbasep_kctx_list_element *element;
+
+	/* Create LPU objects. */
+	for (lpu_id = 0; lpu_id < kbdev->gpu_props.num_job_slots; lpu_id++) {
+		u32 *lpu =
+			&kbdev->gpu_props.props.raw_props.js_features[lpu_id];
+		kbase_tlstream_tl_summary_new_lpu(lpu, lpu_id, *lpu);
+	}
+
+	/* Create Address Space objects. */
+	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
+		kbase_tlstream_tl_summary_new_as(&kbdev->as[as_nr], as_nr);
+
+	/* Create GPU object and make it retain all LPUs and address spaces. */
+	kbase_tlstream_tl_summary_new_gpu(
+			kbdev,
+			kbdev->gpu_props.props.raw_props.gpu_id,
+			kbdev->gpu_props.num_cores);
+
+	for (lpu_id = 0; lpu_id < kbdev->gpu_props.num_job_slots; lpu_id++) {
+		void *lpu =
+			&kbdev->gpu_props.props.raw_props.js_features[lpu_id];
+		kbase_tlstream_tl_summary_lifelink_lpu_gpu(lpu, kbdev);
+	}
+	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
+		kbase_tlstream_tl_summary_lifelink_as_gpu(
+				&kbdev->as[as_nr],
+				kbdev);
+
+	/* Create object for each known context. */
+	mutex_lock(&kbdev->kctx_list_lock);
+	list_for_each_entry(element, &kbdev->kctx_list, link) {
+		kbase_tlstream_tl_summary_new_ctx(
+				element->kctx,
+				(u32)(element->kctx->id),
+				(u32)(element->kctx->tgid));
+	}
+	/* Before releasing the lock, reset body stream buffers.
+	 * This will prevent context creation message to be directed to both
+	 * summary and body stream. */
+	kbase_tlstream_reset_body_streams();
+	mutex_unlock(&kbdev->kctx_list_lock);
+	/* Static object are placed into summary packet that needs to be
+	 * transmitted first. Flush all streams to make it available to
+	 * user space. */
+	kbase_tlstream_flush_streams();
+}
+
+static void kbase_api_handshake(struct uku_version_check_args *version)
 {
 	switch (version->major) {
 #ifdef BASE_LEGACY_UK6_SUPPORT
@@ -172,11 +220,6 @@ static int kbase_api_handshake(struct kbase_context *kctx,
 		version->minor = BASE_UK_VERSION_MINOR;
 		break;
 	}
-
-	/* save the proposed version number for later use */
-	kctx->api_version = KBASE_API_VERSION(version->major, version->minor);
-
-	return 0;
 }
 
 /**
@@ -185,11 +228,6 @@ static int kbase_api_handshake(struct kbase_context *kctx,
  * This is subset of those common Mali errors that can be returned to userspace.
  * Values of matching user and kernel space enumerators MUST be the same.
  * MALI_ERROR_NONE is guaranteed to be 0.
- *
- * @MALI_ERROR_NONE: Success
- * @MALI_ERROR_OUT_OF_GPU_MEMORY: Not used in the kernel driver
- * @MALI_ERROR_OUT_OF_MEMORY: Memory allocation failure
- * @MALI_ERROR_FUNCTION_FAILED: Generic error code
  */
 enum mali_error {
 	MALI_ERROR_NONE = 0,
@@ -210,20 +248,20 @@ enum {
 	inited_backend_late = (1u << 6),
 	inited_device = (1u << 7),
 	inited_vinstr = (1u << 8),
-
+#ifndef CONFIG_MALI_PRFCNT_SET_SECONDARY
+	inited_ipa = (1u << 9),
+#endif /* CONFIG_MALI_PRFCNT_SET_SECONDARY */
 	inited_job_fault = (1u << 10),
-	inited_sysfs_group = (1u << 11),
-	inited_misc_register = (1u << 12),
-	inited_get_device = (1u << 13),
+	inited_misc_register = (1u << 11),
+	inited_get_device = (1u << 12),
+	inited_sysfs_group = (1u << 13),
 	inited_dev_list = (1u << 14),
 	inited_debugfs = (1u << 15),
 	inited_gpu_device = (1u << 16),
 	inited_registers_map = (1u << 17),
 	inited_io_history = (1u << 18),
 	inited_power_control = (1u << 19),
-	inited_buslogger = (1u << 20),
-	inited_protected = (1u << 21),
-	inited_ctx_sched = (1u << 22)
+	inited_buslogger = (1u << 20)
 };
 
 
@@ -242,18 +280,7 @@ void kbase_set_driver_inactive(struct kbase_device *kbdev, bool inactive)
 KBASE_EXPORT_TEST_API(kbase_set_driver_inactive);
 #endif /* CONFIG_MALI_DEBUG */
 
-/**
- * kbase_legacy_dispatch - UKK dispatch function
- *
- * This is the dispatch function for the legacy UKK ioctl interface. No new
- * ioctls should be added to this function, see kbase_ioctl instead.
- *
- * @kctx: The kernel context structure
- * @args: Pointer to the data structure passed from/to user space
- * @args_size: Size of the data structure
- */
-static int kbase_legacy_dispatch(struct kbase_context *kctx,
-		void * const args, u32 args_size)
+static int kbase_dispatch(struct kbase_context *kctx, void * const args, u32 args_size)
 {
 	struct kbase_device *kbdev;
 	union uk_header *ukh = args;
@@ -273,20 +300,16 @@ static int kbase_legacy_dispatch(struct kbase_context *kctx,
 
 	if (UKP_FUNC_ID_CHECK_VERSION == id) {
 		struct uku_version_check_args *version_check;
-		struct kbase_ioctl_version_check version;
 
 		if (args_size != sizeof(struct uku_version_check_args)) {
 			ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			return 0;
 		}
 		version_check = (struct uku_version_check_args *)args;
-		version.minor = version_check->minor;
-		version.major = version_check->major;
-
-		kbase_api_handshake(kctx, &version);
-
-		version_check->minor = version.minor;
-		version_check->major = version.major;
+		kbase_api_handshake(version_check);
+		/* save the proposed version number for later use */
+		kctx->api_version = KBASE_API_VERSION(version_check->major,
+				version_check->minor);
 		ukh->ret = MALI_ERROR_NONE;
 		return 0;
 	}
@@ -356,9 +379,8 @@ static int kbase_legacy_dispatch(struct kbase_context *kctx,
 
 			reg = kbase_mem_alloc(kctx, mem->va_pages,
 					mem->commit_pages, mem->extent,
-					&mem->flags, &mem->gpu_va);
-			mem->va_alignment = 0;
-
+					&mem->flags, &mem->gpu_va,
+					&mem->va_alignment);
 			if (!reg)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			break;
@@ -385,7 +407,6 @@ static int kbase_legacy_dispatch(struct kbase_context *kctx,
 					(enum base_mem_import_type)
 					mem_import->type,
 					phandle,
-					0,
 					&mem_import->gpu_va,
 					&mem_import->va_pages,
 					&mem_import->flags)) {
@@ -447,27 +468,22 @@ copy_failed:
 	case KBASE_FUNC_MEM_COMMIT:
 		{
 			struct kbase_uk_mem_commit *commit = args;
-			int ret;
 
 			if (sizeof(*commit) != args_size)
 				goto bad_size;
 
-			ret = kbase_mem_commit(kctx, commit->gpu_addr,
-					commit->pages);
-
-			ukh->ret = MALI_ERROR_FUNCTION_FAILED;
-			commit->result_subcode =
-				BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS;
-
-			if (ret == 0) {
-				ukh->ret = MALI_ERROR_NONE;
-				commit->result_subcode =
-					BASE_BACKING_THRESHOLD_OK;
-			} else if (ret == -ENOMEM) {
-				commit->result_subcode =
-					BASE_BACKING_THRESHOLD_ERROR_OOM;
+			if (commit->gpu_addr & ~PAGE_MASK) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_COMMIT: commit->gpu_addr: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
 			}
 
+			if (kbase_mem_commit(kctx, commit->gpu_addr,
+					commit->pages,
+					(base_backing_threshold_status *)
+					&commit->result_subcode) != 0)
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+
 			break;
 		}
 
@@ -478,6 +494,19 @@ copy_failed:
 			if (sizeof(*query) != args_size)
 				goto bad_size;
 
+			if (query->gpu_addr & ~PAGE_MASK) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_QUERY: query->gpu_addr: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+			if (query->query != KBASE_MEM_QUERY_COMMIT_SIZE &&
+			    query->query != KBASE_MEM_QUERY_VA_SIZE &&
+				query->query != KBASE_MEM_QUERY_FLAGS) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_QUERY: query->query = %lld unknown", (unsigned long long)query->query);
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
 			if (kbase_mem_query(kctx, query->gpu_addr,
 					query->query, &query->value) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
@@ -494,6 +523,12 @@ copy_failed:
 			if (sizeof(*fc) != args_size)
 				goto bad_size;
 
+			if ((fc->gpu_va & ~PAGE_MASK) && (fc->gpu_va >= PAGE_SIZE)) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_FLAGS_CHANGE: mem->gpu_va: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
 			if (kbase_mem_flags_change(kctx, fc->gpu_va,
 					fc->flags, fc->mask) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
@@ -507,6 +542,12 @@ copy_failed:
 			if (sizeof(*mem) != args_size)
 				goto bad_size;
 
+			if ((mem->gpu_addr & ~PAGE_MASK) && (mem->gpu_addr >= PAGE_SIZE)) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_FREE: mem->gpu_addr: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
 			if (kbase_mem_free(kctx, mem->gpu_addr) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			break;
@@ -519,10 +560,11 @@ copy_failed:
 			if (sizeof(*job) != args_size)
 				goto bad_size;
 
-			if (kbase_jd_submit(kctx, job->addr.value,
-						job->nr_atoms,
-						job->stride,
-						false) != 0)
+#ifdef BASE_LEGACY_UK6_SUPPORT
+			if (kbase_jd_submit(kctx, job, 0) != 0)
+#else
+			if (kbase_jd_submit(kctx, job) != 0)
+#endif /* BASE_LEGACY_UK6_SUPPORT */
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			break;
 		}
@@ -535,10 +577,7 @@ copy_failed:
 			if (sizeof(*job) != args_size)
 				goto bad_size;
 
-			if (kbase_jd_submit(kctx, job->addr.value,
-						job->nr_atoms,
-						job->stride,
-						true) != 0)
+			if (kbase_jd_submit(kctx, job, 1) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			break;
 		}
@@ -551,8 +590,14 @@ copy_failed:
 			if (sizeof(*sn) != args_size)
 				goto bad_size;
 
+			if (sn->sset.basep_sset.mem_handle.basep.handle & ~PAGE_MASK) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_SYNC: sn->sset.basep_sset.mem_handle: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
 #ifndef CONFIG_MALI_COH_USER
-			if (kbase_sync_now(kctx, &sn->sset.basep_sset) != 0)
+			if (kbase_sync_now(kctx, &sn->sset) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 #endif
 			break;
@@ -646,7 +691,7 @@ copy_failed:
 				goto bad_size;
 
 			if (find->gpu_addr & ~PAGE_MASK) {
-				dev_warn(kbdev->dev, "kbase_legacy_dispatch case KBASE_FUNC_FIND_CPU_OFFSET: find->gpu_addr: passed parameter is invalid");
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_FIND_CPU_OFFSET: find->gpu_addr: passed parameter is invalid");
 				goto out_bad;
 			}
 
@@ -657,8 +702,9 @@ copy_failed:
 
 				err = kbasep_find_enclosing_cpu_mapping_offset(
 						kctx,
-						find->cpu_addr,
-						find->size,
+						find->gpu_addr,
+						(uintptr_t) find->cpu_addr,
+						(size_t) find->size,
 						&find->offset);
 
 				if (err)
@@ -682,7 +728,7 @@ copy_failed:
 
 	case KBASE_FUNC_STREAM_CREATE:
 		{
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 			struct kbase_uk_stream_create *screate = (struct kbase_uk_stream_create *)args;
 
 			if (sizeof(*screate) != args_size)
@@ -694,29 +740,28 @@ copy_failed:
 				break;
 			}
 
-			if (kbase_sync_fence_stream_create(screate->name,
-							   &screate->fd) != 0)
+			if (kbase_stream_create(screate->name, &screate->fd) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			else
 				ukh->ret = MALI_ERROR_NONE;
-#else /* CONFIG_SYNC || CONFIG_SYNC_FILE */
+#else /* CONFIG_SYNC */
 			ukh->ret = MALI_ERROR_FUNCTION_FAILED;
-#endif /* CONFIG_SYNC || CONFIG_SYNC_FILE */
+#endif /* CONFIG_SYNC */
 			break;
 		}
 	case KBASE_FUNC_FENCE_VALIDATE:
 		{
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 			struct kbase_uk_fence_validate *fence_validate = (struct kbase_uk_fence_validate *)args;
 
 			if (sizeof(*fence_validate) != args_size)
 				goto bad_size;
 
-			if (kbase_sync_fence_validate(fence_validate->fd) != 0)
+			if (kbase_fence_validate(fence_validate->fd) != 0)
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			else
 				ukh->ret = MALI_ERROR_NONE;
-#endif /* CONFIG_SYNC || CONFIG_SYNC_FILE */
+#endif /* CONFIG_SYNC */
 			break;
 		}
 
@@ -773,7 +818,7 @@ copy_failed:
 #ifdef BASE_LEGACY_UK8_SUPPORT
 	case KBASE_FUNC_KEEP_GPU_POWERED:
 		{
-			dev_warn(kbdev->dev, "kbase_legacy_dispatch case KBASE_FUNC_KEEP_GPU_POWERED: function is deprecated and disabled\n");
+			dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_KEEP_GPU_POWERED: function is deprecated and disabled\n");
 			ukh->ret = MALI_ERROR_FUNCTION_FAILED;
 			break;
 		}
@@ -789,8 +834,7 @@ copy_failed:
 				goto bad_size;
 
 			for (i = FBDUMP_CONTROL_MIN; i < FBDUMP_CONTROL_MAX; i++)
-				controls->profiling_controls[i] =
-					kbdev->kbase_profiling_controls[i];
+				controls->profiling_controls[i] = kbase_get_profiling_control(kbdev, i);
 
 			break;
 		}
@@ -846,6 +890,7 @@ copy_failed:
 			if (kbasep_mem_profile_debugfs_insert(kctx, buf,
 							add_data->len)) {
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				kfree(buf);
 				goto out_bad;
 			}
 
@@ -869,17 +914,20 @@ copy_failed:
 		{
 			struct kbase_uk_tlstream_acquire_v10_4 *tlstream_acquire
 					= args;
-			int ret;
 
 			if (sizeof(*tlstream_acquire) != args_size)
 				goto bad_size;
 
-			ret = kbase_tlstream_acquire(
-						kctx, 0);
-			if (ret < 0)
+			if (0 != kbase_tlstream_acquire(
+						kctx,
+						&tlstream_acquire->fd, 0)) {
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
-			else
-				tlstream_acquire->fd = ret;
+			} else if (0 <= tlstream_acquire->fd) {
+				/* Summary stream was cleared during acquire.
+				 * Create static timeline objects that will be
+				 * read by client. */
+				kbase_create_timeline_objects(kctx);
+			}
 			break;
 		}
 #endif /* BASE_LEGACY_UK10_4_SUPPORT */
@@ -887,7 +935,6 @@ copy_failed:
 		{
 			struct kbase_uk_tlstream_acquire *tlstream_acquire =
 				args;
-			int ret;
 
 			if (sizeof(*tlstream_acquire) != args_size)
 				goto bad_size;
@@ -895,12 +942,17 @@ copy_failed:
 			if (tlstream_acquire->flags & ~BASE_TLSTREAM_FLAGS_MASK)
 				goto out_bad;
 
-			ret = kbase_tlstream_acquire(
-					kctx, tlstream_acquire->flags);
-			if (ret < 0)
+			if (0 != kbase_tlstream_acquire(
+						kctx,
+						&tlstream_acquire->fd,
+						tlstream_acquire->flags)) {
 				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
-			else
-				tlstream_acquire->fd = ret;
+			} else if (0 <= tlstream_acquire->fd) {
+				/* Summary stream was cleared during acquire.
+				 * Create static timeline objects that will be
+				 * read by client. */
+				kbase_create_timeline_objects(kctx);
+			}
 			break;
 		}
 	case KBASE_FUNC_TLSTREAM_FLUSH:
@@ -1008,11 +1060,11 @@ static int assign_irqs(struct platform_device *pdev)
 		}
 
 #ifdef CONFIG_OF
-		if (!strncmp(irq_res->name, "JOB", 4)) {
+		if (!strcmp(irq_res->name, "JOB")) {
 			irqtag = JOB_IRQ_TAG;
-		} else if (!strncmp(irq_res->name, "MMU", 4)) {
+		} else if (!strcmp(irq_res->name, "MMU")) {
 			irqtag = MMU_IRQ_TAG;
-		} else if (!strncmp(irq_res->name, "GPU", 4)) {
+		} else if (!strcmp(irq_res->name, "GPU")) {
 			irqtag = GPU_IRQ_TAG;
 		} else {
 			dev_err(&pdev->dev, "Invalid irq res name: '%s'\n",
@@ -1079,7 +1131,7 @@ EXPORT_SYMBOL(kbase_release_device);
 #if KERNEL_VERSION(4, 4, 0) > LINUX_VERSION_CODE
 /*
  * Older versions, before v4.6, of the kernel doesn't have
- * kstrtobool_from_user(), except longterm 4.4.y which had it added in 4.4.28
+ * kstrtobool_from_user().
  */
 static int kstrtobool_from_user(const char __user *s, size_t count, bool *res)
 {
@@ -1202,7 +1254,7 @@ static int kbase_open(struct inode *inode, struct file *filp)
 			mutex_lock(&kbdev->kctx_list_lock);
 			element->kctx = kctx;
 			list_add(&element->link, &kbdev->kctx_list);
-			KBASE_TLSTREAM_TL_NEW_CTX(
+			kbase_tlstream_tl_new_ctx(
 					element->kctx,
 					(u32)(element->kctx->id),
 					(u32)(element->kctx->tgid));
@@ -1226,9 +1278,10 @@ static int kbase_release(struct inode *inode, struct file *filp)
 	struct kbasep_kctx_list_element *element, *tmp;
 	bool found_element = false;
 
-	KBASE_TLSTREAM_TL_DEL_CTX(kctx);
+	kbase_tlstream_tl_del_ctx(kctx);
 
 #ifdef CONFIG_DEBUG_FS
+	debugfs_remove_recursive(kctx->kctx_dentry);
 	kbasep_mem_profile_debugfs_remove(kctx);
 	kbase_debug_job_fault_context_term(kctx);
 #endif
@@ -1257,634 +1310,39 @@ static int kbase_release(struct inode *inode, struct file *filp)
 		kbase_vinstr_legacy_hwc_setup(
 				kbdev->vinstr_ctx, &kctx->vinstr_cli, &setup);
 	}
-	mutex_unlock(&kctx->vinstr_cli_lock);
-
-	kbase_destroy_context(kctx);
-
-	dev_dbg(kbdev->dev, "deleted base context\n");
-	kbase_release_device(kbdev);
-	return 0;
-}
-
-#define CALL_MAX_SIZE 536
-
-static long kbase_legacy_ioctl(struct file *filp, unsigned int cmd,
-		unsigned long arg)
-{
-	u64 msg[(CALL_MAX_SIZE + 7) >> 3] = { 0xdeadbeefdeadbeefull };	/* alignment fixup */
-	u32 size = _IOC_SIZE(cmd);
-	struct kbase_context *kctx = filp->private_data;
-
-	if (size > CALL_MAX_SIZE)
-		return -ENOTTY;
-
-	if (0 != copy_from_user(&msg, (void __user *)arg, size)) {
-		dev_err(kctx->kbdev->dev, "failed to copy ioctl argument into kernel space\n");
-		return -EFAULT;
-	}
-
-	if (kbase_legacy_dispatch(kctx, &msg, size) != 0)
-		return -EFAULT;
-
-	if (0 != copy_to_user((void __user *)arg, &msg, size)) {
-		dev_err(kctx->kbdev->dev, "failed to copy results of UK call back to user space\n");
-		return -EFAULT;
-	}
-	return 0;
-}
-
-static int kbase_api_set_flags(struct kbase_context *kctx,
-		struct kbase_ioctl_set_flags *flags)
-{
-	int err;
-
-	/* setup pending, try to signal that we'll do the setup,
-	 * if setup was already in progress, err this call
-	 */
-	if (atomic_cmpxchg(&kctx->setup_in_progress, 0, 1) != 0)
-		return -EINVAL;
-
-	err = kbase_context_set_create_flags(kctx, flags->create_flags);
-	/* if bad flags, will stay stuck in setup mode */
-	if (err)
-		return err;
-
-	atomic_set(&kctx->setup_complete, 1);
-	return 0;
-}
-
-static int kbase_api_job_submit(struct kbase_context *kctx,
-		struct kbase_ioctl_job_submit *submit)
-{
-	return kbase_jd_submit(kctx, submit->addr.value, submit->nr_atoms,
-			submit->stride, false);
-}
-
-static int kbase_api_get_gpuprops(struct kbase_context *kctx,
-		struct kbase_ioctl_get_gpuprops *get_props)
-{
-	struct kbase_gpu_props *kprops = &kctx->kbdev->gpu_props;
-	int err;
-
-	if (get_props->flags != 0) {
-		dev_err(kctx->kbdev->dev, "Unsupported flags to get_gpuprops");
-		return -EINVAL;
-	}
-
-	if (get_props->size == 0)
-		return kprops->prop_buffer_size;
-	if (get_props->size < kprops->prop_buffer_size)
-		return -EINVAL;
-
-	err = copy_to_user(get_props->buffer.value, kprops->prop_buffer,
-			kprops->prop_buffer_size);
-	if (err)
-		return err;
-	return kprops->prop_buffer_size;
-}
-
-static int kbase_api_post_term(struct kbase_context *kctx)
-{
-	kbase_event_close(kctx);
-	return 0;
-}
-
-static int kbase_api_mem_alloc(struct kbase_context *kctx,
-		union kbase_ioctl_mem_alloc *alloc)
-{
-	struct kbase_va_region *reg;
-	u64 flags = alloc->in.flags;
-	u64 gpu_va;
-
-#if defined(CONFIG_64BIT)
-	if (!kbase_ctx_flag(kctx, KCTX_COMPAT)) {
-		/* force SAME_VA if a 64-bit client */
-		flags |= BASE_MEM_SAME_VA;
-	}
-#endif
-
-	reg = kbase_mem_alloc(kctx, alloc->in.va_pages,
-			alloc->in.commit_pages,
-			alloc->in.extent,
-			&flags, &gpu_va);
-
-	if (!reg)
-		return -ENOMEM;
-
-	alloc->out.flags = flags;
-	alloc->out.gpu_va = gpu_va;
-
-	return 0;
-}
-
-static int kbase_api_mem_query(struct kbase_context *kctx,
-		union kbase_ioctl_mem_query *query)
-{
-	return kbase_mem_query(kctx, query->in.gpu_addr,
-			query->in.query, &query->out.value);
-}
-
-static int kbase_api_mem_free(struct kbase_context *kctx,
-		struct kbase_ioctl_mem_free *free)
-{
-	return kbase_mem_free(kctx, free->gpu_addr);
-}
-
-static int kbase_api_hwcnt_reader_setup(struct kbase_context *kctx,
-		struct kbase_ioctl_hwcnt_reader_setup *setup)
-{
-	int ret;
-	struct kbase_uk_hwcnt_reader_setup args = {
-		.buffer_count = setup->buffer_count,
-		.jm_bm = setup->jm_bm,
-		.shader_bm = setup->shader_bm,
-		.tiler_bm = setup->tiler_bm,
-		.mmu_l2_bm = setup->mmu_l2_bm
-	};
-
-	mutex_lock(&kctx->vinstr_cli_lock);
-	ret = kbase_vinstr_hwcnt_reader_setup(kctx->kbdev->vinstr_ctx, &args);
-	mutex_unlock(&kctx->vinstr_cli_lock);
-
-	if (ret)
-		return ret;
-	return args.fd;
-}
-
-static int kbase_api_hwcnt_enable(struct kbase_context *kctx,
-		struct kbase_ioctl_hwcnt_enable *enable)
-{
-	int ret;
-	struct kbase_uk_hwcnt_setup args = {
-		.dump_buffer = enable->dump_buffer,
-		.jm_bm = enable->jm_bm,
-		.shader_bm = enable->shader_bm,
-		.tiler_bm = enable->tiler_bm,
-		.mmu_l2_bm = enable->mmu_l2_bm
-	};
-
-	mutex_lock(&kctx->vinstr_cli_lock);
-	ret = kbase_vinstr_legacy_hwc_setup(kctx->kbdev->vinstr_ctx,
-			&kctx->vinstr_cli, &args);
-	mutex_unlock(&kctx->vinstr_cli_lock);
-
-	return ret;
-}
-
-static int kbase_api_hwcnt_dump(struct kbase_context *kctx)
-{
-	int ret;
-
-	mutex_lock(&kctx->vinstr_cli_lock);
-	ret = kbase_vinstr_hwc_dump(kctx->vinstr_cli,
-			BASE_HWCNT_READER_EVENT_MANUAL);
-	mutex_unlock(&kctx->vinstr_cli_lock);
-
-	return ret;
-}
-
-static int kbase_api_hwcnt_clear(struct kbase_context *kctx)
-{
-	int ret;
-
-	mutex_lock(&kctx->vinstr_cli_lock);
-	ret = kbase_vinstr_hwc_clear(kctx->vinstr_cli);
-	mutex_unlock(&kctx->vinstr_cli_lock);
-
-	return ret;
-}
-
-static int kbase_api_disjoint_query(struct kbase_context *kctx,
-		struct kbase_ioctl_disjoint_query *query)
-{
-	query->counter = kbase_disjoint_event_get(kctx->kbdev);
-
-	return 0;
-}
-
-static int kbase_api_get_ddk_version(struct kbase_context *kctx,
-		struct kbase_ioctl_get_ddk_version *version)
-{
-	int ret;
-	int len = sizeof(KERNEL_SIDE_DDK_VERSION_STRING);
-
-	if (version->version_buffer.value == NULL)
-		return len;
-
-	if (version->size < len)
-		return -EOVERFLOW;
-
-	ret = copy_to_user(version->version_buffer.value,
-			KERNEL_SIDE_DDK_VERSION_STRING,
-			sizeof(KERNEL_SIDE_DDK_VERSION_STRING));
-
-	if (ret)
-		return ret;
-
-	return len;
-}
-
-static int kbase_api_mem_jit_init(struct kbase_context *kctx,
-		struct kbase_ioctl_mem_jit_init *jit_init)
-{
-	return kbase_region_tracker_init_jit(kctx, jit_init->va_pages);
-}
-
-static int kbase_api_mem_sync(struct kbase_context *kctx,
-		struct kbase_ioctl_mem_sync *sync)
-{
-#ifdef CONFIG_MALI_COH_USER
-	return 0;
-#endif
-	struct basep_syncset sset = {
-		.mem_handle.basep.handle = sync->handle,
-		.user_addr = sync->user_addr,
-		.size = sync->size,
-		.type = sync->type
-	};
-
-	return kbase_sync_now(kctx, &sset);
-}
-
-static int kbase_api_mem_find_cpu_offset(struct kbase_context *kctx,
-		union kbase_ioctl_mem_find_cpu_offset *find)
-{
-	return kbasep_find_enclosing_cpu_mapping_offset(
-			kctx,
-			find->in.cpu_addr,
-			find->in.size,
-			&find->out.offset);
-}
-
-static int kbase_api_get_context_id(struct kbase_context *kctx,
-		struct kbase_ioctl_get_context_id *info)
-{
-	info->id = kctx->id;
-
-	return 0;
-}
-
-static int kbase_api_tlstream_acquire(struct kbase_context *kctx,
-		struct kbase_ioctl_tlstream_acquire *acquire)
-{
-	return kbase_tlstream_acquire(kctx, acquire->flags);
-}
-
-static int kbase_api_tlstream_flush(struct kbase_context *kctx)
-{
-	kbase_tlstream_flush_streams();
-
-	return 0;
-}
-
-static int kbase_api_mem_commit(struct kbase_context *kctx,
-		struct kbase_ioctl_mem_commit *commit)
-{
-	return kbase_mem_commit(kctx, commit->gpu_addr, commit->pages);
-}
-
-static int kbase_api_mem_alias(struct kbase_context *kctx,
-		union kbase_ioctl_mem_alias *alias)
-{
-	struct base_mem_aliasing_info *ai;
-	u64 flags;
-	int err;
-
-	if (alias->in.nents == 0 || alias->in.nents > 2048)
-		return -EINVAL;
-
-	ai = vmalloc(sizeof(*ai) * alias->in.nents);
-	if (!ai)
-		return -ENOMEM;
-
-	err = copy_from_user(ai, alias->in.aliasing_info.value,
-			sizeof(*ai) * alias->in.nents);
-	if (err) {
-		vfree(ai);
-		return err;
-	}
-
-	flags = alias->in.flags;
-
-	alias->out.gpu_va = kbase_mem_alias(kctx, &flags,
-			alias->in.stride, alias->in.nents,
-			ai, &alias->out.va_pages);
-
-	alias->out.flags = flags;
-
-	vfree(ai);
-
-	if (alias->out.gpu_va == 0)
-		return -ENOMEM;
-
-	return 0;
-}
-
-static int kbase_api_mem_import(struct kbase_context *kctx,
-		union kbase_ioctl_mem_import *import)
-{
-	int ret;
-	u64 flags = import->in.flags;
-
-	ret = kbase_mem_import(kctx,
-			import->in.type,
-			import->in.phandle.value,
-			import->in.padding,
-			&import->out.gpu_va,
-			&import->out.va_pages,
-			&flags);
-
-	import->out.flags = flags;
-
-	return ret;
-}
-
-static int kbase_api_mem_flags_change(struct kbase_context *kctx,
-		struct kbase_ioctl_mem_flags_change *change)
-{
-	return kbase_mem_flags_change(kctx, change->gpu_va,
-			change->flags, change->mask);
-}
-
-static int kbase_api_stream_create(struct kbase_context *kctx,
-		struct kbase_ioctl_stream_create *stream)
-{
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-	int fd, ret;
-
-	/* Name must be NULL-terminated and padded with NULLs, so check last
-	 * character is NULL
-	 */
-	if (stream->name[sizeof(stream->name)-1] != 0)
-		return -EINVAL;
-
-	ret = kbase_sync_fence_stream_create(stream->name, &fd);
-
-	if (ret)
-		return ret;
-	return fd;
-#else
-	return -ENOENT;
-#endif
-}
-
-static int kbase_api_fence_validate(struct kbase_context *kctx,
-		struct kbase_ioctl_fence_validate *validate)
-{
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-	return kbase_sync_fence_validate(validate->fd);
-#else
-	return -ENOENT;
-#endif
-}
-
-static int kbase_api_get_profiling_controls(struct kbase_context *kctx,
-		struct kbase_ioctl_get_profiling_controls *controls)
-{
-	if (controls->count > FBDUMP_CONTROL_MAX)
-		return -EINVAL;
-
-	return copy_to_user(controls->buffer.value,
-			&kctx->kbdev->kbase_profiling_controls[
-				FBDUMP_CONTROL_MIN],
-			controls->count * sizeof(u32));
-}
-
-static int kbase_api_mem_profile_add(struct kbase_context *kctx,
-		struct kbase_ioctl_mem_profile_add *data)
-{
-	char *buf;
-	int err;
-
-	if (data->len > KBASE_MEM_PROFILE_MAX_BUF_SIZE) {
-		dev_err(kctx->kbdev->dev, "mem_profile_add: buffer too big\n");
-		return -EINVAL;
-	}
-
-	buf = kmalloc(data->len, GFP_KERNEL);
-	if (ZERO_OR_NULL_PTR(buf))
-		return -ENOMEM;
-
-	err = copy_from_user(buf, data->buffer.value, data->len);
-	if (err) {
-		kfree(buf);
-		return err;
-	}
-
-	return kbasep_mem_profile_debugfs_insert(kctx, buf, data->len);
-}
-
-static int kbase_api_soft_event_update(struct kbase_context *kctx,
-		struct kbase_ioctl_soft_event_update *update)
-{
-	if (update->flags != 0)
-		return -EINVAL;
-
-	return kbase_soft_event_update(kctx, update->event, update->new_status);
-}
-
-#if MALI_UNIT_TEST
-static int kbase_api_tlstream_test(struct kbase_context *kctx,
-		struct kbase_ioctl_tlstream_test *test)
-{
-	kbase_tlstream_test(
-			test->tpw_count,
-			test->msg_delay,
-			test->msg_count,
-			test->aux_msg);
-
-	return 0;
-}
+	mutex_unlock(&kctx->vinstr_cli_lock);
 
-static int kbase_api_tlstream_stats(struct kbase_context *kctx,
-		struct kbase_ioctl_tlstream_stats *stats)
-{
-	kbase_tlstream_stats(
-			&stats->bytes_collected,
-			&stats->bytes_generated);
+	kbase_destroy_context(kctx);
 
+	dev_dbg(kbdev->dev, "deleted base context\n");
+	kbase_release_device(kbdev);
 	return 0;
 }
-#endif /* MALI_UNIT_TEST */
-
-#define KBASE_HANDLE_IOCTL(cmd, function)                          \
-	case cmd:                                                  \
-	do {                                                       \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_NONE);          \
-		return function(kctx);                             \
-	} while (0)
 
-#define KBASE_HANDLE_IOCTL_IN(cmd, function, type)                 \
-	case cmd:                                                  \
-	do {                                                       \
-		type param;                                        \
-		int err;                                           \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_WRITE);         \
-		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));     \
-		err = copy_from_user(&param, uarg, sizeof(param)); \
-		if (err)                                           \
-			return -EFAULT;                            \
-		return function(kctx, &param);                     \
-	} while (0)
-
-#define KBASE_HANDLE_IOCTL_OUT(cmd, function, type)                \
-	case cmd:                                                  \
-	do {                                                       \
-		type param;                                        \
-		int ret, err;                                      \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_READ);          \
-		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));     \
-		ret = function(kctx, &param);                      \
-		err = copy_to_user(uarg, &param, sizeof(param));   \
-		if (err)                                           \
-			return -EFAULT;                            \
-		return ret;                                        \
-	} while (0)
-
-#define KBASE_HANDLE_IOCTL_INOUT(cmd, function, type)                  \
-	case cmd:                                                      \
-	do {                                                           \
-		type param;                                            \
-		int ret, err;                                          \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != (_IOC_WRITE|_IOC_READ)); \
-		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));         \
-		err = copy_from_user(&param, uarg, sizeof(param));     \
-		if (err)                                               \
-			return -EFAULT;                                \
-		ret = function(kctx, &param);                          \
-		err = copy_to_user(uarg, &param, sizeof(param));       \
-		if (err)                                               \
-			return -EFAULT;                                \
-		return ret;                                            \
-	} while (0)
+#define CALL_MAX_SIZE 536
 
 static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 {
+	u64 msg[(CALL_MAX_SIZE + 7) >> 3] = { 0xdeadbeefdeadbeefull };	/* alignment fixup */
+	u32 size = _IOC_SIZE(cmd);
 	struct kbase_context *kctx = filp->private_data;
-	struct kbase_device *kbdev = kctx->kbdev;
-	void __user *uarg = (void __user *)arg;
 
-	/* The UK ioctl values overflow the cmd field causing the type to be
-	 * incremented
-	 */
-	if (_IOC_TYPE(cmd) == LINUX_UK_BASE_MAGIC+2)
-		return kbase_legacy_ioctl(filp, cmd, arg);
+	if (size > CALL_MAX_SIZE)
+		return -ENOTTY;
 
-	/* The UK version check IOCTL doesn't overflow the cmd field, so is
-	 * handled separately here
-	 */
-	if (cmd == _IOC(_IOC_READ|_IOC_WRITE, LINUX_UK_BASE_MAGIC,
-				UKP_FUNC_ID_CHECK_VERSION,
-				sizeof(struct uku_version_check_args)))
-		return kbase_legacy_ioctl(filp, cmd, arg);
-
-	/* Only these ioctls are available until setup is complete */
-	switch (cmd) {
-		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_VERSION_CHECK,
-				kbase_api_handshake,
-				struct kbase_ioctl_version_check);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_SET_FLAGS,
-				kbase_api_set_flags,
-				struct kbase_ioctl_set_flags);
+	if (0 != copy_from_user(&msg, (void __user *)arg, size)) {
+		dev_err(kctx->kbdev->dev, "failed to copy ioctl argument into kernel space\n");
+		return -EFAULT;
 	}
 
-	/* Block call until version handshake and setup is complete */
-	if (kctx->api_version == 0 || !atomic_read(&kctx->setup_complete))
-		return -EINVAL;
-
-	/* Normal ioctls */
-	switch (cmd) {
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_JOB_SUBMIT,
-				kbase_api_job_submit,
-				struct kbase_ioctl_job_submit);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_GET_GPUPROPS,
-				kbase_api_get_gpuprops,
-				struct kbase_ioctl_get_gpuprops);
-		KBASE_HANDLE_IOCTL(KBASE_IOCTL_POST_TERM,
-				kbase_api_post_term);
-		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_MEM_ALLOC,
-				kbase_api_mem_alloc,
-				union kbase_ioctl_mem_alloc);
-		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_MEM_QUERY,
-				kbase_api_mem_query,
-				union kbase_ioctl_mem_query);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_MEM_FREE,
-				kbase_api_mem_free,
-				struct kbase_ioctl_mem_free);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_HWCNT_READER_SETUP,
-				kbase_api_hwcnt_reader_setup,
-				struct kbase_ioctl_hwcnt_reader_setup);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_HWCNT_ENABLE,
-				kbase_api_hwcnt_enable,
-				struct kbase_ioctl_hwcnt_enable);
-		KBASE_HANDLE_IOCTL(KBASE_IOCTL_HWCNT_DUMP,
-				kbase_api_hwcnt_dump);
-		KBASE_HANDLE_IOCTL(KBASE_IOCTL_HWCNT_CLEAR,
-				kbase_api_hwcnt_clear);
-		KBASE_HANDLE_IOCTL_OUT(KBASE_IOCTL_DISJOINT_QUERY,
-				kbase_api_disjoint_query,
-				struct kbase_ioctl_disjoint_query);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_GET_DDK_VERSION,
-				kbase_api_get_ddk_version,
-				struct kbase_ioctl_get_ddk_version);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_MEM_JIT_INIT,
-				kbase_api_mem_jit_init,
-				struct kbase_ioctl_mem_jit_init);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_MEM_SYNC,
-				kbase_api_mem_sync,
-				struct kbase_ioctl_mem_sync);
-		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_MEM_FIND_CPU_OFFSET,
-				kbase_api_mem_find_cpu_offset,
-				union kbase_ioctl_mem_find_cpu_offset);
-		KBASE_HANDLE_IOCTL_OUT(KBASE_IOCTL_GET_CONTEXT_ID,
-				kbase_api_get_context_id,
-				struct kbase_ioctl_get_context_id);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_TLSTREAM_ACQUIRE,
-				kbase_api_tlstream_acquire,
-				struct kbase_ioctl_tlstream_acquire);
-		KBASE_HANDLE_IOCTL(KBASE_IOCTL_TLSTREAM_FLUSH,
-				kbase_api_tlstream_flush);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_MEM_COMMIT,
-				kbase_api_mem_commit,
-				struct kbase_ioctl_mem_commit);
-		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_MEM_ALIAS,
-				kbase_api_mem_alias,
-				union kbase_ioctl_mem_alias);
-		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_MEM_IMPORT,
-				kbase_api_mem_import,
-				union kbase_ioctl_mem_import);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_MEM_FLAGS_CHANGE,
-				kbase_api_mem_flags_change,
-				struct kbase_ioctl_mem_flags_change);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_STREAM_CREATE,
-				kbase_api_stream_create,
-				struct kbase_ioctl_stream_create);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_FENCE_VALIDATE,
-				kbase_api_fence_validate,
-				struct kbase_ioctl_fence_validate);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_GET_PROFILING_CONTROLS,
-				kbase_api_get_profiling_controls,
-				struct kbase_ioctl_get_profiling_controls);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_MEM_PROFILE_ADD,
-				kbase_api_mem_profile_add,
-				struct kbase_ioctl_mem_profile_add);
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_SOFT_EVENT_UPDATE,
-				kbase_api_soft_event_update,
-				struct kbase_ioctl_soft_event_update);
+	if (kbase_dispatch(kctx, &msg, size) != 0)
+		return -EFAULT;
 
-#if MALI_UNIT_TEST
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_TLSTREAM_TEST,
-				kbase_api_tlstream_test,
-				struct kbase_ioctl_tlstream_test);
-		KBASE_HANDLE_IOCTL_OUT(KBASE_IOCTL_TLSTREAM_STATS,
-				kbase_api_tlstream_stats,
-				struct kbase_ioctl_tlstream_stats);
-#endif
+	if (0 != copy_to_user((void __user *)arg, &msg, size)) {
+		dev_err(kctx->kbdev->dev, "failed to copy results of UK call back to user space\n");
+		return -EFAULT;
 	}
-
-	dev_warn(kbdev->dev, "Unknown ioctl 0x%x nr:%d", cmd, _IOC_NR(cmd));
-
-	return -ENOIOCTLCMD;
+	return 0;
 }
 
 static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
@@ -1957,56 +1415,13 @@ static int kbase_check_flags(int flags)
 	return 0;
 }
 
-
-/**
- * align_and_check - Align the specified pointer to the provided alignment and
- *                   check that it is still in range.
- * @gap_end:        Highest possible start address for allocation (end of gap in
- *                  address space)
- * @gap_start:      Start address of current memory area / gap in address space
- * @info:           vm_unmapped_area_info structure passed to caller, containing
- *                  alignment, length and limits for the allocation
- * @is_shader_code: True if the allocation is for shader code (which has
- *                  additional alignment requirements)
- *
- * Return: true if gap_end is now aligned correctly and is still in range,
- *         false otherwise
- */
-static bool align_and_check(unsigned long *gap_end, unsigned long gap_start,
-		struct vm_unmapped_area_info *info, bool is_shader_code)
-{
-	/* Compute highest gap address at the desired alignment */
-	(*gap_end) -= info->length;
-	(*gap_end) -= (*gap_end - info->align_offset) & info->align_mask;
-
-	if (is_shader_code) {
-		/* Check for 4GB boundary */
-		if (0 == (*gap_end & BASE_MEM_MASK_4GB))
-			(*gap_end) -= (info->align_offset ? info->align_offset :
-					info->length);
-		if (0 == ((*gap_end + info->length) & BASE_MEM_MASK_4GB))
-			(*gap_end) -= (info->align_offset ? info->align_offset :
-					info->length);
-
-		if (!(*gap_end & BASE_MEM_MASK_4GB) || !((*gap_end +
-				info->length) & BASE_MEM_MASK_4GB))
-			return false;
-	}
-
-
-	if ((*gap_end < info->low_limit) || (*gap_end < gap_start))
-		return false;
-
-
-	return true;
-}
-
+#ifdef CONFIG_64BIT
 /* The following function is taken from the kernel and just
  * renamed. As it's not exported to modules we must copy-paste it here.
  */
 
 static unsigned long kbase_unmapped_area_topdown(struct vm_unmapped_area_info
-		*info, bool is_shader_code)
+		*info)
 {
 	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma;
@@ -2032,10 +1447,8 @@ static unsigned long kbase_unmapped_area_topdown(struct vm_unmapped_area_info
 
 	/* Check highest gap, which does not precede any rbtree node */
 	gap_start = mm->highest_vm_end;
-	if (gap_start <= high_limit) {
-		if (align_and_check(&gap_end, gap_start, info, is_shader_code))
-			return gap_end;
-	}
+	if (gap_start <= high_limit)
+		goto found_highest;
 
 	/* Check if rbtree root looks promising */
 	if (RB_EMPTY_ROOT(&mm->mm_rb))
@@ -2062,16 +1475,8 @@ check_current:
 		gap_end = vma->vm_start;
 		if (gap_end < low_limit)
 			return -ENOMEM;
-		if (gap_start <= high_limit && gap_end - gap_start >= length) {
-			/* We found a suitable gap. Clip it with the original
-			 * high_limit. */
-			if (gap_end > info->high_limit)
-				gap_end = info->high_limit;
-
-			if (align_and_check(&gap_end, gap_start, info,
-					is_shader_code))
-				return gap_end;
-		}
+		if (gap_start <= high_limit && gap_end - gap_start >= length)
+			goto found;
 
 		/* Visit left subtree if it looks promising */
 		if (vma->vm_rb.rb_left) {
@@ -2099,9 +1504,22 @@ check_current:
 		}
 	}
 
-	return -ENOMEM;
+found:
+	/* We found a suitable gap. Clip it with the original high_limit. */
+	if (gap_end > info->high_limit)
+		gap_end = info->high_limit;
+
+found_highest:
+	/* Compute highest gap address at the desired alignment */
+	gap_end -= info->length;
+	gap_end -= (gap_end - info->align_offset) & info->align_mask;
+
+	VM_BUG_ON(gap_end < info->low_limit);
+	VM_BUG_ON(gap_end < gap_start);
+	return gap_end;
 }
 
+
 static unsigned long kbase_get_unmapped_area(struct file *filp,
 		const unsigned long addr, const unsigned long len,
 		const unsigned long pgoff, const unsigned long flags)
@@ -2111,87 +1529,41 @@ static unsigned long kbase_get_unmapped_area(struct file *filp,
 	struct kbase_context *kctx = filp->private_data;
 	struct mm_struct *mm = current->mm;
 	struct vm_unmapped_area_info info;
-	unsigned long align_offset = 0;
-	unsigned long align_mask = 0;
-	unsigned long high_limit = mm->mmap_base;
-	unsigned long low_limit = PAGE_SIZE;
-	int cpu_va_bits = BITS_PER_LONG;
-	int gpu_pc_bits =
-	      kctx->kbdev->gpu_props.props.core_props.log2_program_counter_size;
-	bool is_shader_code = false;
-	unsigned long ret;
 
 	/* err on fixed address */
 	if ((flags & MAP_FIXED) || addr)
 		return -EINVAL;
 
-#ifdef CONFIG_64BIT
 	/* too big? */
 	if (len > TASK_SIZE - SZ_2M)
 		return -ENOMEM;
 
-	if (!kbase_ctx_flag(kctx, KCTX_COMPAT)) {
+	if (kbase_ctx_flag(kctx, KCTX_COMPAT))
+		return current->mm->get_unmapped_area(filp, addr, len, pgoff,
+				flags);
 
-		if (kbase_hw_has_feature(kctx->kbdev,
-						BASE_HW_FEATURE_33BIT_VA)) {
-			high_limit = kctx->same_va_end << PAGE_SHIFT;
-		} else {
-			high_limit = min_t(unsigned long, mm->mmap_base,
+	if (kbase_hw_has_feature(kctx->kbdev, BASE_HW_FEATURE_33BIT_VA)) {
+		info.high_limit = kctx->same_va_end << PAGE_SHIFT;
+		info.align_mask = 0;
+		info.align_offset = 0;
+	} else {
+		info.high_limit = min_t(unsigned long, mm->mmap_base,
 					(kctx->same_va_end << PAGE_SHIFT));
-			if (len >= SZ_2M) {
-				align_offset = SZ_2M;
-				align_mask = SZ_2M - 1;
-			}
+		if (len >= SZ_2M) {
+			info.align_offset = SZ_2M;
+			info.align_mask = SZ_2M - 1;
+		} else {
+			info.align_mask = 0;
+			info.align_offset = 0;
 		}
-
-		low_limit = SZ_2M;
-	} else {
-		cpu_va_bits = 32;
-	}
-#endif /* CONFIG_64BIT */
-	if ((PFN_DOWN(BASE_MEM_COOKIE_BASE) <= pgoff) &&
-		(PFN_DOWN(BASE_MEM_FIRST_FREE_ADDRESS) > pgoff)) {
-			int cookie = pgoff - PFN_DOWN(BASE_MEM_COOKIE_BASE);
-
-			if (!kctx->pending_regions[cookie])
-				return -EINVAL;
-
-			if (!(kctx->pending_regions[cookie]->flags &
-							KBASE_REG_GPU_NX)) {
-				if (cpu_va_bits > gpu_pc_bits) {
-					align_offset = 1ULL << gpu_pc_bits;
-					align_mask = align_offset - 1;
-					is_shader_code = true;
-				}
-			}
-#ifndef CONFIG_64BIT
-	} else {
-		return current->mm->get_unmapped_area(filp, addr, len, pgoff,
-						      flags);
-#endif
 	}
 
 	info.flags = 0;
 	info.length = len;
-	info.low_limit = low_limit;
-	info.high_limit = high_limit;
-	info.align_offset = align_offset;
-	info.align_mask = align_mask;
-
-	ret = kbase_unmapped_area_topdown(&info, is_shader_code);
-
-	if (IS_ERR_VALUE(ret) && high_limit == mm->mmap_base &&
-			high_limit < (kctx->same_va_end << PAGE_SHIFT)) {
-		/* Retry above mmap_base */
-		info.low_limit = mm->mmap_base;
-		info.high_limit = min_t(u64, TASK_SIZE,
-					(kctx->same_va_end << PAGE_SHIFT));
-
-		ret = kbase_unmapped_area_topdown(&info, is_shader_code);
-	}
-
-	return ret;
+	info.low_limit = SZ_2M;
+	return kbase_unmapped_area_topdown(&info);
 }
+#endif
 
 static const struct file_operations kbase_fops = {
 	.owner = THIS_MODULE,
@@ -2203,7 +1575,9 @@ static const struct file_operations kbase_fops = {
 	.compat_ioctl = kbase_ioctl,
 	.mmap = kbase_mmap,
 	.check_flags = kbase_check_flags,
+#ifdef CONFIG_64BIT
 	.get_unmapped_area = kbase_get_unmapped_area,
+#endif
 };
 
 #ifndef CONFIG_MALI_NO_MALI
@@ -2218,18 +1592,17 @@ u32 kbase_os_reg_read(struct kbase_device *kbdev, u16 offset)
 }
 #endif /* !CONFIG_MALI_NO_MALI */
 
-/**
- * show_policy - Show callback for the power_policy sysfs file.
+/** Show callback for the @c power_policy sysfs file.
  *
- * This function is called to get the contents of the power_policy sysfs
+ * This function is called to get the contents of the @c power_policy sysfs
  * file. This is a list of the available policies with the currently active one
  * surrounded by square brackets.
  *
- * @dev:	The device this sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The output buffer for the sysfs file contents
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
  *
- * Return: The number of bytes output to @buf.
+ * @return The number of bytes output to @c buf.
  */
 static ssize_t show_policy(struct device *dev, struct device_attribute *attr, char *const buf)
 {
@@ -2267,20 +1640,19 @@ static ssize_t show_policy(struct device *dev, struct device_attribute *attr, ch
 	return ret;
 }
 
-/**
- * set_policy - Store callback for the power_policy sysfs file.
+/** Store callback for the @c power_policy sysfs file.
  *
- * This function is called when the power_policy sysfs file is written to.
+ * This function is called when the @c power_policy sysfs file is written to.
  * It matches the requested policy against the available policies and if a
- * matching policy is found calls kbase_pm_set_policy() to change the
+ * matching policy is found calls @ref kbase_pm_set_policy to change the
  * policy.
  *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * @return @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_policy(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -2314,8 +1686,7 @@ static ssize_t set_policy(struct device *dev, struct device_attribute *attr, con
 	return count;
 }
 
-/*
- * The sysfs file power_policy.
+/** The sysfs file @c power_policy.
  *
  * This is used for obtaining information about the available policies,
  * determining which policy is currently active, and changing the active
@@ -2323,18 +1694,17 @@ static ssize_t set_policy(struct device *dev, struct device_attribute *attr, con
  */
 static DEVICE_ATTR(power_policy, S_IRUGO | S_IWUSR, show_policy, set_policy);
 
-/**
- * show_ca_policy - Show callback for the core_availability_policy sysfs file.
+/** Show callback for the @c core_availability_policy sysfs file.
  *
- * This function is called to get the contents of the core_availability_policy
+ * This function is called to get the contents of the @c core_availability_policy
  * sysfs file. This is a list of the available policies with the currently
  * active one surrounded by square brackets.
  *
- * @dev:	The device this sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The output buffer for the sysfs file contents
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
  *
- * Return: The number of bytes output to @buf.
+ * @return The number of bytes output to @c buf.
  */
 static ssize_t show_ca_policy(struct device *dev, struct device_attribute *attr, char * const buf)
 {
@@ -2372,20 +1742,19 @@ static ssize_t show_ca_policy(struct device *dev, struct device_attribute *attr,
 	return ret;
 }
 
-/**
- * set_ca_policy - Store callback for the core_availability_policy sysfs file.
+/** Store callback for the @c core_availability_policy sysfs file.
  *
- * This function is called when the core_availability_policy sysfs file is
+ * This function is called when the @c core_availability_policy sysfs file is
  * written to. It matches the requested policy against the available policies
- * and if a matching policy is found calls kbase_pm_set_policy() to change
+ * and if a matching policy is found calls @ref kbase_pm_set_policy to change
  * the policy.
  *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * @return @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_ca_policy(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -2419,8 +1788,7 @@ static ssize_t set_ca_policy(struct device *dev, struct device_attribute *attr,
 	return count;
 }
 
-/*
- * The sysfs file core_availability_policy
+/** The sysfs file @c core_availability_policy
  *
  * This is used for obtaining information about the available policies,
  * determining which policy is currently active, and changing the active
@@ -2428,16 +1796,16 @@ static ssize_t set_ca_policy(struct device *dev, struct device_attribute *attr,
  */
 static DEVICE_ATTR(core_availability_policy, S_IRUGO | S_IWUSR, show_ca_policy, set_ca_policy);
 
-/*
- * show_core_mask - Show callback for the core_mask sysfs file.
+/** Show callback for the @c core_mask sysfs file.
  *
- * This function is called to get the contents of the core_mask sysfs file.
+ * This function is called to get the contents of the @c core_mask sysfs
+ * file.
  *
- * @dev:	The device this sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The output buffer for the sysfs file contents
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
  *
- * Return: The number of bytes output to @buf.
+ * @return The number of bytes output to @c buf.
  */
 static ssize_t show_core_mask(struct device *dev, struct device_attribute *attr, char * const buf)
 {
@@ -2465,17 +1833,16 @@ static ssize_t show_core_mask(struct device *dev, struct device_attribute *attr,
 	return ret;
 }
 
-/**
- * set_core_mask - Store callback for the core_mask sysfs file.
+/** Store callback for the @c core_mask sysfs file.
  *
- * This function is called when the core_mask sysfs file is written to.
+ * This function is called when the @c core_mask sysfs file is written to.
  *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * @return @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_core_mask(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -2538,8 +1905,7 @@ static ssize_t set_core_mask(struct device *dev, struct device_attribute *attr,
 	return -EINVAL;
 }
 
-/*
- * The sysfs file core_mask.
+/** The sysfs file @c core_mask.
  *
  * This is used to restrict shader core availability for debugging purposes.
  * Reading it will show the current core mask and the mask of cores available.
@@ -2548,7 +1914,7 @@ static ssize_t set_core_mask(struct device *dev, struct device_attribute *attr,
 static DEVICE_ATTR(core_mask, S_IRUGO | S_IWUSR, show_core_mask, set_core_mask);
 
 /**
- * set_soft_job_timeout - Store callback for the soft_job_timeout sysfs
+ * set_soft_job_timeout() - Store callback for the soft_job_timeout sysfs
  * file.
  *
  * @dev: The device this sysfs file is for.
@@ -2586,7 +1952,7 @@ static ssize_t set_soft_job_timeout(struct device *dev,
 }
 
 /**
- * show_soft_job_timeout - Show callback for the soft_job_timeout sysfs
+ * show_soft_job_timeout() - Show callback for the soft_job_timeout sysfs
  * file.
  *
  * This will return the timeout for the software jobs.
@@ -2630,13 +1996,12 @@ static u32 timeout_ms_to_ticks(struct kbase_device *kbdev, long timeout_ms,
 	}
 }
 
-/**
- * set_js_timeouts - Store callback for the js_timeouts sysfs file.
+/** Store callback for the @c js_timeouts sysfs file.
  *
- * This function is called to get the contents of the js_timeouts sysfs
+ * This function is called to get the contents of the @c js_timeouts sysfs
  * file. This file contains five values separated by whitespace. The values
- * are basically the same as %JS_SOFT_STOP_TICKS, %JS_HARD_STOP_TICKS_SS,
- * %JS_HARD_STOP_TICKS_DUMPING, %JS_RESET_TICKS_SS, %JS_RESET_TICKS_DUMPING
+ * are basically the same as JS_SOFT_STOP_TICKS, JS_HARD_STOP_TICKS_SS,
+ * JS_HARD_STOP_TICKS_DUMPING, JS_RESET_TICKS_SS, JS_RESET_TICKS_DUMPING
  * configuration values (in that order), with the difference that the js_timeout
  * values are expressed in MILLISECONDS.
  *
@@ -2644,12 +2009,12 @@ static u32 timeout_ms_to_ticks(struct kbase_device *kbdev, long timeout_ms,
  * use by the job scheduler to get override. Note that a value needs to
  * be other than 0 for it to override the current job scheduler value.
  *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * @return @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_js_timeouts(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -2734,18 +2099,17 @@ static unsigned long get_js_timeout_in_ms(
 	return ms;
 }
 
-/**
- * show_js_timeouts - Show callback for the js_timeouts sysfs file.
+/** Show callback for the @c js_timeouts sysfs file.
  *
- * This function is called to get the contents of the js_timeouts sysfs
+ * This function is called to get the contents of the @c js_timeouts sysfs
  * file. It returns the last set values written to the js_timeouts sysfs file.
  * If the file didn't get written yet, the values will be current setting in
  * use.
- * @dev:	The device this sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The output buffer for the sysfs file contents
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
  *
- * Return: The number of bytes output to @buf.
+ * @return The number of bytes output to @c buf.
  */
 static ssize_t show_js_timeouts(struct device *dev, struct device_attribute *attr, char * const buf)
 {
@@ -2797,8 +2161,7 @@ static ssize_t show_js_timeouts(struct device *dev, struct device_attribute *att
 	return ret;
 }
 
-/*
- * The sysfs file js_timeouts.
+/** The sysfs file @c js_timeouts.
  *
  * This is used to override the current job scheduler values for
  * JS_STOP_STOP_TICKS_SS
@@ -2834,7 +2197,7 @@ static u32 get_new_js_timeout(
  * to. It checks the data written, and if valid updates the js_scheduling_period
  * value
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * Return: @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_js_scheduling_period(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
@@ -2913,7 +2276,7 @@ static ssize_t set_js_scheduling_period(struct device *dev,
  * This function is called to get the current period used for the JS scheduling
  * period.
  *
- * Return: The number of bytes output to @buf.
+ * Return: The number of bytes output to buf.
  */
 static ssize_t show_js_scheduling_period(struct device *dev,
 		struct device_attribute *attr, char * const buf)
@@ -2938,15 +2301,14 @@ static DEVICE_ATTR(js_scheduling_period, S_IRUGO | S_IWUSR,
 		show_js_scheduling_period, set_js_scheduling_period);
 
 #if !MALI_CUSTOMER_RELEASE
-/**
- * set_force_replay - Store callback for the force_replay sysfs file.
+/** Store callback for the @c force_replay sysfs file.
  *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * @return @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_force_replay(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -2992,18 +2354,17 @@ static ssize_t set_force_replay(struct device *dev, struct device_attribute *att
 	return -EINVAL;
 }
 
-/**
- * show_force_replay - Show callback for the force_replay sysfs file.
+/** Show callback for the @c force_replay sysfs file.
  *
- * This function is called to get the contents of the force_replay sysfs
+ * This function is called to get the contents of the @c force_replay sysfs
  * file. It returns the last set value written to the force_replay sysfs file.
  * If the file didn't get written yet, the values will be 0.
  *
- * @dev:	The device this sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The output buffer for the sysfs file contents
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
  *
- * Return: The number of bytes output to @buf.
+ * @return The number of bytes output to @c buf.
  */
 static ssize_t show_force_replay(struct device *dev,
 		struct device_attribute *attr, char * const buf)
@@ -3034,8 +2395,8 @@ static ssize_t show_force_replay(struct device *dev,
 	return ret;
 }
 
-/*
- * The sysfs file force_replay.
+/** The sysfs file @c force_replay.
+ *
  */
 static DEVICE_ATTR(force_replay, S_IRUGO | S_IWUSR, show_force_replay,
 		set_force_replay);
@@ -3089,9 +2450,8 @@ static ssize_t show_js_softstop_always(struct device *dev,
 }
 
 /*
- * By default, soft-stops are disabled when only a single context is present.
- * The ability to enable soft-stop when only a single context is present can be
- * used for debug and unit-testing purposes.
+ * By default, soft-stops are disabled when only a single context is present. The ability to
+ * enable soft-stop when only a single context is present can be used for debug and unit-testing purposes.
  * (see CL t6xx_stress_1 unit-test as an example whereby this feature is used.)
  */
 static DEVICE_ATTR(js_softstop_always, S_IRUGO | S_IWUSR, show_js_softstop_always, set_js_softstop_always);
@@ -3112,7 +2472,7 @@ struct kbasep_debug_command {
 	kbasep_debug_command_func *func;
 };
 
-/* Debug commands supported by the driver */
+/** Debug commands supported by the driver */
 static const struct kbasep_debug_command debug_commands[] = {
 	{
 	 .str = "dumptrace",
@@ -3120,17 +2480,16 @@ static const struct kbasep_debug_command debug_commands[] = {
 	 }
 };
 
-/**
- * show_debug - Show callback for the debug_command sysfs file.
+/** Show callback for the @c debug_command sysfs file.
  *
- * This function is called to get the contents of the debug_command sysfs
+ * This function is called to get the contents of the @c debug_command sysfs
  * file. This is a list of the available debug commands, separated by newlines.
  *
- * @dev:	The device this sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The output buffer for the sysfs file contents
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
  *
- * Return: The number of bytes output to @buf.
+ * @return The number of bytes output to @c buf.
  */
 static ssize_t show_debug(struct device *dev, struct device_attribute *attr, char * const buf)
 {
@@ -3155,20 +2514,19 @@ static ssize_t show_debug(struct device *dev, struct device_attribute *attr, cha
 	return ret;
 }
 
-/**
- * issue_debug - Store callback for the debug_command sysfs file.
+/** Store callback for the @c debug_command sysfs file.
  *
- * This function is called when the debug_command sysfs file is written to.
+ * This function is called when the @c debug_command sysfs file is written to.
  * It matches the requested command against the available commands, and if
  * a matching command is found calls the associated function from
- * @debug_commands to issue the command.
+ * @ref debug_commands to issue the command.
  *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * @return @c count if the function succeeded. An error code on failure.
  */
 static ssize_t issue_debug(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -3192,7 +2550,7 @@ static ssize_t issue_debug(struct device *dev, struct device_attribute *attr, co
 	return -EINVAL;
 }
 
-/* The sysfs file debug_command.
+/** The sysfs file @c debug_command.
  *
  * This is used to issue general debug commands to the device driver.
  * Reading it will produce a list of debug commands, separated by newlines.
@@ -3210,11 +2568,11 @@ static DEVICE_ATTR(debug_command, S_IRUGO | S_IWUSR, show_debug, issue_debug);
  * This function is called to get a description of the present Mali
  * GPU via the gpuinfo sysfs entry.  This includes the GPU family, the
  * number of cores, the hardware version and the raw product id.  For
- * example
+ * example:
  *
  *    Mali-T60x MP4 r0p0 0x6956
  *
- * Return: The number of bytes output to @buf.
+ * Return: The number of bytes output to buf.
  */
 static ssize_t kbase_show_gpuinfo(struct device *dev,
 				  struct device_attribute *attr, char *buf)
@@ -3235,8 +2593,6 @@ static ssize_t kbase_show_gpuinfo(struct device *dev,
 		  .name = "Mali-G71" },
 		{ .id = GPU_ID2_PRODUCT_THEX >> GPU_ID_VERSION_PRODUCT_ID_SHIFT,
 		  .name = "Mali-THEx" },
-		{ .id = GPU_ID2_PRODUCT_TSIX >> GPU_ID_VERSION_PRODUCT_ID_SHIFT,
-		  .name = "Mali-G51" },
 	};
 	const char *product_name = "(Unknown Mali GPU)";
 	struct kbase_device *kbdev;
@@ -3269,7 +2625,7 @@ static ssize_t kbase_show_gpuinfo(struct device *dev,
 		}
 	}
 
-	return scnprintf(buf, PAGE_SIZE, "%s %d cores r%dp%d 0x%04X\n",
+	return scnprintf(buf, PAGE_SIZE, "%s MP%d r%dp%d 0x%04X\n",
 		product_name, kbdev->gpu_props.num_cores,
 		(gpu_id & GPU_ID_VERSION_MAJOR) >> GPU_ID_VERSION_MAJOR_SHIFT,
 		(gpu_id & GPU_ID_VERSION_MINOR) >> GPU_ID_VERSION_MINOR_SHIFT,
@@ -3287,7 +2643,7 @@ static DEVICE_ATTR(gpuinfo, S_IRUGO, kbase_show_gpuinfo, NULL);
  * This function is called when the dvfs_period sysfs file is written to. It
  * checks the data written, and if valid updates the DVFS period variable,
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * Return: @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_dvfs_period(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
@@ -3322,7 +2678,7 @@ static ssize_t set_dvfs_period(struct device *dev,
  * This function is called to get the current period used for the DVFS sample
  * timer.
  *
- * Return: The number of bytes output to @buf.
+ * Return: The number of bytes output to buf.
  */
 static ssize_t show_dvfs_period(struct device *dev,
 		struct device_attribute *attr, char * const buf)
@@ -3357,7 +2713,7 @@ static DEVICE_ATTR(dvfs_period, S_IRUGO | S_IWUSR, show_dvfs_period,
  * shader is powered off), and poweroff_gpu_ticks (the number of poweroff timer
  * ticks before the GPU is powered off), in that order.
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * Return: @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_pm_poweroff(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
@@ -3396,7 +2752,7 @@ static ssize_t set_pm_poweroff(struct device *dev,
  * This function is called to get the current period used for the DVFS sample
  * timer.
  *
- * Return: The number of bytes output to @buf.
+ * Return: The number of bytes output to buf.
  */
 static ssize_t show_pm_poweroff(struct device *dev,
 		struct device_attribute *attr, char * const buf)
@@ -3429,7 +2785,7 @@ static DEVICE_ATTR(pm_poweroff, S_IRUGO | S_IWUSR, show_pm_poweroff,
  * This function is called when the reset_timeout sysfs file is written to. It
  * checks the data written, and if valid updates the reset timeout.
  *
- * Return: @count if the function succeeded. An error code on failure.
+ * Return: @c count if the function succeeded. An error code on failure.
  */
 static ssize_t set_reset_timeout(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
@@ -3463,7 +2819,7 @@ static ssize_t set_reset_timeout(struct device *dev,
  *
  * This function is called to get the current reset timeout.
  *
- * Return: The number of bytes output to @buf.
+ * Return: The number of bytes output to buf.
  */
 static ssize_t show_reset_timeout(struct device *dev,
 		struct device_attribute *attr, char * const buf)
@@ -3563,199 +2919,43 @@ static ssize_t set_mem_pool_max_size(struct device *dev,
 static DEVICE_ATTR(mem_pool_max_size, S_IRUGO | S_IWUSR, show_mem_pool_max_size,
 		set_mem_pool_max_size);
 
-#ifdef CONFIG_DEBUG_FS
-
-/* Number of entries in serialize_jobs_settings[] */
-#define NR_SERIALIZE_JOBS_SETTINGS 5
-/* Maximum string length in serialize_jobs_settings[].name */
-#define MAX_SERIALIZE_JOBS_NAME_LEN 16
-
-static struct
-{
-	char *name;
-	u8 setting;
-} serialize_jobs_settings[NR_SERIALIZE_JOBS_SETTINGS] = {
-	{"none", 0},
-	{"intra-slot", KBASE_SERIALIZE_INTRA_SLOT},
-	{"inter-slot", KBASE_SERIALIZE_INTER_SLOT},
-	{"full", KBASE_SERIALIZE_INTRA_SLOT | KBASE_SERIALIZE_INTER_SLOT},
-	{"full-reset", KBASE_SERIALIZE_INTRA_SLOT | KBASE_SERIALIZE_INTER_SLOT |
-			KBASE_SERIALIZE_RESET}
-};
 
-/**
- * kbasep_serialize_jobs_seq_show - Show callback for the serialize_jobs debugfs
- *                                  file
- * @sfile: seq_file pointer
- * @data:  Private callback data
- *
- * This function is called to get the contents of the serialize_jobs debugfs
- * file. This is a list of the available settings with the currently active one
- * surrounded by square brackets.
- *
- * Return: 0 on success, or an error code on error
- */
-static int kbasep_serialize_jobs_seq_show(struct seq_file *sfile, void *data)
+static int kbasep_protected_mode_enter(struct kbase_device *kbdev)
 {
-	struct kbase_device *kbdev = sfile->private;
-	int i;
-
-	CSTD_UNUSED(data);
-
-	for (i = 0; i < NR_SERIALIZE_JOBS_SETTINGS; i++) {
-		if (kbdev->serialize_jobs == serialize_jobs_settings[i].setting)
-			seq_printf(sfile, "[%s] ",
-					serialize_jobs_settings[i].name);
-		else
-			seq_printf(sfile, "%s ",
-					serialize_jobs_settings[i].name);
-	}
-
-	seq_puts(sfile, "\n");
-
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+		GPU_COMMAND_SET_PROTECTED_MODE, NULL);
 	return 0;
 }
 
-/**
- * kbasep_serialize_jobs_debugfs_write - Store callback for the serialize_jobs
- *                                       debugfs file.
- * @file:  File pointer
- * @ubuf:  User buffer containing data to store
- * @count: Number of bytes in user buffer
- * @ppos:  File position
- *
- * This function is called when the serialize_jobs debugfs file is written to.
- * It matches the requested setting against the available settings and if a
- * matching setting is found updates kbdev->serialize_jobs.
- *
- * Return: @count if the function succeeded. An error code on failure.
- */
-static ssize_t kbasep_serialize_jobs_debugfs_write(struct file *file,
-		const char __user *ubuf, size_t count, loff_t *ppos)
-{
-	struct seq_file *s = file->private_data;
-	struct kbase_device *kbdev = s->private;
-	char buf[MAX_SERIALIZE_JOBS_NAME_LEN];
-	int i;
-	bool valid = false;
-
-	CSTD_UNUSED(ppos);
-
-	count = min_t(size_t, sizeof(buf) - 1, count);
-	if (copy_from_user(buf, ubuf, count))
-		return -EFAULT;
-
-	buf[count] = 0;
-
-	for (i = 0; i < NR_SERIALIZE_JOBS_SETTINGS; i++) {
-		if (sysfs_streq(serialize_jobs_settings[i].name, buf)) {
-			kbdev->serialize_jobs =
-					serialize_jobs_settings[i].setting;
-			valid = true;
-			break;
-		}
-	}
-
-	if (!valid) {
-		dev_err(kbdev->dev, "serialize_jobs: invalid setting\n");
-		return -EINVAL;
-	}
-
-	return count;
-}
-
-/**
- * kbasep_serialize_jobs_debugfs_open - Open callback for the serialize_jobs
- *                                     debugfs file
- * @in:   inode pointer
- * @file: file pointer
- *
- * Return: Zero on success, error code on failure
- */
-static int kbasep_serialize_jobs_debugfs_open(struct inode *in,
-		struct file *file)
+static bool kbasep_protected_mode_supported(struct kbase_device *kbdev)
 {
-	return single_open(file, kbasep_serialize_jobs_seq_show, in->i_private);
+	return true;
 }
 
-static const struct file_operations kbasep_serialize_jobs_debugfs_fops = {
-	.open = kbasep_serialize_jobs_debugfs_open,
-	.read = seq_read,
-	.write = kbasep_serialize_jobs_debugfs_write,
-	.llseek = seq_lseek,
-	.release = single_release,
+static struct kbase_protected_ops kbasep_protected_ops = {
+	.protected_mode_enter = kbasep_protected_mode_enter,
+	.protected_mode_reset = NULL,
+	.protected_mode_supported = kbasep_protected_mode_supported,
 };
 
-#endif /* CONFIG_DEBUG_FS */
-
-static int kbasep_protected_mode_init(struct kbase_device *kbdev)
+static void kbasep_protected_mode_init(struct kbase_device *kbdev)
 {
-#ifdef CONFIG_OF
-	struct device_node *protected_node;
-	struct platform_device *pdev;
-	struct protected_mode_device *protected_dev;
-#endif
+	kbdev->protected_ops = NULL;
 
 	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PROTECTED_MODE)) {
 		/* Use native protected ops */
-		kbdev->protected_dev = kzalloc(sizeof(*kbdev->protected_dev),
-				GFP_KERNEL);
-		if (!kbdev->protected_dev)
-			return -ENOMEM;
-		kbdev->protected_dev->data = kbdev;
-		kbdev->protected_ops = &kbase_native_protected_ops;
-		kbdev->protected_mode_support = true;
-		return 0;
-	}
-
-	kbdev->protected_mode_support = false;
-
-#ifdef CONFIG_OF
-	protected_node = of_parse_phandle(kbdev->dev->of_node,
-			"protected-mode-switcher", 0);
-
-	if (!protected_node)
-		protected_node = of_parse_phandle(kbdev->dev->of_node,
-				"secure-mode-switcher", 0);
-
-	if (!protected_node) {
-		/* If protected_node cannot be looked up then we assume
-		 * protected mode is not supported on this platform. */
-		dev_info(kbdev->dev, "Protected mode not available\n");
-		return 0;
-	}
-
-	pdev = of_find_device_by_node(protected_node);
-	if (!pdev)
-		return -EINVAL;
-
-	protected_dev = platform_get_drvdata(pdev);
-	if (!protected_dev)
-		return -EPROBE_DEFER;
-
-	kbdev->protected_ops = &protected_dev->ops;
-	kbdev->protected_dev = protected_dev;
-
-	if (kbdev->protected_ops) {
-		int err;
-
-		/* Make sure protected mode is disabled on startup */
-		mutex_lock(&kbdev->pm.lock);
-		err = kbdev->protected_ops->protected_mode_disable(
-				kbdev->protected_dev);
-		mutex_unlock(&kbdev->pm.lock);
-
-		/* protected_mode_disable() returns -EINVAL if not supported */
-		kbdev->protected_mode_support = (err != -EINVAL);
+		kbdev->protected_ops = &kbasep_protected_ops;
 	}
+#ifdef PROTECTED_CALLBACKS
+	else
+		kbdev->protected_ops = PROTECTED_CALLBACKS;
 #endif
-	return 0;
-}
 
-static void kbasep_protected_mode_term(struct kbase_device *kbdev)
-{
-	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PROTECTED_MODE))
-		kfree(kbdev->protected_dev);
+	if (kbdev->protected_ops)
+		kbdev->protected_mode_support =
+				kbdev->protected_ops->protected_mode_supported(kbdev);
+	else
+		kbdev->protected_mode_support = false;
 }
 
 #ifdef CONFIG_MALI_NO_MALI
@@ -3917,8 +3117,7 @@ if (kbdev->clock != NULL) {
 
 static void power_control_term(struct kbase_device *kbdev)
 {
-#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)) || \
-		defined(LSK_OPPV2_BACKPORT)
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
 	dev_pm_opp_of_remove_table(kbdev->dev);
 #elif (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 19, 0))
 	of_free_opp_table(kbdev->dev);
@@ -3975,7 +3174,6 @@ DEFINE_SIMPLE_ATTRIBUTE(fops_##type##_quirks, type##_quirks_get,\
 MAKE_QUIRK_ACCESSORS(sc);
 MAKE_QUIRK_ACCESSORS(tiler);
 MAKE_QUIRK_ACCESSORS(mmu);
-MAKE_QUIRK_ACCESSORS(jm);
 
 #endif /* KBASE_GPU_RESET_EN */
 
@@ -4059,8 +3257,6 @@ static int kbase_device_debugfs_init(struct kbase_device *kbdev)
 	kbasep_gpu_memory_debugfs_init(kbdev);
 	kbase_as_fault_debugfs_init(kbdev);
 #if KBASE_GPU_RESET_EN
-	/* fops_* variables created by invocations of macro
-	 * MAKE_QUIRK_ACCESSORS() above. */
 	debugfs_create_file("quirks_sc", 0644,
 			kbdev->mali_debugfs_directory, kbdev,
 			&fops_sc_quirks);
@@ -4070,9 +3266,6 @@ static int kbase_device_debugfs_init(struct kbase_device *kbdev)
 	debugfs_create_file("quirks_mmu", 0644,
 			kbdev->mali_debugfs_directory, kbdev,
 			&fops_mmu_quirks);
-	debugfs_create_file("quirks_jm", 0644,
-			kbdev->mali_debugfs_directory, kbdev,
-			&fops_jm_quirks);
 #endif /* KBASE_GPU_RESET_EN */
 
 #ifndef CONFIG_MALI_COH_USER
@@ -4099,19 +3292,6 @@ static int kbase_device_debugfs_init(struct kbase_device *kbdev)
 	kbasep_trace_timeline_debugfs_init(kbdev);
 #endif /* CONFIG_MALI_TRACE_TIMELINE */
 
-#ifdef CONFIG_MALI_DEVFREQ
-#ifdef CONFIG_DEVFREQ_THERMAL
-	if (kbdev->inited_subsys & inited_devfreq)
-		kbase_ipa_debugfs_init(kbdev);
-#endif /* CONFIG_DEVFREQ_THERMAL */
-#endif /* CONFIG_MALI_DEVFREQ */
-
-#ifdef CONFIG_DEBUG_FS
-	debugfs_create_file("serialize_jobs", S_IRUGO | S_IWUSR,
-			kbdev->mali_debugfs_directory, kbdev,
-			&kbasep_serialize_jobs_debugfs_fops);
-#endif /* CONFIG_DEBUG_FS */
-
 	return 0;
 
 out:
@@ -4133,27 +3313,13 @@ static inline int kbase_device_debugfs_init(struct kbase_device *kbdev)
 static inline void kbase_device_debugfs_term(struct kbase_device *kbdev) { }
 #endif /* CONFIG_DEBUG_FS */
 
-static void kbase_device_coherency_init(struct kbase_device *kbdev,
-		unsigned prod_id)
+static void kbase_device_coherency_init(struct kbase_device *kbdev, u32 gpu_id)
 {
 #ifdef CONFIG_OF
 	u32 supported_coherency_bitmap =
 		kbdev->gpu_props.props.raw_props.coherency_mode;
 	const void *coherency_override_dts;
 	u32 override_coherency;
-
-	/* Only for tMIx :
-	 * (COHERENCY_ACE_LITE | COHERENCY_ACE) was incorrectly
-	 * documented for tMIx so force correct value here.
-	 */
-	if (GPU_ID_IS_NEW_FORMAT(prod_id) &&
-		   (GPU_ID2_MODEL_MATCH_VALUE(prod_id) ==
-				   GPU_ID2_PRODUCT_TMIX))
-		if (supported_coherency_bitmap ==
-				COHERENCY_FEATURE_BIT(COHERENCY_ACE))
-			supported_coherency_bitmap |=
-				COHERENCY_FEATURE_BIT(COHERENCY_ACE_LITE);
-
 #endif /* CONFIG_OF */
 
 	kbdev->system_coherency = COHERENCY_NONE;
@@ -4239,8 +3405,6 @@ static int kbase_platform_device_remove(struct platform_device *pdev)
 	if (!kbdev)
 		return -ENODEV;
 
-	kfree(kbdev->gpu_props.prop_buffer);
-
 #ifdef CONFIG_MALI_FPGA_BUS_LOGGER
 	if (kbdev->inited_subsys & inited_buslogger) {
 		bl_core_client_unregister(kbdev->buslogger);
@@ -4248,6 +3412,10 @@ static int kbase_platform_device_remove(struct platform_device *pdev)
 	}
 #endif
 
+	if (kbdev->inited_subsys & inited_sysfs_group) {
+		sysfs_remove_group(&kbdev->dev->kobj, &kbase_attr_group);
+		kbdev->inited_subsys &= ~inited_sysfs_group;
+	}
 
 	if (kbdev->inited_subsys & inited_dev_list) {
 		dev_list = kbase_dev_list_get();
@@ -4261,11 +3429,6 @@ static int kbase_platform_device_remove(struct platform_device *pdev)
 		kbdev->inited_subsys &= ~inited_misc_register;
 	}
 
-	if (kbdev->inited_subsys & inited_sysfs_group) {
-		sysfs_remove_group(&kbdev->dev->kobj, &kbase_attr_group);
-		kbdev->inited_subsys &= ~inited_sysfs_group;
-	}
-
 	if (kbdev->inited_subsys & inited_get_device) {
 		put_device(kbdev->dev);
 		kbdev->inited_subsys &= ~inited_get_device;
@@ -4280,6 +3443,14 @@ static int kbase_platform_device_remove(struct platform_device *pdev)
 		kbase_debug_job_fault_dev_term(kbdev);
 		kbdev->inited_subsys &= ~inited_job_fault;
 	}
+
+#ifndef CONFIG_MALI_PRFCNT_SET_SECONDARY
+	if (kbdev->inited_subsys & inited_ipa) {
+		kbase_ipa_term(kbdev->ipa_ctx);
+		kbdev->inited_subsys &= ~inited_ipa;
+	}
+#endif /* CONFIG_MALI_PRFCNT_SET_SECONDARY */
+
 	if (kbdev->inited_subsys & inited_vinstr) {
 		kbase_vinstr_term(kbdev->vinstr_ctx);
 		kbdev->inited_subsys &= ~inited_vinstr;
@@ -4310,11 +3481,6 @@ static int kbase_platform_device_remove(struct platform_device *pdev)
 	if (kbdev->inited_subsys & inited_mem)
 		kbase_mem_halt(kbdev);
 
-	if (kbdev->inited_subsys & inited_protected) {
-		kbasep_protected_mode_term(kbdev);
-		kbdev->inited_subsys &= ~inited_protected;
-	}
-
 	if (kbdev->inited_subsys & inited_js) {
 		kbasep_js_devdata_term(kbdev);
 		kbdev->inited_subsys &= ~inited_js;
@@ -4330,11 +3496,6 @@ static int kbase_platform_device_remove(struct platform_device *pdev)
 		kbdev->inited_subsys &= ~inited_pm_runtime_init;
 	}
 
-	if (kbdev->inited_subsys & inited_ctx_sched) {
-		kbase_ctx_sched_term(kbdev);
-		kbdev->inited_subsys &= ~inited_ctx_sched;
-	}
-
 	if (kbdev->inited_subsys & inited_device) {
 		kbase_device_term(kbdev);
 		kbdev->inited_subsys &= ~inited_device;
@@ -4392,7 +3553,6 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	struct kbase_device *kbdev;
 	struct mali_base_gpu_core_props *core_props;
 	u32 gpu_id;
-	unsigned prod_id;
 	const struct list_head *dev_list;
 	int err = 0;
 
@@ -4404,6 +3564,7 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 		return err;
 	}
 #endif
+
 	kbdev = kbase_device_alloc();
 	if (!kbdev) {
 		dev_err(&pdev->dev, "Allocate device failed\n");
@@ -4474,6 +3635,8 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	core_props->gpu_freq_khz_min = GPU_FREQ_KHZ_MIN;
 	core_props->gpu_freq_khz_max = GPU_FREQ_KHZ_MAX;
 
+	kbdev->gpu_props.irq_throttle_time_us = DEFAULT_IRQ_THROTTLE_TIME_US;
+
 	err = kbase_device_init(kbdev);
 	if (err) {
 		dev_err(kbdev->dev, "Device initialization failed (%d)\n", err);
@@ -4482,15 +3645,6 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	}
 	kbdev->inited_subsys |= inited_device;
 
-	err = kbase_ctx_sched_init(kbdev);
-	if (err) {
-		dev_err(kbdev->dev, "Context scheduler initialization failed (%d)\n",
-				err);
-		kbase_platform_device_remove(pdev);
-		return err;
-	}
-	kbdev->inited_subsys |= inited_ctx_sched;
-
 	if (kbdev->pm.callback_power_runtime_init) {
 		err = kbdev->pm.callback_power_runtime_init(kbdev);
 		if (err) {
@@ -4512,22 +3666,11 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 
 	gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
 	gpu_id &= GPU_ID_VERSION_PRODUCT_ID;
-	prod_id = gpu_id >> GPU_ID_VERSION_PRODUCT_ID_SHIFT;
+	gpu_id = gpu_id >> GPU_ID_VERSION_PRODUCT_ID_SHIFT;
 
-	kbase_device_coherency_init(kbdev, prod_id);
-
-	err = kbasep_protected_mode_init(kbdev);
-	if (err) {
-		dev_err(kbdev->dev, "Protected mode subsystem initialization failed\n");
-		kbase_platform_device_remove(pdev);
-		return err;
-	}
-	kbdev->inited_subsys |= inited_protected;
+	kbase_device_coherency_init(kbdev, gpu_id);
 
-	dev_list = kbase_dev_list_get();
-	list_add(&kbdev->entry, &kbase_dev_list);
-	kbase_dev_list_put(dev_list);
-	kbdev->inited_subsys |= inited_dev_list;
+	kbasep_protected_mode_init(kbdev);
 
 	err = kbasep_js_devdata_init(kbdev);
 	if (err) {
@@ -4555,10 +3698,12 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 
 #ifdef CONFIG_MALI_DEVFREQ
 	err = kbase_devfreq_init(kbdev);
-	if (!err)
-		kbdev->inited_subsys |= inited_devfreq;
-	else
-		dev_err(kbdev->dev, "Continuing without devfreq\n");
+	if (err) {
+		dev_err(kbdev->dev, "Fevfreq initialization failed\n");
+		kbase_platform_device_remove(pdev);
+		return err;
+	}
+	kbdev->inited_subsys |= inited_devfreq;
 #endif /* CONFIG_MALI_DEVFREQ */
 
 	kbdev->vinstr_ctx = kbase_vinstr_init(kbdev);
@@ -4570,6 +3715,17 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	}
 	kbdev->inited_subsys |= inited_vinstr;
 
+#ifndef CONFIG_MALI_PRFCNT_SET_SECONDARY
+	kbdev->ipa_ctx = kbase_ipa_init(kbdev);
+	if (!kbdev->ipa_ctx) {
+		dev_err(kbdev->dev, "IPA initialization failed\n");
+		kbase_platform_device_remove(pdev);
+		return -EINVAL;
+	}
+
+	kbdev->inited_subsys |= inited_ipa;
+#endif  /* CONFIG_MALI_PRFCNT_SET_SECONDARY */
+
 	err = kbase_debug_job_fault_dev_init(kbdev);
 	if (err) {
 		dev_err(kbdev->dev, "Job fault debug initialization failed\n");
@@ -4596,26 +3752,6 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	kbdev->mdev.parent = get_device(kbdev->dev);
 	kbdev->inited_subsys |= inited_get_device;
 
-	/* This needs to happen before registering the device with misc_register(),
-	 * otherwise it causes a race condition between registering the device and a
-	 * uevent event being generated for userspace, causing udev rules to run
-	 * which might expect certain sysfs attributes present. As a result of the
-	 * race condition we avoid, some Mali sysfs entries may have appeared to
-	 * udev to not exist.
-
-	 * For more information, see
-	 * https://www.kernel.org/doc/Documentation/driver-model/device.txt, the
-	 * paragraph that starts with "Word of warning", currently the second-last
-	 * paragraph.
-	 */
-	err = sysfs_create_group(&kbdev->dev->kobj, &kbase_attr_group);
-	if (err) {
-		dev_err(&pdev->dev, "SysFS group creation failed\n");
-		kbase_platform_device_remove(pdev);
-		return err;
-	}
-	kbdev->inited_subsys |= inited_sysfs_group;
-
 	err = misc_register(&kbdev->mdev);
 	if (err) {
 		dev_err(kbdev->dev, "Misc device registration failed for %s\n",
@@ -4625,6 +3761,18 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	}
 	kbdev->inited_subsys |= inited_misc_register;
 
+	dev_list = kbase_dev_list_get();
+	list_add(&kbdev->entry, &kbase_dev_list);
+	kbase_dev_list_put(dev_list);
+	kbdev->inited_subsys |= inited_dev_list;
+
+	err = sysfs_create_group(&kbdev->dev->kobj, &kbase_attr_group);
+	if (err) {
+		dev_err(&pdev->dev, "SysFS group creation failed\n");
+		kbase_platform_device_remove(pdev);
+		return err;
+	}
+	kbdev->inited_subsys |= inited_sysfs_group;
 
 #ifdef CONFIG_MALI_FPGA_BUS_LOGGER
 	err = bl_core_client_register(kbdev->devname,
@@ -4640,13 +3788,6 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 	}
 #endif
 
-	err = kbase_gpuprops_populate_user_buffer(kbdev);
-	if (err) {
-		dev_err(&pdev->dev, "GPU property population failed");
-		kbase_platform_device_remove(pdev);
-		return err;
-	}
-
 	dev_info(kbdev->dev,
 			"Probed as %s\n", dev_name(kbdev->mdev.this_device));
 
@@ -4657,14 +3798,14 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 
 #undef KBASEP_DEFAULT_REGISTER_HISTORY_SIZE
 
-/**
- * kbase_device_suspend - Suspend callback from the OS.
+
+/** Suspend callback from the OS.
  *
  * This is called by Linux when the device should suspend.
  *
- * @dev:  The device to suspend
+ * @param dev  The device to suspend
  *
- * Return: A standard Linux error code
+ * @return A standard Linux error code
  */
 static int kbase_device_suspend(struct device *dev)
 {
@@ -4673,24 +3814,22 @@ static int kbase_device_suspend(struct device *dev)
 	if (!kbdev)
 		return -ENODEV;
 
-#if defined(CONFIG_MALI_DEVFREQ) && \
+#if defined(CONFIG_PM_DEVFREQ) && \
 		(LINUX_VERSION_CODE >= KERNEL_VERSION(3, 8, 0))
-	if (kbdev->inited_subsys & inited_devfreq)
-		devfreq_suspend_device(kbdev->devfreq);
+	devfreq_suspend_device(kbdev->devfreq);
 #endif
 
 	kbase_pm_suspend(kbdev);
 	return 0;
 }
 
-/**
- * kbase_device_resume - Resume callback from the OS.
+/** Resume callback from the OS.
  *
  * This is called by Linux when the device should resume from suspension.
  *
- * @dev:  The device to resume
+ * @param dev  The device to resume
  *
- * Return: A standard Linux error code
+ * @return A standard Linux error code
  */
 static int kbase_device_resume(struct device *dev)
 {
@@ -4701,24 +3840,21 @@ static int kbase_device_resume(struct device *dev)
 
 	kbase_pm_resume(kbdev);
 
-#if defined(CONFIG_MALI_DEVFREQ) && \
+#if defined(CONFIG_PM_DEVFREQ) && \
 		(LINUX_VERSION_CODE >= KERNEL_VERSION(3, 8, 0))
-	if (kbdev->inited_subsys & inited_devfreq)
-		devfreq_resume_device(kbdev->devfreq);
+	devfreq_resume_device(kbdev->devfreq);
 #endif
 	return 0;
 }
 
-/**
- * kbase_device_runtime_suspend - Runtime suspend callback from the OS.
+/** Runtime suspend callback from the OS.
  *
- * This is called by Linux when the device should prepare for a condition in
- * which it will not be able to communicate with the CPU(s) and RAM due to
- * power management.
+ * This is called by Linux when the device should prepare for a condition in which it will
+ * not be able to communicate with the CPU(s) and RAM due to power management.
  *
- * @dev:  The device to suspend
+ * @param dev  The device to suspend
  *
- * Return: A standard Linux error code
+ * @return A standard Linux error code
  */
 #ifdef KBASE_PM_RUNTIME
 static int kbase_device_runtime_suspend(struct device *dev)
@@ -4728,10 +3864,9 @@ static int kbase_device_runtime_suspend(struct device *dev)
 	if (!kbdev)
 		return -ENODEV;
 
-#if defined(CONFIG_MALI_DEVFREQ) && \
+#if defined(CONFIG_PM_DEVFREQ) && \
 		(LINUX_VERSION_CODE >= KERNEL_VERSION(3, 8, 0))
-	if (kbdev->inited_subsys & inited_devfreq)
-		devfreq_suspend_device(kbdev->devfreq);
+	devfreq_suspend_device(kbdev->devfreq);
 #endif
 
 	if (kbdev->pm.backend.callback_power_runtime_off) {
@@ -4742,14 +3877,13 @@ static int kbase_device_runtime_suspend(struct device *dev)
 }
 #endif /* KBASE_PM_RUNTIME */
 
-/**
- * kbase_device_runtime_resume - Runtime resume callback from the OS.
+/** Runtime resume callback from the OS.
  *
  * This is called by Linux when the device should go into a fully active state.
  *
- * @dev:  The device to suspend
+ * @param dev  The device to suspend
  *
- * Return: A standard Linux error code
+ * @return A standard Linux error code
  */
 
 #ifdef KBASE_PM_RUNTIME
@@ -4766,10 +3900,9 @@ static int kbase_device_runtime_resume(struct device *dev)
 		dev_dbg(dev, "runtime resume\n");
 	}
 
-#if defined(CONFIG_MALI_DEVFREQ) && \
+#if defined(CONFIG_PM_DEVFREQ) && \
 		(LINUX_VERSION_CODE >= KERNEL_VERSION(3, 8, 0))
-	if (kbdev->inited_subsys & inited_devfreq)
-		devfreq_resume_device(kbdev->devfreq);
+	devfreq_resume_device(kbdev->devfreq);
 #endif
 
 	return ret;
@@ -4803,7 +3936,7 @@ static int kbase_device_runtime_idle(struct device *dev)
 }
 #endif /* KBASE_PM_RUNTIME */
 
-/* The power management operations for the platform driver.
+/** The power management operations for the platform driver.
  */
 static const struct dev_pm_ops kbase_pm_ops = {
 	.suspend = kbase_device_suspend,
diff --git a/drivers/gpu/arm/midgard/mali_kbase_ctx_sched.c b/drivers/gpu/arm/midgard/mali_kbase_ctx_sched.c
deleted file mode 100644
index e2f7baa..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_ctx_sched.c
+++ /dev/null
@@ -1,203 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#include <mali_kbase.h>
-#include <mali_kbase_config_defaults.h>
-
-#include "mali_kbase_ctx_sched.h"
-
-int kbase_ctx_sched_init(struct kbase_device *kbdev)
-{
-	int as_present = (1U << kbdev->nr_hw_address_spaces) - 1;
-
-	/* These two must be recalculated if nr_hw_address_spaces changes
-	 * (e.g. for HW workarounds) */
-	kbdev->nr_user_address_spaces = kbdev->nr_hw_address_spaces;
-	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
-		bool use_workaround;
-
-		use_workaround = DEFAULT_SECURE_BUT_LOSS_OF_PERFORMANCE;
-		if (use_workaround) {
-			dev_dbg(kbdev->dev, "GPU has HW ISSUE 8987, and driver configured for security workaround: 1 address space only");
-			kbdev->nr_user_address_spaces = 1;
-		}
-	}
-
-	kbdev->as_free = as_present; /* All ASs initially free */
-
-	memset(kbdev->as_to_kctx, 0, sizeof(kbdev->as_to_kctx));
-
-	return 0;
-}
-
-void kbase_ctx_sched_term(struct kbase_device *kbdev)
-{
-	s8 i;
-
-	/* Sanity checks */
-	for (i = 0; i != kbdev->nr_hw_address_spaces; ++i) {
-		WARN_ON(kbdev->as_to_kctx[i] != NULL);
-		WARN_ON(!(kbdev->as_free & (1u << i)));
-	}
-}
-
-/* kbasep_ctx_sched_find_as_for_ctx - Find a free address space
- *
- * @kbdev: The context for which to find a free address space
- *
- * Return: A valid AS if successful, otherwise KBASEP_AS_NR_INVALID
- *
- * This function returns an address space available for use. It would prefer
- * returning an AS that has been previously assigned to the context to
- * avoid having to reprogram the MMU.
- */
-static int kbasep_ctx_sched_find_as_for_ctx(struct kbase_context *kctx)
-{
-	struct kbase_device *const kbdev = kctx->kbdev;
-	int free_as;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	/* First check if the previously assigned AS is available */
-	if ((kctx->as_nr != KBASEP_AS_NR_INVALID) &&
-			(kbdev->as_free & (1u << kctx->as_nr)))
-		return kctx->as_nr;
-
-	/* The previously assigned AS was taken, we'll be returning any free
-	 * AS at this point.
-	 */
-	free_as = ffs(kbdev->as_free) - 1;
-	if (free_as >= 0 && free_as < kbdev->nr_hw_address_spaces)
-		return free_as;
-
-	return KBASEP_AS_NR_INVALID;
-}
-
-int kbase_ctx_sched_retain_ctx(struct kbase_context *kctx)
-{
-	struct kbase_device *const kbdev = kctx->kbdev;
-
-	lockdep_assert_held(&kbdev->mmu_hw_mutex);
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	WARN_ON(!kbdev->pm.backend.gpu_powered);
-
-	if (atomic_inc_return(&kctx->refcount) == 1) {
-		int const free_as = kbasep_ctx_sched_find_as_for_ctx(kctx);
-
-		if (free_as != KBASEP_AS_NR_INVALID) {
-			kbdev->as_free &= ~(1u << free_as);
-			/* Only program the MMU if the context has not been
-			 * assigned the same address space before.
-			 */
-			if (free_as != kctx->as_nr) {
-				struct kbase_context *const prev_kctx =
-					kbdev->as_to_kctx[free_as];
-
-				if (prev_kctx) {
-					WARN_ON(atomic_read(&prev_kctx->refcount) != 0);
-					kbase_mmu_disable(prev_kctx);
-					prev_kctx->as_nr = KBASEP_AS_NR_INVALID;
-				}
-
-				kctx->as_nr = free_as;
-				kbdev->as_to_kctx[free_as] = kctx;
-				kbase_mmu_update(kctx);
-			}
-		} else {
-			atomic_dec(&kctx->refcount);
-
-			/* Failed to find an available address space, we must
-			 * be returning an error at this point.
-			 */
-			WARN_ON(kctx->as_nr != KBASEP_AS_NR_INVALID);
-		}
-	}
-
-	return kctx->as_nr;
-}
-
-void kbase_ctx_sched_retain_ctx_refcount(struct kbase_context *kctx)
-{
-	struct kbase_device *const kbdev = kctx->kbdev;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-	WARN_ON(atomic_read(&kctx->refcount) == 0);
-	WARN_ON(kctx->as_nr == KBASEP_AS_NR_INVALID);
-	WARN_ON(kbdev->as_to_kctx[kctx->as_nr] != kctx);
-
-	atomic_inc(&kctx->refcount);
-}
-
-void kbase_ctx_sched_release_ctx(struct kbase_context *kctx)
-{
-	struct kbase_device *const kbdev = kctx->kbdev;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	if (atomic_dec_return(&kctx->refcount) == 0)
-		kbdev->as_free |= (1u << kctx->as_nr);
-}
-
-void kbase_ctx_sched_remove_ctx(struct kbase_context *kctx)
-{
-	struct kbase_device *const kbdev = kctx->kbdev;
-
-	lockdep_assert_held(&kbdev->mmu_hw_mutex);
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	WARN_ON(atomic_read(&kctx->refcount) != 0);
-
-	if (kctx->as_nr != KBASEP_AS_NR_INVALID) {
-		if (kbdev->pm.backend.gpu_powered)
-			kbase_mmu_disable(kctx);
-
-		kbdev->as_to_kctx[kctx->as_nr] = NULL;
-		kctx->as_nr = KBASEP_AS_NR_INVALID;
-	}
-}
-
-void kbase_ctx_sched_restore_all_as(struct kbase_device *kbdev)
-{
-	s8 i;
-
-	lockdep_assert_held(&kbdev->mmu_hw_mutex);
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	WARN_ON(!kbdev->pm.backend.gpu_powered);
-
-	for (i = 0; i != kbdev->nr_hw_address_spaces; ++i) {
-		struct kbase_context *kctx;
-
-		kctx = kbdev->as_to_kctx[i];
-		if (kctx) {
-			if (atomic_read(&kctx->refcount)) {
-				WARN_ON(kctx->as_nr != i);
-
-				kbase_mmu_update(kctx);
-			} else {
-				/* This context might have been assigned an
-				 * AS before, clear it.
-				 */
-				kbdev->as_to_kctx[kctx->as_nr] = NULL;
-				kctx->as_nr = KBASEP_AS_NR_INVALID;
-			}
-		} else {
-			kbase_mmu_disable_as(kbdev, i);
-		}
-	}
-}
diff --git a/drivers/gpu/arm/midgard/mali_kbase_ctx_sched.h b/drivers/gpu/arm/midgard/mali_kbase_ctx_sched.h
deleted file mode 100644
index e551525..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_ctx_sched.h
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KBASE_CTX_SCHED_H_
-#define _KBASE_CTX_SCHED_H_
-
-#include <mali_kbase.h>
-
-/* The Context Scheduler manages address space assignment and reference
- * counting to kbase_context. The interface has been designed to minimise
- * interactions between the Job Scheduler and Power Management/MMU to support
- * both the existing Job Scheduler and Command Stream Frontend interface.
- *
- * The initial implementation of the Context Scheduler does not schedule
- * contexts. Instead it relies on the Job Scheduler/CSF to make decisions of
- * when to schedule/evict contexts if address spaces are starved. In the
- * future, once an interface between the CS and JS/CSF have been devised to
- * provide enough information about how each context is consuming GPU resources,
- * those decisions can be made in the CS itself, thereby reducing duplicated
- * code.
- */
-
-/* base_ctx_sched_init - Initialise the context scheduler
- *
- * @kbdev: The device for which the context scheduler needs to be
- *         initialised
- *
- * Return: 0 for success, otherwise failure
- *
- * This must be called during device initilisation. The number of hardware
- * address spaces must already be established before calling this function.
- */
-int kbase_ctx_sched_init(struct kbase_device *kbdev);
-
-/* base_ctx_sched_term - Terminate the context scheduler
- *
- * @kbdev: The device for which the context scheduler needs to be
- *         terminated
- *
- * This must be called during device termination after all contexts have been
- * destroyed.
- */
-void kbase_ctx_sched_term(struct kbase_device *kbdev);
-
-/* kbase_ctx_sched_retain_ctx - Retain a reference to the @ref kbase_context
- *
- * @kctx: The context to which to retain a reference
- *
- * Return: The address space that the context has been assigned to or
- *         KBASEP_AS_NR_INVALID if no address space was available.
- *
- * This function should be called whenever an address space should be assigned
- * to a context and programmed onto the MMU. It should typically be called
- * when jobs are ready to be submitted to the GPU.
- *
- * It can be called as many times as necessary. The address space will be
- * assigned to the context for as long as there is a reference to said context.
- *
- * The kbase_device::mmu_hw_mutex and kbase_device::hwaccess_lock locks must be
- * held whilst calling this function.
- */
-int kbase_ctx_sched_retain_ctx(struct kbase_context *kctx);
-
-/* kbase_ctx_sched_retain_ctx_refcount
- *
- * @kctx: The context to which to retain a reference
- *
- * This function only retains a reference to the context. It must be called
- * only when the context already has a reference.
- *
- * This is typically called inside an atomic session where we know the context
- * is already scheduled in but want to take an extra reference to ensure that
- * it doesn't get descheduled.
- *
- * The kbase_device::hwaccess_lock must be held whilst calling this function
- */
-void kbase_ctx_sched_retain_ctx_refcount(struct kbase_context *kctx);
-
-/* kbase_ctx_sched_release_ctx - Release a reference to the @ref kbase_context
- *
- * @kctx: The context from which to release a reference
- *
- * This function should be called whenever an address space could be unassigned
- * from a context. When there are no more references to said context, the
- * address space previously assigned to this context shall be reassigned to
- * other contexts as needed.
- *
- * The kbase_device::hwaccess_lock must be held whilst calling this function
- */
-void kbase_ctx_sched_release_ctx(struct kbase_context *kctx);
-
-/* kbase_ctx_sched_remove_ctx - Unassign previously assigned address space
- *
- * @kctx: The context to be removed
- *
- * This function should be called when a context is being destroyed. The
- * context must no longer have any reference. If it has been assigned an
- * address space before then the AS will be unprogrammed.
- *
- * The kbase_device::mmu_hw_mutex and kbase_device::hwaccess_lock locks must be
- * held whilst calling this function.
- */
-void kbase_ctx_sched_remove_ctx(struct kbase_context *kctx);
-
-/* kbase_ctx_sched_restore_all_as - Reprogram all address spaces
- *
- * @kbdev: The device for which address spaces to be reprogrammed
- *
- * This function shall reprogram all address spaces previously assigned to
- * contexts. It can be used after the GPU is reset.
- *
- * The kbase_device::mmu_hw_mutex and kbase_device::hwaccess_lock locks must be
- * held whilst calling this function.
- */
-void kbase_ctx_sched_restore_all_as(struct kbase_device *kbdev);
-
-#endif /* _KBASE_CTX_SCHED_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_debug_job_fault.c b/drivers/gpu/arm/midgard/mali_kbase_debug_job_fault.c
index f29430d..83c5c37 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_debug_job_fault.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_debug_job_fault.c
@@ -17,7 +17,6 @@
 
 #include <mali_kbase.h>
 #include <linux/spinlock.h>
-#include <mali_kbase_hwaccess_jm.h>
 
 #ifdef CONFIG_DEBUG_FS
 
@@ -332,7 +331,11 @@ static void *debug_job_fault_start(struct seq_file *m, loff_t *pos)
 		 * job done but we delayed it. Now we should clean cache
 		 * earlier. Then the GPU memory dump should be correct.
 		 */
-		kbase_backend_cacheclean(kbdev, event->katom);
+		if (event->katom->need_cache_flush_cores_retained) {
+			kbase_gpu_cacheclean(kbdev, event->katom);
+			event->katom->need_cache_flush_cores_retained = 0;
+		}
+
 	} else
 		return NULL;
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_debug_mem_view.c b/drivers/gpu/arm/midgard/mali_kbase_debug_mem_view.c
index 6f2cbdf..a98355e 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_debug_mem_view.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_debug_mem_view.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2013-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -90,10 +90,8 @@ static void *debug_mem_next(struct seq_file *m, void *v, loff_t *pos)
 		return data;
 	}
 
-	if (list_is_last(data->lh, &mem_data->mapping_list)) {
-		kfree(data);
+	if (list_is_last(data->lh, &mem_data->mapping_list))
 		return NULL;
-	}
 
 	data->lh = data->lh->next;
 	data->offset = 0;
@@ -155,42 +153,11 @@ static const struct seq_operations ops = {
 	.show = debug_mem_show,
 };
 
-static int debug_mem_zone_open(struct rb_root *rbtree,
-						struct debug_mem_data *mem_data)
-{
-	int ret = 0;
-	struct rb_node *p;
-	struct kbase_va_region *reg;
-	struct debug_mem_mapping *mapping;
-
-	for (p = rb_first(rbtree); p; p = rb_next(p)) {
-		reg = rb_entry(p, struct kbase_va_region, rblink);
-
-		if (reg->gpu_alloc == NULL)
-			/* Empty region - ignore */
-			continue;
-
-		mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
-		if (!mapping) {
-			ret = -ENOMEM;
-			goto out;
-		}
-
-		mapping->alloc = kbase_mem_phy_alloc_get(reg->gpu_alloc);
-		mapping->start_pfn = reg->start_pfn;
-		mapping->nr_pages = reg->nr_pages;
-		mapping->flags = reg->flags;
-		list_add_tail(&mapping->node, &mem_data->mapping_list);
-	}
-
-out:
-	return ret;
-}
-
 static int debug_mem_open(struct inode *i, struct file *file)
 {
 	struct file *kctx_file = i->i_private;
 	struct kbase_context *kctx = kctx_file->private_data;
+	struct rb_node *p;
 	struct debug_mem_data *mem_data;
 	int ret;
 
@@ -212,22 +179,28 @@ static int debug_mem_open(struct inode *i, struct file *file)
 
 	kbase_gpu_vm_lock(kctx);
 
-	ret = debug_mem_zone_open(&kctx->reg_rbtree_same, mem_data);
-	if (0 != ret) {
-		kbase_gpu_vm_unlock(kctx);
-		goto out;
-	}
+	for (p = rb_first(&kctx->reg_rbtree); p; p = rb_next(p)) {
+		struct kbase_va_region *reg;
+		struct debug_mem_mapping *mapping;
 
-	ret = debug_mem_zone_open(&kctx->reg_rbtree_exec, mem_data);
-	if (0 != ret) {
-		kbase_gpu_vm_unlock(kctx);
-		goto out;
-	}
+		reg = rb_entry(p, struct kbase_va_region, rblink);
 
-	ret = debug_mem_zone_open(&kctx->reg_rbtree_custom, mem_data);
-	if (0 != ret) {
-		kbase_gpu_vm_unlock(kctx);
-		goto out;
+		if (reg->gpu_alloc == NULL)
+			/* Empty region - ignore */
+			continue;
+
+		mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+		if (!mapping) {
+			ret = -ENOMEM;
+			kbase_gpu_vm_unlock(kctx);
+			goto out;
+		}
+
+		mapping->alloc = kbase_mem_phy_alloc_get(reg->gpu_alloc);
+		mapping->start_pfn = reg->start_pfn;
+		mapping->nr_pages = reg->nr_pages;
+		mapping->flags = reg->flags;
+		list_add_tail(&mapping->node, &mem_data->mapping_list);
 	}
 
 	kbase_gpu_vm_unlock(kctx);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_defs.h b/drivers/gpu/arm/midgard/mali_kbase_defs.h
index 91f159b..edd6711 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_defs.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_defs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -35,7 +35,6 @@
 #include <mali_kbase_mmu_mode.h>
 #include <mali_kbase_instr_defs.h>
 #include <mali_kbase_pm.h>
-#include <protected_mode_switcher.h>
 
 #include <linux/atomic.h>
 #include <linux/mempool.h>
@@ -51,19 +50,19 @@
 #include <linux/kds.h>
 #endif				/* CONFIG_KDS */
 
-#if defined(CONFIG_SYNC)
-#include <sync.h>
-#else
-#include "mali_kbase_fence_defs.h"
-#endif
+#ifdef CONFIG_SYNC
+#include "sync.h"
+#endif				/* CONFIG_SYNC */
+
+#include "mali_kbase_dma_fence.h"
 
 #ifdef CONFIG_DEBUG_FS
 #include <linux/debugfs.h>
 #endif				/* CONFIG_DEBUG_FS */
 
-#ifdef CONFIG_MALI_DEVFREQ
+#ifdef CONFIG_PM_DEVFREQ
 #include <linux/devfreq.h>
-#endif /* CONFIG_MALI_DEVFREQ */
+#endif /* CONFIG_DEVFREQ */
 
 #include <linux/clk.h>
 #include <linux/regulator/consumer.h>
@@ -212,13 +211,6 @@
 
 #define KBASEP_ATOM_ID_INVALID BASE_JD_ATOM_COUNT
 
-/* Serialize atoms within a slot (ie only one atom per job slot) */
-#define KBASE_SERIALIZE_INTRA_SLOT (1 << 0)
-/* Serialize atoms between slots (ie only one job slot running at any time) */
-#define KBASE_SERIALIZE_INTER_SLOT (1 << 1)
-/* Reset the GPU after each atom completion */
-#define KBASE_SERIALIZE_RESET (1 << 2)
-
 #ifdef CONFIG_DEBUG_FS
 struct base_job_fault_event {
 
@@ -360,7 +352,7 @@ enum kbase_atom_gpu_rb_state {
 	KBASE_ATOM_GPU_RB_SUBMITTED,
 	/* Atom must be returned to JS as soon as it reaches the head of the
 	 * ringbuffer due to a previous failure */
-	KBASE_ATOM_GPU_RB_RETURN_TO_JS = -1
+	KBASE_ATOM_GPU_RB_RETURN_TO_JS
 };
 
 enum kbase_atom_enter_protected_state {
@@ -409,6 +401,7 @@ struct kbase_ext_res {
 struct kbase_jd_atom {
 	struct work_struct work;
 	ktime_t start_timestamp;
+	u64 time_spent_us; /**< Total time spent on the GPU in microseconds */
 
 	struct base_jd_udata udata;
 	struct kbase_context *kctx;
@@ -436,26 +429,15 @@ struct kbase_jd_atom {
 	struct kds_resource_set *kds_rset;
 	bool kds_dep_satisfied;
 #endif				/* CONFIG_KDS */
-#if defined(CONFIG_SYNC)
-	/* Stores either an input or output fence, depending on soft-job type */
+#ifdef CONFIG_SYNC
 	struct sync_fence *fence;
 	struct sync_fence_waiter sync_waiter;
 #endif				/* CONFIG_SYNC */
-#if defined(CONFIG_MALI_DMA_FENCE) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_MALI_DMA_FENCE
 	struct {
-		/* Use the functions/API defined in mali_kbase_fence.h to
-		 * when working with this sub struct */
-#if defined(CONFIG_SYNC_FILE)
-		/* Input fence */
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-		struct fence *fence_in;
-#else
-		struct dma_fence *fence_in;
-#endif
-#endif
-		/* This points to the dma-buf output fence for this atom. If
-		 * this is NULL then there is no fence for this atom and the
-		 * following fields related to dma_fence may have invalid data.
+		/* This points to the dma-buf fence for this atom. If this is
+		 * NULL then there is no fence for this atom and the other
+		 * fields related to dma_fence may have invalid data.
 		 *
 		 * The context and seqno fields contain the details for this
 		 * fence.
@@ -464,11 +446,7 @@ struct kbase_jd_atom {
 		 * regardless of the event_code of the katom (signal also on
 		 * failure).
 		 */
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
 		struct fence *fence;
-#else
-		struct dma_fence *fence;
-#endif
 		/* The dma-buf fence context number for this atom. A unique
 		 * context number is allocated to each katom in the context on
 		 * context creation.
@@ -507,7 +485,7 @@ struct kbase_jd_atom {
 		 */
 		atomic_t dep_count;
 	} dma_fence;
-#endif /* CONFIG_MALI_DMA_FENCE || CONFIG_SYNC_FILE*/
+#endif /* CONFIG_MALI_DMA_FENCE */
 
 	/* Note: refer to kbasep_js_atom_retained_state, which will take a copy of some of the following members */
 	enum base_jd_event_code event_code;
@@ -517,7 +495,7 @@ struct kbase_jd_atom {
 	 * NOTE: see if this can be unified into the another member e.g. the event */
 	int retry_submit_on_slot;
 
-	u32 ticks;
+	union kbasep_js_policy_job_info sched_info;
 	/* JS atom priority with respect to other atoms on its kctx. */
 	int sched_priority;
 
@@ -562,7 +540,7 @@ struct kbase_jd_atom {
 	struct base_job_fault_event fault_event;
 #endif
 
-	/* List head used for three different purposes:
+	/* List head used for two different purposes:
 	 *  1. Overflow list for JS ring buffers. If an atom is ready to run,
 	 *     but there is no room in the JS ring buffer, then the atom is put
 	 *     on the ring buffer's overflow list using this list node.
@@ -570,10 +548,7 @@ struct kbase_jd_atom {
 	 */
 	struct list_head queue;
 
-	/* Used to keep track of all JIT free/alloc jobs in submission order
-	 */
-	struct list_head jit_node;
-	bool jit_blocked;
+	struct kbase_va_region *jit_addr_reg;
 
 	/* If non-zero, this indicates that the atom will fail with the set
 	 * event_code when the atom is processed. */
@@ -628,7 +603,8 @@ struct kbase_jd_context {
 	 * This waitq can be waited upon to find out when the context jobs are all
 	 * done/cancelled (including those that might've been blocked on
 	 * dependencies) - and so, whether it can be terminated. However, it should
-	 * only be terminated once it is not present in the run-pool (see
+	 * only be terminated once it is neither present in the policy-queue (see
+	 * kbasep_js_policy_try_evict_ctx() ) nor the run-pool (see
 	 * kbasep_js_kctx_info::ctx::is_scheduled).
 	 *
 	 * Since the waitq is only set under kbase_jd_context::lock,
@@ -902,6 +878,40 @@ struct kbase_pm_device_data {
 };
 
 /**
+ * struct kbase_protected_ops - Platform specific functions for GPU protected
+ * mode operations
+ * @protected_mode_enter: Callback to enter protected mode on the GPU
+ * @protected_mode_reset: Callback to reset the GPU and exit protected mode.
+ * @protected_mode_supported: Callback to check if protected mode is supported.
+ */
+struct kbase_protected_ops {
+	/**
+	 * protected_mode_enter() - Enter protected mode on the GPU
+	 * @kbdev:	The kbase device
+	 *
+	 * Return: 0 on success, non-zero on error
+	 */
+	int (*protected_mode_enter)(struct kbase_device *kbdev);
+
+	/**
+	 * protected_mode_reset() - Reset the GPU and exit protected mode
+	 * @kbdev:	The kbase device
+	 *
+	 * Return: 0 on success, non-zero on error
+	 */
+	int (*protected_mode_reset)(struct kbase_device *kbdev);
+
+	/**
+	 * protected_mode_supported() - Check if protected mode is supported
+	 * @kbdev:	The kbase device
+	 *
+	 * Return: 0 on success, non-zero on error
+	 */
+	bool (*protected_mode_supported)(struct kbase_device *kbdev);
+};
+
+
+/**
  * struct kbase_mem_pool - Page based memory pool for kctx/kbdev
  * @kbdev:     Kbase device where memory is used
  * @cur_size:  Number of free pages currently in the pool (may exceed @max_size
@@ -926,18 +936,6 @@ struct kbase_mem_pool {
 	struct kbase_mem_pool *next_pool;
 };
 
-/**
- * struct kbase_devfreq_opp - Lookup table for converting between nominal OPP
- *                            frequency, and real frequency and core mask
- * @opp_freq:  Nominal OPP frequency
- * @real_freq: Real GPU frequency
- * @core_mask: Shader core mask
- */
-struct kbase_devfreq_opp {
-	u64 opp_freq;
-	u64 real_freq;
-	u64 core_mask;
-};
 
 #define DEVNAME_SIZE	16
 
@@ -985,14 +983,6 @@ struct kbase_device {
 	struct kbase_mmu_mode const *mmu_mode;
 
 	struct kbase_as as[BASE_MAX_NR_AS];
-	/* The below variables (as_free and as_to_kctx) are managed by the
-	 * Context Scheduler. The kbasep_js_device_data::runpool_irq::lock must
-	 * be held whilst accessing these.
-	 */
-	u16 as_free; /* Bitpattern of free Address Spaces */
-	/* Mapping from active Address Spaces to kbase_context */
-	struct kbase_context *as_to_kctx[BASE_MAX_NR_AS];
-
 
 	spinlock_t mmu_mask_change;
 
@@ -1048,7 +1038,6 @@ struct kbase_device {
 	u64 shader_available_bitmap;
 	u64 tiler_available_bitmap;
 	u64 l2_available_bitmap;
-	u64 stack_available_bitmap;
 
 	u64 shader_ready_bitmap;
 	u64 shader_transitioning_bitmap;
@@ -1069,6 +1058,9 @@ struct kbase_device {
 
 	struct kbase_vinstr_context *vinstr_ctx;
 
+	/*value to be written to the irq_throttle register each time an irq is served */
+	atomic_t irq_throttle_cycles;
+
 #if KBASE_TRACE_ENABLE
 	spinlock_t              trace_lock;
 	u16                     trace_first_out;
@@ -1087,32 +1079,21 @@ struct kbase_device {
 	struct list_head        kctx_list;
 	struct mutex            kctx_list_lock;
 
-#ifdef CONFIG_MALI_DEVFREQ
+#ifdef CONFIG_PM_DEVFREQ
 	struct devfreq_dev_profile devfreq_profile;
 	struct devfreq *devfreq;
 	unsigned long current_freq;
-	unsigned long current_nominal_freq;
 	unsigned long current_voltage;
-	u64 current_core_mask;
-	struct kbase_devfreq_opp *opp_table;
-	int num_opps;
 #ifdef CONFIG_DEVFREQ_THERMAL
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0)
 	struct devfreq_cooling_device *devfreq_cooling;
 #else
 	struct thermal_cooling_device *devfreq_cooling;
 #endif
-	/* Current IPA model - true for configured model, false for fallback */
-	atomic_t ipa_use_configured_model;
-	struct {
-		/* Access to this struct must be with ipa.lock held */
-		struct mutex lock;
-		struct kbase_ipa_model *configured_model;
-		struct kbase_ipa_model *fallback_model;
-	} ipa;
-#endif /* CONFIG_DEVFREQ_THERMAL */
-#endif /* CONFIG_MALI_DEVFREQ */
+#endif
+#endif
 
+	struct kbase_ipa_context *ipa_ctx;
 
 #ifdef CONFIG_MALI_TRACE_TIMELINE
 	struct kbase_trace_kbdev_timeline timeline;
@@ -1211,11 +1192,8 @@ struct kbase_device {
 	u32 snoop_enable_smc;
 	u32 snoop_disable_smc;
 
-	/* Protected mode operations */
-	struct protected_mode_ops *protected_ops;
-
-	/* Protected device attached to this kbase device */
-	struct protected_mode_device *protected_dev;
+	/* Protected operations */
+	struct kbase_protected_ops *protected_ops;
 
 	/*
 	 * true when GPU is put into protected mode
@@ -1254,9 +1232,6 @@ struct kbase_device {
 
 	/* Protects access to MMU operations */
 	struct mutex mmu_hw_mutex;
-
-	/* Current serialization mode. See KBASE_SERIALIZE_* for details */
-	u8 serialize_jobs;
 };
 
 /**
@@ -1309,10 +1284,6 @@ struct jsctx_queue {
  *
  * @KCTX_DYING: Set when the context process is in the process of being evicted.
  *
- * @KCTX_NO_IMPLICIT_SYNC: Set when explicit Android fences are in use on this
- * context, to disable use of implicit dma-buf fences. This is used to avoid
- * potential synchronization deadlocks.
- *
  * All members need to be separate bits. This enum is intended for use in a
  * bitmask where multiple values get OR-ed together.
  */
@@ -1327,7 +1298,6 @@ enum kbase_context_flags {
 	KCTX_PRIVILEGED = 1U << 7,
 	KCTX_SCHEDULED = 1U << 8,
 	KCTX_DYING = 1U << 9,
-	KCTX_NO_IMPLICIT_SYNC = 1U << 10,
 };
 
 struct kbase_context {
@@ -1355,12 +1325,7 @@ struct kbase_context {
 
 	struct mutex            mmu_lock;
 	struct mutex            reg_lock; /* To be converted to a rwlock? */
-	struct rb_root reg_rbtree_same; /* RB tree of GPU (live) regions,
-					 * SAME_VA zone */
-	struct rb_root reg_rbtree_exec; /* RB tree of GPU (live) regions,
-					 * EXEC zone */
-	struct rb_root reg_rbtree_custom; /* RB tree of GPU (live) regions,
-					 * CUSTOM_VA zone */
+	struct rb_root          reg_rbtree; /* Red-Black tree of GPU regions (live regions) */
 
 	unsigned long    cookies;
 	struct kbase_va_region *pending_regions[BITS_PER_LONG];
@@ -1377,6 +1342,7 @@ struct kbase_context {
 
 	struct shrinker         reclaim;
 	struct list_head        evict_list;
+	struct mutex            evict_lock;
 
 	struct list_head waiting_soft_jobs;
 	spinlock_t waiting_soft_jobs_lock;
@@ -1401,14 +1367,6 @@ struct kbase_context {
 	 * you can take whilst doing this) */
 	int as_nr;
 
-	/* Keeps track of the number of users of this context. A user can be a
-	 * job that is available for execution, instrumentation needing to 'pin'
-	 * a context for counter collection, etc. If the refcount reaches 0 then
-	 * this context is considered inactive and the previously programmed
-	 * AS might be cleared at any point.
-	 */
-	atomic_t refcount;
-
 	/* NOTE:
 	 *
 	 * Flags are in jctx.sched_info.ctx.flags
@@ -1450,15 +1408,6 @@ struct kbase_context {
 	atomic_t atoms_pulled;
 	/* Number of atoms currently pulled from this context, per slot */
 	atomic_t atoms_pulled_slot[BASE_JM_MAX_NR_SLOTS];
-	/* Number of atoms currently pulled from this context, per slot and
-	 * priority. Hold hwaccess_lock when accessing */
-	int atoms_pulled_slot_pri[BASE_JM_MAX_NR_SLOTS][
-			KBASE_JS_ATOM_SCHED_PRIO_COUNT];
-
-	/* true if slot is blocked on the given priority. This will be set on a
-	 * soft-stop */
-	bool blocked_js[BASE_JM_MAX_NR_SLOTS][KBASE_JS_ATOM_SCHED_PRIO_COUNT];
-
 	/* Bitmask of slots that can be pulled from */
 	u32 slots_pullable;
 
@@ -1485,18 +1434,9 @@ struct kbase_context {
 	struct list_head jit_active_head;
 	struct list_head jit_pool_head;
 	struct list_head jit_destroy_head;
-	struct mutex jit_evict_lock;
+	struct mutex jit_lock;
 	struct work_struct jit_work;
 
-	/* A list of the JIT soft-jobs in submission order
-	 * (protected by kbase_jd_context.lock)
-	 */
-	struct list_head jit_atoms_head;
-	/* A list of pending JIT alloc soft-jobs (using the 'queue' list_head)
-	 * (protected by kbase_jd_context.lock)
-	 */
-	struct list_head jit_pending_alloc;
-
 	/* External sticky resource management */
 	struct list_head ext_res_meta_head;
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_device.c b/drivers/gpu/arm/midgard/mali_kbase_device.c
index d635fcc..7484eec 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_device.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_device.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -245,10 +245,11 @@ int kbase_device_init(struct kbase_device * const kbdev)
 
 	kbdev->reset_timeout_ms = DEFAULT_RESET_TIMEOUT_MS;
 
-	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-		kbdev->mmu_mode = kbase_mmu_mode_get_aarch64();
-	else
-		kbdev->mmu_mode = kbase_mmu_mode_get_lpae();
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+	kbdev->mmu_mode = kbase_mmu_mode_get_aarch64();
+#else
+	kbdev->mmu_mode = kbase_mmu_mode_get_lpae();
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
 
 #ifdef CONFIG_MALI_DEBUG
 	init_waitqueue_head(&kbdev->driver_inactive_wait);
@@ -655,6 +656,28 @@ void kbase_set_profiling_control(struct kbase_device *kbdev, u32 control, u32 va
 	}
 }
 
+u32 kbase_get_profiling_control(struct kbase_device *kbdev, u32 control)
+{
+	u32 ret_value = 0;
+
+	switch (control) {
+	case FBDUMP_CONTROL_ENABLE:
+		/* fall through */
+	case FBDUMP_CONTROL_RATE:
+		/* fall through */
+	case SW_COUNTER_ENABLE:
+		/* fall through */
+	case FBDUMP_CONTROL_RESIZE_FACTOR:
+		ret_value = kbdev->kbase_profiling_controls[control];
+		break;
+	default:
+		dev_err(kbdev->dev, "Profiling control %d not found\n", control);
+		break;
+	}
+
+	return ret_value;
+}
+
 /*
  * Called by gator to control the production of
  * profiling information at runtime
diff --git a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c
index 9197743..97bb6c5 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,6 +22,7 @@
 #include "mali_kbase_dma_fence.h"
 
 #include <linux/atomic.h>
+#include <linux/fence.h>
 #include <linux/list.h>
 #include <linux/lockdep.h>
 #include <linux/mutex.h>
@@ -33,6 +34,10 @@
 
 #include <mali_kbase.h>
 
+
+/* Spin lock protecting all Mali fences as fence->lock. */
+static DEFINE_SPINLOCK(kbase_dma_fence_lock);
+
 static void
 kbase_dma_fence_work(struct work_struct *pwork);
 
@@ -44,12 +49,67 @@ kbase_dma_fence_waiters_add(struct kbase_jd_atom *katom)
 	list_add_tail(&katom->queue, &kctx->dma_fence.waiting_resource);
 }
 
-static void
+void
 kbase_dma_fence_waiters_remove(struct kbase_jd_atom *katom)
 {
 	list_del(&katom->queue);
 }
 
+static const char *
+kbase_dma_fence_get_driver_name(struct fence *fence)
+{
+	return kbase_drv_name;
+}
+
+static const char *
+kbase_dma_fence_get_timeline_name(struct fence *fence)
+{
+	return kbase_timeline_name;
+}
+
+static bool
+kbase_dma_fence_enable_signaling(struct fence *fence)
+{
+	/* If in the future we need to add code here remember to
+	 * to get a reference to the fence and release it when signaling
+	 * as stated in fence.h
+	 */
+	return true;
+}
+
+static void
+kbase_dma_fence_fence_value_str(struct fence *fence, char *str, int size)
+{
+	snprintf(str, size, "%u", fence->seqno);
+}
+
+static const struct fence_ops kbase_dma_fence_ops = {
+	.get_driver_name = kbase_dma_fence_get_driver_name,
+	.get_timeline_name = kbase_dma_fence_get_timeline_name,
+	.enable_signaling = kbase_dma_fence_enable_signaling,
+	/* Use the default wait */
+	.wait = fence_default_wait,
+	.fence_value_str = kbase_dma_fence_fence_value_str,
+};
+
+static struct fence *
+kbase_dma_fence_new(unsigned int context, unsigned int seqno)
+{
+	struct fence *fence;
+
+	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
+	if (!fence)
+		return NULL;
+
+	fence_init(fence,
+		   &kbase_dma_fence_ops,
+		   &kbase_dma_fence_lock,
+		   context,
+		   seqno);
+
+	return fence;
+}
+
 static int
 kbase_dma_fence_lock_reservations(struct kbase_dma_fence_resv_info *info,
 				  struct ww_acquire_ctx *ctx)
@@ -129,6 +189,59 @@ kbase_dma_fence_queue_work(struct kbase_jd_atom *katom)
 }
 
 /**
+ * kbase_dma_fence_free_callbacks - Free dma-fence callbacks on a katom
+ * @katom: Pointer to katom
+ * @queue_worker: Boolean indicating if fence worker is to be queued when
+ *                dep_count reaches 0.
+ *
+ * This function will free all fence callbacks on the katom's list of
+ * callbacks. Callbacks that have not yet been called, because their fence
+ * hasn't yet signaled, will first be removed from the fence.
+ *
+ * Locking: katom->dma_fence.callbacks list assumes jctx.lock is held.
+ */
+static void
+kbase_dma_fence_free_callbacks(struct kbase_jd_atom *katom, bool queue_worker)
+{
+	struct kbase_dma_fence_cb *cb, *tmp;
+
+	lockdep_assert_held(&katom->kctx->jctx.lock);
+
+	/* Clean up and free callbacks. */
+	list_for_each_entry_safe(cb, tmp, &katom->dma_fence.callbacks, node) {
+		bool ret;
+
+		/* Cancel callbacks that hasn't been called yet. */
+		ret = fence_remove_callback(cb->fence, &cb->fence_cb);
+		if (ret) {
+			int ret;
+
+			/* Fence had not signaled, clean up after
+			 * canceling.
+			 */
+			ret = atomic_dec_return(&katom->dma_fence.dep_count);
+
+			if (unlikely(queue_worker && ret == 0)) {
+				/*
+				 * dep_count went to zero and queue_worker is
+				 * true. Queue the worker to handle the
+				 * completion of the katom.
+				 */
+				kbase_dma_fence_queue_work(katom);
+			}
+		}
+
+		/*
+		 * Release the reference taken in
+		 * kbase_dma_fence_add_callback().
+		 */
+		fence_put(cb->fence);
+		list_del(&cb->node);
+		kfree(cb);
+	}
+}
+
+/**
  * kbase_dma_fence_cancel_atom() - Cancels waiting on an atom
  * @katom:	Katom to cancel
  *
@@ -140,12 +253,14 @@ kbase_dma_fence_cancel_atom(struct kbase_jd_atom *katom)
 	lockdep_assert_held(&katom->kctx->jctx.lock);
 
 	/* Cancel callbacks and clean up. */
-	kbase_fence_free_callbacks(katom);
+	kbase_dma_fence_free_callbacks(katom, false);
+
+	KBASE_DEBUG_ASSERT(atomic_read(&katom->dma_fence.dep_count) == 0);
 
 	/* Mark the atom as handled in case all fences signaled just before
 	 * canceling the callbacks and the worker was queued.
 	 */
-	kbase_fence_dep_count_set(katom, -1);
+	atomic_set(&katom->dma_fence.dep_count, -1);
 
 	/* Prevent job_done_nolock from being called twice on an atom when
 	 * there is a race between job completion and cancellation.
@@ -175,15 +290,15 @@ kbase_dma_fence_work(struct work_struct *pwork)
 	ctx = &katom->kctx->jctx;
 
 	mutex_lock(&ctx->lock);
-	if (kbase_fence_dep_count_read(katom) != 0)
+	if (atomic_read(&katom->dma_fence.dep_count) != 0)
 		goto out;
 
-	kbase_fence_dep_count_set(katom, -1);
+	atomic_set(&katom->dma_fence.dep_count, -1);
 
 	/* Remove atom from list of dma-fence waiting atoms. */
 	kbase_dma_fence_waiters_remove(katom);
 	/* Cleanup callbacks. */
-	kbase_fence_free_callbacks(katom);
+	kbase_dma_fence_free_callbacks(katom, false);
 	/*
 	 * Queue atom on GPU, unless it has already completed due to a failing
 	 * dependency. Run jd_done_nolock() on the katom if it is completed.
@@ -197,15 +312,64 @@ out:
 	mutex_unlock(&ctx->lock);
 }
 
+/**
+ * kbase_dma_fence_add_callback() - Add callback on @fence to block @katom
+ * @katom: Pointer to katom that will be blocked by @fence
+ * @fence: Pointer to fence on which to set up the callback
+ * @callback: Pointer to function to be called when fence is signaled
+ *
+ * Caller needs to hold a reference to @fence when calling this function, and
+ * the caller is responsible for releasing that reference.  An additional
+ * reference to @fence will be taken when the callback was successfully set up
+ * and @fence needs to be kept valid until the callback has been called and
+ * cleanup have been done.
+ *
+ * Return: 0 on success: fence was either already signalled, or callback was
+ * set up. Negative error code is returned on error.
+ */
+static int
+kbase_dma_fence_add_callback(struct kbase_jd_atom *katom,
+			     struct fence *fence,
+			     fence_func_t callback)
+{
+	int err = 0;
+	struct kbase_dma_fence_cb *kbase_fence_cb;
+
+	kbase_fence_cb = kmalloc(sizeof(*kbase_fence_cb), GFP_KERNEL);
+	if (!kbase_fence_cb)
+		return -ENOMEM;
+
+	kbase_fence_cb->fence = fence;
+	kbase_fence_cb->katom = katom;
+	INIT_LIST_HEAD(&kbase_fence_cb->node);
+
+	err = fence_add_callback(fence, &kbase_fence_cb->fence_cb, callback);
+	if (err == -ENOENT) {
+		/* Fence signaled, clear the error and return */
+		err = 0;
+		kbase_fence_cb->fence = NULL;
+		kfree(kbase_fence_cb);
+	} else if (err) {
+		kfree(kbase_fence_cb);
+	} else {
+		/*
+		 * Get reference to fence that will be kept until callback gets
+		 * cleaned up in kbase_dma_fence_free_callbacks().
+		 */
+		fence_get(fence);
+		atomic_inc(&katom->dma_fence.dep_count);
+		/* Add callback to katom's list of callbacks */
+		list_add(&kbase_fence_cb->node, &katom->dma_fence.callbacks);
+	}
+
+	return err;
+}
+
 static void
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
 kbase_dma_fence_cb(struct fence *fence, struct fence_cb *cb)
-#else
-kbase_dma_fence_cb(struct dma_fence *fence, struct dma_fence_cb *cb)
-#endif
 {
-	struct kbase_fence_cb *kcb = container_of(cb,
-				struct kbase_fence_cb,
+	struct kbase_dma_fence_cb *kcb = container_of(cb,
+				struct kbase_dma_fence_cb,
 				fence_cb);
 	struct kbase_jd_atom *katom = kcb->katom;
 
@@ -213,8 +377,7 @@ kbase_dma_fence_cb(struct dma_fence *fence, struct dma_fence_cb *cb)
 	 * preventing this callback from ever scheduling work. Which in turn
 	 * would reschedule the atom.
 	 */
-
-	if (kbase_fence_dep_count_dec_and_test(katom))
+	if (atomic_dec_and_test(&katom->dma_fence.dep_count))
 		kbase_dma_fence_queue_work(katom);
 }
 
@@ -223,13 +386,8 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 					 struct reservation_object *resv,
 					 bool exclusive)
 {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
 	struct fence *excl_fence = NULL;
 	struct fence **shared_fences = NULL;
-#else
-	struct dma_fence *excl_fence = NULL;
-	struct dma_fence **shared_fences = NULL;
-#endif
 	unsigned int shared_count = 0;
 	int err, i;
 
@@ -241,16 +399,16 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 		return err;
 
 	if (excl_fence) {
-		err = kbase_fence_add_callback(katom,
-						excl_fence,
-						kbase_dma_fence_cb);
+		err = kbase_dma_fence_add_callback(katom,
+						   excl_fence,
+						   kbase_dma_fence_cb);
 
 		/* Release our reference, taken by reservation_object_get_fences_rcu(),
 		 * to the fence. We have set up our callback (if that was possible),
 		 * and it's the fence's owner is responsible for singling the fence
 		 * before allowing it to disappear.
 		 */
-		dma_fence_put(excl_fence);
+		fence_put(excl_fence);
 
 		if (err)
 			goto out;
@@ -258,9 +416,9 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 
 	if (exclusive) {
 		for (i = 0; i < shared_count; i++) {
-			err = kbase_fence_add_callback(katom,
-							shared_fences[i],
-							kbase_dma_fence_cb);
+			err = kbase_dma_fence_add_callback(katom,
+							   shared_fences[i],
+							   kbase_dma_fence_cb);
 			if (err)
 				goto out;
 		}
@@ -273,7 +431,7 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 	 */
 out:
 	for (i = 0; i < shared_count; i++)
-		dma_fence_put(shared_fences[i]);
+		fence_put(shared_fences[i]);
 	kfree(shared_fences);
 
 	if (err) {
@@ -281,7 +439,7 @@ out:
 		 * On error, cancel and clean up all callbacks that was set up
 		 * before the error.
 		 */
-		kbase_fence_free_callbacks(katom);
+		kbase_dma_fence_free_callbacks(katom, false);
 	}
 
 	return err;
@@ -310,16 +468,13 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 			 struct kbase_dma_fence_resv_info *info)
 {
 	int err, i;
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
 	struct fence *fence;
-#else
-	struct dma_fence *fence;
-#endif
 	struct ww_acquire_ctx ww_ctx;
 
 	lockdep_assert_held(&katom->kctx->jctx.lock);
 
-	fence = kbase_fence_out_new(katom);
+	fence = kbase_dma_fence_new(katom->dma_fence.context,
+				    atomic_inc_return(&katom->dma_fence.seqno));
 	if (!fence) {
 		err = -ENOMEM;
 		dev_err(katom->kctx->kbdev->dev,
@@ -327,14 +482,15 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 		return err;
 	}
 
-	kbase_fence_dep_count_set(katom, 1);
+	katom->dma_fence.fence = fence;
+	atomic_set(&katom->dma_fence.dep_count, 1);
 
 	err = kbase_dma_fence_lock_reservations(info, &ww_ctx);
 	if (err) {
 		dev_err(katom->kctx->kbdev->dev,
 			"Error %d locking reservations.\n", err);
-		kbase_fence_dep_count_set(katom, -1);
-		kbase_fence_out_remove(katom);
+		atomic_set(&katom->dma_fence.dep_count, -1);
+		fence_put(fence);
 		return err;
 	}
 
@@ -356,7 +512,7 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 				goto end;
 			}
 
-			reservation_object_add_shared_fence(obj, fence);
+			reservation_object_add_shared_fence(obj, katom->dma_fence.fence);
 		} else {
 			err = kbase_dma_fence_add_reservation_callback(katom, obj, true);
 			if (err) {
@@ -365,7 +521,7 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 				goto end;
 			}
 
-			reservation_object_add_excl_fence(obj, fence);
+			reservation_object_add_excl_fence(obj, katom->dma_fence.fence);
 		}
 	}
 
@@ -374,9 +530,9 @@ end:
 
 	if (likely(!err)) {
 		/* Test if the callbacks are already triggered */
-		if (kbase_fence_dep_count_dec_and_test(katom)) {
-			kbase_fence_dep_count_set(katom, -1);
-			kbase_fence_free_callbacks(katom);
+		if (atomic_dec_and_test(&katom->dma_fence.dep_count)) {
+			atomic_set(&katom->dma_fence.dep_count, -1);
+			kbase_dma_fence_free_callbacks(katom, false);
 		} else {
 			/* Add katom to the list of dma-buf fence waiting atoms
 			 * only if it is still waiting.
@@ -389,8 +545,8 @@ end:
 		 * kill it for us), signal the fence, free callbacks and the
 		 * fence.
 		 */
-		kbase_fence_free_callbacks(katom);
-		kbase_fence_dep_count_set(katom, -1);
+		kbase_dma_fence_free_callbacks(katom, false);
+		atomic_set(&katom->dma_fence.dep_count, -1);
 		kbase_dma_fence_signal(katom);
 	}
 
@@ -413,8 +569,7 @@ void kbase_dma_fence_cancel_all_atoms(struct kbase_context *kctx)
 void kbase_dma_fence_cancel_callbacks(struct kbase_jd_atom *katom)
 {
 	/* Cancel callbacks and clean up. */
-	if (kbase_fence_free_callbacks(katom))
-		kbase_dma_fence_queue_work(katom);
+	kbase_dma_fence_free_callbacks(katom, true);
 }
 
 void kbase_dma_fence_signal(struct kbase_jd_atom *katom)
@@ -422,12 +577,14 @@ void kbase_dma_fence_signal(struct kbase_jd_atom *katom)
 	if (!katom->dma_fence.fence)
 		return;
 
-	/* Signal the atom's fence. */
-	dma_fence_signal(katom->dma_fence.fence);
+	KBASE_DEBUG_ASSERT(atomic_read(&katom->dma_fence.dep_count) == -1);
 
-	kbase_fence_out_remove(katom);
+	/* Signal the atom's fence. */
+	fence_signal(katom->dma_fence.fence);
+	fence_put(katom->dma_fence.fence);
+	katom->dma_fence.fence = NULL;
 
-	kbase_fence_free_callbacks(katom);
+	kbase_dma_fence_free_callbacks(katom, false);
 }
 
 void kbase_dma_fence_term(struct kbase_context *kctx)
diff --git a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h
index c9ab403..3b0a69b 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_dma_fence.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -20,9 +20,9 @@
 
 #ifdef CONFIG_MALI_DMA_FENCE
 
+#include <linux/fence.h>
 #include <linux/list.h>
 #include <linux/reservation.h>
-#include <mali_kbase_fence.h>
 
 
 /* Forward declaration from mali_kbase_defs.h */
@@ -30,6 +30,20 @@ struct kbase_jd_atom;
 struct kbase_context;
 
 /**
+ * struct kbase_dma_fence_cb - Mali dma-fence callback data struct
+ * @fence_cb: Callback function
+ * @katom:    Pointer to katom that is waiting on this callback
+ * @fence:    Pointer to the fence object on which this callback is waiting
+ * @node:     List head for linking this callback to the katom
+ */
+struct kbase_dma_fence_cb {
+	struct fence_cb fence_cb;
+	struct kbase_jd_atom *katom;
+	struct fence *fence;
+	struct list_head node;
+};
+
+/**
  * struct kbase_dma_fence_resv_info - Structure with list of reservation objects
  * @resv_objs:             Array of reservation objects to attach the
  *                         new fence to.
@@ -117,6 +131,11 @@ void kbase_dma_fence_term(struct kbase_context *kctx);
  */
 int kbase_dma_fence_init(struct kbase_context *kctx);
 
+/**
+ * kbase_dma_fence_waiters_remove()- Remove katom from dma-fence wait list
+ * @katom: Pointer to katom to remove from list
+ */
+void kbase_dma_fence_waiters_remove(struct kbase_jd_atom *katom);
 
 #else /* CONFIG_MALI_DMA_FENCE */
 /* Dummy functions for when dma-buf fence isn't enabled. */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_event.c b/drivers/gpu/arm/midgard/mali_kbase_event.c
index 1881486..f07406c 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_event.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_event.c
@@ -35,8 +35,8 @@ static struct base_jd_udata kbase_event_process(struct kbase_context *kctx, stru
 
 	KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_sub_return(1, &kctx->timeline.jd_atoms_in_flight));
 
-	KBASE_TLSTREAM_TL_NRET_ATOM_CTX(katom, kctx);
-	KBASE_TLSTREAM_TL_DEL_ATOM(katom);
+	kbase_tlstream_tl_nret_atom_ctx(katom, kctx);
+	kbase_tlstream_tl_del_atom(katom);
 
 	katom->status = KBASE_JD_ATOM_STATE_UNUSED;
 
@@ -180,7 +180,7 @@ void kbase_event_post(struct kbase_context *ctx, struct kbase_jd_atom *atom)
 		kbase_event_process_noreport(ctx, atom);
 		return;
 	}
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(atom, TL_ATOM_STATE_POSTED);
+	kbase_tlstream_tl_attrib_atom_state(atom, TL_ATOM_STATE_POSTED);
 	if (atom->core_req & BASE_JD_REQ_EVENT_COALESCE) {
 		/* Don't report the event until other event(s) have completed */
 		mutex_lock(&ctx->event_mutex);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_fence.c b/drivers/gpu/arm/midgard/mali_kbase_fence.c
deleted file mode 100644
index fcb3733..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_fence.c
+++ /dev/null
@@ -1,196 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#include <linux/atomic.h>
-#include <linux/list.h>
-#include <linux/spinlock.h>
-#include <mali_kbase_fence_defs.h>
-#include <mali_kbase_fence.h>
-#include <mali_kbase.h>
-
-/* Spin lock protecting all Mali fences as fence->lock. */
-static DEFINE_SPINLOCK(kbase_fence_lock);
-
-static const char *
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_get_driver_name(struct fence *fence)
-#else
-kbase_fence_get_driver_name(struct dma_fence *fence)
-#endif
-{
-	return kbase_drv_name;
-}
-
-static const char *
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_get_timeline_name(struct fence *fence)
-#else
-kbase_fence_get_timeline_name(struct dma_fence *fence)
-#endif
-{
-	return kbase_timeline_name;
-}
-
-static bool
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_enable_signaling(struct fence *fence)
-#else
-kbase_fence_enable_signaling(struct dma_fence *fence)
-#endif
-{
-	return true;
-}
-
-static void
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_fence_value_str(struct fence *fence, char *str, int size)
-#else
-kbase_fence_fence_value_str(struct dma_fence *fence, char *str, int size)
-#endif
-{
-	snprintf(str, size, "%u", fence->seqno);
-}
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-const struct fence_ops kbase_fence_ops = {
-	.wait = fence_default_wait,
-#else
-const struct dma_fence_ops kbase_fence_ops = {
-	.wait = dma_fence_default_wait,
-#endif
-	.get_driver_name = kbase_fence_get_driver_name,
-	.get_timeline_name = kbase_fence_get_timeline_name,
-	.enable_signaling = kbase_fence_enable_signaling,
-	.fence_value_str = kbase_fence_fence_value_str
-};
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-struct fence *
-kbase_fence_out_new(struct kbase_jd_atom *katom)
-#else
-struct dma_fence *
-kbase_fence_out_new(struct kbase_jd_atom *katom)
-#endif
-{
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence;
-#else
-	struct dma_fence *fence;
-#endif
-
-	WARN_ON(katom->dma_fence.fence);
-
-	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
-	if (!fence)
-		return NULL;
-
-	dma_fence_init(fence,
-		       &kbase_fence_ops,
-		       &kbase_fence_lock,
-		       katom->dma_fence.context,
-		       atomic_inc_return(&katom->dma_fence.seqno));
-
-	katom->dma_fence.fence = fence;
-
-	return fence;
-}
-
-bool
-kbase_fence_free_callbacks(struct kbase_jd_atom *katom)
-{
-	struct kbase_fence_cb *cb, *tmp;
-	bool res = false;
-
-	lockdep_assert_held(&katom->kctx->jctx.lock);
-
-	/* Clean up and free callbacks. */
-	list_for_each_entry_safe(cb, tmp, &katom->dma_fence.callbacks, node) {
-		bool ret;
-
-		/* Cancel callbacks that hasn't been called yet. */
-		ret = dma_fence_remove_callback(cb->fence, &cb->fence_cb);
-		if (ret) {
-			int ret;
-
-			/* Fence had not signaled, clean up after
-			 * canceling.
-			 */
-			ret = atomic_dec_return(&katom->dma_fence.dep_count);
-
-			if (unlikely(ret == 0))
-				res = true;
-		}
-
-		/*
-		 * Release the reference taken in
-		 * kbase_fence_add_callback().
-		 */
-		dma_fence_put(cb->fence);
-		list_del(&cb->node);
-		kfree(cb);
-	}
-
-	return res;
-}
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-int
-kbase_fence_add_callback(struct kbase_jd_atom *katom,
-			 struct fence *fence,
-			 fence_func_t callback)
-#else
-int
-kbase_fence_add_callback(struct kbase_jd_atom *katom,
-			 struct dma_fence *fence,
-			 dma_fence_func_t callback)
-#endif
-{
-	int err = 0;
-	struct kbase_fence_cb *kbase_fence_cb;
-
-	if (!fence)
-		return -EINVAL;
-
-	kbase_fence_cb = kmalloc(sizeof(*kbase_fence_cb), GFP_KERNEL);
-	if (!kbase_fence_cb)
-		return -ENOMEM;
-
-	kbase_fence_cb->fence = fence;
-	kbase_fence_cb->katom = katom;
-	INIT_LIST_HEAD(&kbase_fence_cb->node);
-
-	err = dma_fence_add_callback(fence, &kbase_fence_cb->fence_cb,
-				     callback);
-	if (err == -ENOENT) {
-		/* Fence signaled, clear the error and return */
-		err = 0;
-		kfree(kbase_fence_cb);
-	} else if (err) {
-		kfree(kbase_fence_cb);
-	} else {
-		/*
-		 * Get reference to fence that will be kept until callback gets
-		 * cleaned up in kbase_fence_free_callbacks().
-		 */
-		dma_fence_get(fence);
-		atomic_inc(&katom->dma_fence.dep_count);
-		/* Add callback to katom's list of callbacks */
-		list_add(&kbase_fence_cb->node, &katom->dma_fence.callbacks);
-	}
-
-	return err;
-}
diff --git a/drivers/gpu/arm/midgard/mali_kbase_fence.h b/drivers/gpu/arm/midgard/mali_kbase_fence.h
deleted file mode 100644
index 8d39299..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_fence.h
+++ /dev/null
@@ -1,266 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KBASE_FENCE_H_
-#define _KBASE_FENCE_H_
-
-/*
- * mali_kbase_fence.[hc] has common fence code used by both
- * - CONFIG_MALI_DMA_FENCE - implicit DMA fences
- * - CONFIG_SYNC_FILE      - explicit fences beginning with 4.9 kernel
- */
-
-#if defined(CONFIG_MALI_DMA_FENCE) || defined(CONFIG_SYNC_FILE)
-
-#include <linux/list.h>
-#include "mali_kbase_fence_defs.h"
-#include "mali_kbase.h"
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-extern const struct fence_ops kbase_fence_ops;
-#else
-extern const struct dma_fence_ops kbase_fence_ops;
-#endif
-
-/**
-* struct kbase_fence_cb - Mali dma-fence callback data struct
-* @fence_cb: Callback function
-* @katom:    Pointer to katom that is waiting on this callback
-* @fence:    Pointer to the fence object on which this callback is waiting
-* @node:     List head for linking this callback to the katom
-*/
-struct kbase_fence_cb {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence_cb fence_cb;
-	struct fence *fence;
-#else
-	struct dma_fence_cb fence_cb;
-	struct dma_fence *fence;
-#endif
-	struct kbase_jd_atom *katom;
-	struct list_head node;
-};
-
-/**
- * kbase_fence_out_new() - Creates a new output fence and puts it on the atom
- * @katom: Atom to create an output fence for
- *
- * return: A new fence object on success, NULL on failure.
- */
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-struct fence *kbase_fence_out_new(struct kbase_jd_atom *katom);
-#else
-struct dma_fence *kbase_fence_out_new(struct kbase_jd_atom *katom);
-#endif
-
-#if defined(CONFIG_SYNC_FILE)
-/**
- * kbase_fence_fence_in_set() - Assign input fence to atom
- * @katom: Atom to assign input fence to
- * @fence: Input fence to assign to atom
- *
- * This function will take ownership of one fence reference!
- */
-#define kbase_fence_fence_in_set(katom, fence) \
-	do { \
-		WARN_ON((katom)->dma_fence.fence_in); \
-		(katom)->dma_fence.fence_in = fence; \
-	} while (0)
-#endif
-
-/**
- * kbase_fence_out_remove() - Removes the output fence from atom
- * @katom: Atom to remove output fence for
- *
- * This will also release the reference to this fence which the atom keeps
- */
-static inline void kbase_fence_out_remove(struct kbase_jd_atom *katom)
-{
-	if (katom->dma_fence.fence) {
-		dma_fence_put(katom->dma_fence.fence);
-		katom->dma_fence.fence = NULL;
-	}
-}
-
-#if defined(CONFIG_SYNC_FILE)
-/**
- * kbase_fence_out_remove() - Removes the input fence from atom
- * @katom: Atom to remove input fence for
- *
- * This will also release the reference to this fence which the atom keeps
- */
-static inline void kbase_fence_in_remove(struct kbase_jd_atom *katom)
-{
-	if (katom->dma_fence.fence_in) {
-		dma_fence_put(katom->dma_fence.fence_in);
-		katom->dma_fence.fence_in = NULL;
-	}
-}
-#endif
-
-/**
- * kbase_fence_out_is_ours() - Check if atom has a valid fence created by us
- * @katom: Atom to check output fence for
- *
- * Return: true if fence exists and is valid, otherwise false
- */
-static inline bool kbase_fence_out_is_ours(struct kbase_jd_atom *katom)
-{
-	return katom->dma_fence.fence &&
-				katom->dma_fence.fence->ops == &kbase_fence_ops;
-}
-
-/**
- * kbase_fence_out_signal() - Signal output fence of atom
- * @katom: Atom to signal output fence for
- * @status: Status to signal with (0 for success, < 0 for error)
- *
- * Return: 0 on success, < 0 on error
- */
-static inline int kbase_fence_out_signal(struct kbase_jd_atom *katom,
-					 int status)
-{
-	katom->dma_fence.fence->status = status;
-	return dma_fence_signal(katom->dma_fence.fence);
-}
-
-/**
- * kbase_fence_add_callback() - Add callback on @fence to block @katom
- * @katom: Pointer to katom that will be blocked by @fence
- * @fence: Pointer to fence on which to set up the callback
- * @callback: Pointer to function to be called when fence is signaled
- *
- * Caller needs to hold a reference to @fence when calling this function, and
- * the caller is responsible for releasing that reference.  An additional
- * reference to @fence will be taken when the callback was successfully set up
- * and @fence needs to be kept valid until the callback has been called and
- * cleanup have been done.
- *
- * Return: 0 on success: fence was either already signaled, or callback was
- * set up. Negative error code is returned on error.
- */
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-int kbase_fence_add_callback(struct kbase_jd_atom *katom,
-			     struct fence *fence,
-			     fence_func_t callback);
-#else
-int kbase_fence_add_callback(struct kbase_jd_atom *katom,
-			     struct dma_fence *fence,
-			     dma_fence_func_t callback);
-#endif
-
-/**
- * kbase_fence_dep_count_set() - Set dep_count value on atom to specified value
- * @katom: Atom to set dep_count for
- * @val: value to set dep_count to
- *
- * The dep_count is available to the users of this module so that they can
- * synchronize completion of the wait with cancellation and adding of more
- * callbacks. For instance, a user could do the following:
- *
- * dep_count set to 1
- * callback #1 added, dep_count is increased to 2
- *                             callback #1 happens, dep_count decremented to 1
- *                             since dep_count > 0, no completion is done
- * callback #2 is added, dep_count is increased to 2
- * dep_count decremented to 1
- *                             callback #2 happens, dep_count decremented to 0
- *                             since dep_count now is zero, completion executes
- *
- * The dep_count can also be used to make sure that the completion only
- * executes once. This is typically done by setting dep_count to -1 for the
- * thread that takes on this responsibility.
- */
-static inline void
-kbase_fence_dep_count_set(struct kbase_jd_atom *katom, int val)
-{
-	atomic_set(&katom->dma_fence.dep_count, val);
-}
-
-/**
- * kbase_fence_dep_count_dec_and_test() - Decrements dep_count
- * @katom: Atom to decrement dep_count for
- *
- * See @kbase_fence_dep_count_set for general description about dep_count
- *
- * Return: true if value was decremented to zero, otherwise false
- */
-static inline bool
-kbase_fence_dep_count_dec_and_test(struct kbase_jd_atom *katom)
-{
-	return atomic_dec_and_test(&katom->dma_fence.dep_count);
-}
-
-/**
- * kbase_fence_dep_count_read() - Returns the current dep_count value
- * @katom: Pointer to katom
- *
- * See @kbase_fence_dep_count_set for general description about dep_count
- *
- * Return: The current dep_count value
- */
-static inline int kbase_fence_dep_count_read(struct kbase_jd_atom *katom)
-{
-	return atomic_read(&katom->dma_fence.dep_count);
-}
-
-/**
- * kbase_fence_free_callbacks() - Free dma-fence callbacks on a katom
- * @katom: Pointer to katom
- *
- * This function will free all fence callbacks on the katom's list of
- * callbacks. Callbacks that have not yet been called, because their fence
- * hasn't yet signaled, will first be removed from the fence.
- *
- * Locking: katom->dma_fence.callbacks list assumes jctx.lock is held.
- *
- * Return: true if dep_count reached 0, otherwise false.
- */
-bool kbase_fence_free_callbacks(struct kbase_jd_atom *katom);
-
-#if defined(CONFIG_SYNC_FILE)
-/**
- * kbase_fence_in_get() - Retrieve input fence for atom.
- * @katom: Atom to get input fence from
- *
- * A ref will be taken for the fence, so use @kbase_fence_put() to release it
- *
- * Return: The fence, or NULL if there is no input fence for atom
- */
-#define kbase_fence_in_get(katom) dma_fence_get((katom)->dma_fence.fence_in)
-#endif
-
-/**
- * kbase_fence_out_get() - Retrieve output fence for atom.
- * @katom: Atom to get output fence from
- *
- * A ref will be taken for the fence, so use @kbase_fence_put() to release it
- *
- * Return: The fence, or NULL if there is no output fence for atom
- */
-#define kbase_fence_out_get(katom) dma_fence_get((katom)->dma_fence.fence)
-
-/**
- * kbase_fence_put() - Releases a reference to a fence
- * @fence: Fence to release reference for.
- */
-#define kbase_fence_put(fence) dma_fence_put(fence)
-
-
-#endif /* CONFIG_MALI_DMA_FENCE || defined(CONFIG_SYNC_FILE */
-
-#endif /* _KBASE_FENCE_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_fence_defs.h b/drivers/gpu/arm/midgard/mali_kbase_fence_defs.h
deleted file mode 100644
index fa2c6df..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_fence_defs.h
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KBASE_FENCE_DEFS_H_
-#define _KBASE_FENCE_DEFS_H_
-
-/*
- * There was a big rename in the 4.10 kernel (fence* -> dma_fence*)
- * This file hides the compatibility issues with this for the rest the driver
- */
-
-#if defined(CONFIG_MALI_DMA_FENCE) || defined(CONFIG_SYNC_FILE)
-
-#include <linux/version.h>
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-
-#include <linux/fence.h>
-
-#define dma_fence_context_alloc(a) fence_context_alloc(a)
-#define dma_fence_init(a, b, c, d, e) fence_init(a, b, c, d, e)
-#define dma_fence_get(a) fence_get(a)
-#define dma_fence_put(a) fence_put(a)
-#define dma_fence_signal(a) fence_signal(a)
-#define dma_fence_is_signaled(a) fence_is_signaled(a)
-#define dma_fence_add_callback(a, b, c) fence_add_callback(a, b, c)
-#define dma_fence_remove_callback(a, b) fence_remove_callback(a, b)
-
-#else
-
-#include <linux/dma-fence.h>
-
-#endif /* < 4.10.0 */
-
-#endif /* CONFIG_MALI_DMA_FENCE || CONFIG_SYNC_FILE */
-
-#endif /* _KBASE_FENCE_DEFS_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gator_api.c b/drivers/gpu/arm/midgard/mali_kbase_gator_api.c
index 860e101..3292fa9 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_gator_api.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_gator_api.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -64,10 +64,6 @@ const char * const *kbase_gator_hwcnt_init_names(uint32_t *total_counters)
 			hardware_counters = hardware_counters_mali_tHEx;
 			count = ARRAY_SIZE(hardware_counters_mali_tHEx);
 			break;
-		case GPU_ID2_PRODUCT_TSIX:
-			hardware_counters = hardware_counters_mali_tSIx;
-			count = ARRAY_SIZE(hardware_counters_mali_tSIx);
-			break;
 		default:
 			hardware_counters = NULL;
 			count = 0;
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names.h b/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names.h
index cad19b6..7ec05c1 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -2160,11 +2160,5 @@ static const char * const hardware_counters_mali_t88x[] = {
 
 #include "mali_kbase_gator_hwcnt_names_thex.h"
 
-#include "mali_kbase_gator_hwcnt_names_tsix.h"
-
-
-#ifdef MALI_INCLUDE_TKAX
-#include "mali_kbase_gator_hwcnt_names_tkax.h"
-#endif /* MALI_INCLUDE_TKAX */
 
 #endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names_tsix.h b/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names_tsix.h
deleted file mode 100644
index be09c45..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_gator_hwcnt_names_tsix.h
+++ /dev/null
@@ -1,291 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/*
- * This header was autogenerated, it should not be edited.
- */
-
-#ifndef _KBASE_GATOR_HWCNT_NAMES_TSIX_H_
-#define _KBASE_GATOR_HWCNT_NAMES_TSIX_H_
-
-static const char * const hardware_counters_mali_tSIx[] = {
-	/* Performance counters for the Job Manager */
-	"",
-	"",
-	"",
-	"",
-	"TSIx_MESSAGES_SENT",
-	"TSIx_MESSAGES_RECEIVED",
-	"TSIx_GPU_ACTIVE",
-	"TSIx_IRQ_ACTIVE",
-	"TSIx_JS0_JOBS",
-	"TSIx_JS0_TASKS",
-	"TSIx_JS0_ACTIVE",
-	"",
-	"TSIx_JS0_WAIT_READ",
-	"TSIx_JS0_WAIT_ISSUE",
-	"TSIx_JS0_WAIT_DEPEND",
-	"TSIx_JS0_WAIT_FINISH",
-	"TSIx_JS1_JOBS",
-	"TSIx_JS1_TASKS",
-	"TSIx_JS1_ACTIVE",
-	"",
-	"TSIx_JS1_WAIT_READ",
-	"TSIx_JS1_WAIT_ISSUE",
-	"TSIx_JS1_WAIT_DEPEND",
-	"TSIx_JS1_WAIT_FINISH",
-	"TSIx_JS2_JOBS",
-	"TSIx_JS2_TASKS",
-	"TSIx_JS2_ACTIVE",
-	"",
-	"TSIx_JS2_WAIT_READ",
-	"TSIx_JS2_WAIT_ISSUE",
-	"TSIx_JS2_WAIT_DEPEND",
-	"TSIx_JS2_WAIT_FINISH",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-
-	/* Performance counters for the Tiler */
-	"",
-	"",
-	"",
-	"",
-	"TSIx_TILER_ACTIVE",
-	"TSIx_JOBS_PROCESSED",
-	"TSIx_TRIANGLES",
-	"TSIx_LINES",
-	"TSIx_POINTS",
-	"TSIx_FRONT_FACING",
-	"TSIx_BACK_FACING",
-	"TSIx_PRIM_VISIBLE",
-	"TSIx_PRIM_CULLED",
-	"TSIx_PRIM_CLIPPED",
-	"TSIx_PRIM_SAT_CULLED",
-	"",
-	"",
-	"TSIx_BUS_READ",
-	"",
-	"TSIx_BUS_WRITE",
-	"TSIx_LOADING_DESC",
-	"TSIx_IDVS_POS_SHAD_REQ",
-	"TSIx_IDVS_POS_SHAD_WAIT",
-	"TSIx_IDVS_POS_SHAD_STALL",
-	"TSIx_IDVS_POS_FIFO_FULL",
-	"TSIx_PREFETCH_STALL",
-	"TSIx_VCACHE_HIT",
-	"TSIx_VCACHE_MISS",
-	"TSIx_VCACHE_LINE_WAIT",
-	"TSIx_VFETCH_POS_READ_WAIT",
-	"TSIx_VFETCH_VERTEX_WAIT",
-	"TSIx_VFETCH_STALL",
-	"TSIx_PRIMASSY_STALL",
-	"TSIx_BBOX_GEN_STALL",
-	"TSIx_IDVS_VBU_HIT",
-	"TSIx_IDVS_VBU_MISS",
-	"TSIx_IDVS_VBU_LINE_DEALLOCATE",
-	"TSIx_IDVS_VAR_SHAD_REQ",
-	"TSIx_IDVS_VAR_SHAD_STALL",
-	"TSIx_BINNER_STALL",
-	"TSIx_ITER_STALL",
-	"TSIx_COMPRESS_MISS",
-	"TSIx_COMPRESS_STALL",
-	"TSIx_PCACHE_HIT",
-	"TSIx_PCACHE_MISS",
-	"TSIx_PCACHE_MISS_STALL",
-	"TSIx_PCACHE_EVICT_STALL",
-	"TSIx_PMGR_PTR_WR_STALL",
-	"TSIx_PMGR_PTR_RD_STALL",
-	"TSIx_PMGR_CMD_WR_STALL",
-	"TSIx_WRBUF_ACTIVE",
-	"TSIx_WRBUF_HIT",
-	"TSIx_WRBUF_MISS",
-	"TSIx_WRBUF_NO_FREE_LINE_STALL",
-	"TSIx_WRBUF_NO_AXI_ID_STALL",
-	"TSIx_WRBUF_AXI_STALL",
-	"",
-	"",
-	"",
-	"TSIx_UTLB_TRANS",
-	"TSIx_UTLB_TRANS_HIT",
-	"TSIx_UTLB_TRANS_STALL",
-	"TSIx_UTLB_TRANS_MISS_DELAY",
-	"TSIx_UTLB_MMU_REQ",
-
-	/* Performance counters for the Shader Core */
-	"",
-	"",
-	"",
-	"",
-	"TSIx_FRAG_ACTIVE",
-	"TSIx_FRAG_PRIMITIVES",
-	"TSIx_FRAG_PRIM_RAST",
-	"TSIx_FRAG_FPK_ACTIVE",
-	"TSIx_FRAG_STARVING",
-	"TSIx_FRAG_WARPS",
-	"TSIx_FRAG_PARTIAL_WARPS",
-	"TSIx_FRAG_QUADS_RAST",
-	"TSIx_FRAG_QUADS_EZS_TEST",
-	"TSIx_FRAG_QUADS_EZS_UPDATE",
-	"TSIx_FRAG_QUADS_EZS_KILL",
-	"TSIx_FRAG_LZS_TEST",
-	"TSIx_FRAG_LZS_KILL",
-	"",
-	"TSIx_FRAG_PTILES",
-	"TSIx_FRAG_TRANS_ELIM",
-	"TSIx_QUAD_FPK_KILLER",
-	"",
-	"TSIx_COMPUTE_ACTIVE",
-	"TSIx_COMPUTE_TASKS",
-	"TSIx_COMPUTE_WARPS",
-	"TSIx_COMPUTE_STARVING",
-	"TSIx_EXEC_CORE_ACTIVE",
-	"TSIx_EXEC_ACTIVE",
-	"TSIx_EXEC_INSTR_COUNT",
-	"TSIx_EXEC_INSTR_DIVERGED",
-	"TSIx_EXEC_INSTR_STARVING",
-	"TSIx_ARITH_INSTR_SINGLE_FMA",
-	"TSIx_ARITH_INSTR_DOUBLE",
-	"TSIx_ARITH_INSTR_MSG",
-	"TSIx_ARITH_INSTR_MSG_ONLY",
-	"TSIx_TEX_MSGI_NUM_QUADS",
-	"TSIx_TEX_DFCH_NUM_PASSES",
-	"TSIx_TEX_DFCH_NUM_PASSES_MISS",
-	"TSIx_TEX_DFCH_NUM_PASSES_MIP_MAP",
-	"TSIx_TEX_TIDX_NUM_SPLIT_MIP_MAP",
-	"TSIx_TEX_TFCH_NUM_LINES_FETCHED",
-	"TSIx_TEX_TFCH_NUM_LINES_FETCHED_BLOCK_COMPRESSED",
-	"TSIx_TEX_TFCH_NUM_OPERATIONS",
-	"TSIx_TEX_FILT_NUM_OPERATIONS",
-	"TSIx_LS_MEM_READ_FULL",
-	"TSIx_LS_MEM_READ_SHORT",
-	"TSIx_LS_MEM_WRITE_FULL",
-	"TSIx_LS_MEM_WRITE_SHORT",
-	"TSIx_LS_MEM_ATOMIC",
-	"TSIx_VARY_INSTR",
-	"TSIx_VARY_SLOT_32",
-	"TSIx_VARY_SLOT_16",
-	"TSIx_ATTR_INSTR",
-	"TSIx_ARITH_INSTR_FP_MUL",
-	"TSIx_BEATS_RD_FTC",
-	"TSIx_BEATS_RD_FTC_EXT",
-	"TSIx_BEATS_RD_LSC",
-	"TSIx_BEATS_RD_LSC_EXT",
-	"TSIx_BEATS_RD_TEX",
-	"TSIx_BEATS_RD_TEX_EXT",
-	"TSIx_BEATS_RD_OTHER",
-	"TSIx_BEATS_WR_LSC",
-	"TSIx_BEATS_WR_TIB",
-	"",
-
-	/* Performance counters for the Memory System */
-	"",
-	"",
-	"",
-	"",
-	"TSIx_MMU_REQUESTS",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"TSIx_L2_RD_MSG_IN",
-	"TSIx_L2_RD_MSG_IN_STALL",
-	"TSIx_L2_WR_MSG_IN",
-	"TSIx_L2_WR_MSG_IN_STALL",
-	"TSIx_L2_SNP_MSG_IN",
-	"TSIx_L2_SNP_MSG_IN_STALL",
-	"TSIx_L2_RD_MSG_OUT",
-	"TSIx_L2_RD_MSG_OUT_STALL",
-	"TSIx_L2_WR_MSG_OUT",
-	"TSIx_L2_ANY_LOOKUP",
-	"TSIx_L2_READ_LOOKUP",
-	"TSIx_L2_WRITE_LOOKUP",
-	"TSIx_L2_EXT_SNOOP_LOOKUP",
-	"TSIx_L2_EXT_READ",
-	"TSIx_L2_EXT_READ_NOSNP",
-	"TSIx_L2_EXT_READ_UNIQUE",
-	"TSIx_L2_EXT_READ_BEATS",
-	"TSIx_L2_EXT_AR_STALL",
-	"TSIx_L2_EXT_AR_CNT_Q1",
-	"TSIx_L2_EXT_AR_CNT_Q2",
-	"TSIx_L2_EXT_AR_CNT_Q3",
-	"TSIx_L2_EXT_RRESP_0_127",
-	"TSIx_L2_EXT_RRESP_128_191",
-	"TSIx_L2_EXT_RRESP_192_255",
-	"TSIx_L2_EXT_RRESP_256_319",
-	"TSIx_L2_EXT_RRESP_320_383",
-	"TSIx_L2_EXT_WRITE",
-	"TSIx_L2_EXT_WRITE_NOSNP_FULL",
-	"TSIx_L2_EXT_WRITE_NOSNP_PTL",
-	"TSIx_L2_EXT_WRITE_SNP_FULL",
-	"TSIx_L2_EXT_WRITE_SNP_PTL",
-	"TSIx_L2_EXT_WRITE_BEATS",
-	"TSIx_L2_EXT_W_STALL",
-	"TSIx_L2_EXT_AW_CNT_Q1",
-	"TSIx_L2_EXT_AW_CNT_Q2",
-	"TSIx_L2_EXT_AW_CNT_Q3",
-	"TSIx_L2_EXT_SNOOP",
-	"TSIx_L2_EXT_SNOOP_STALL",
-	"TSIx_L2_EXT_SNOOP_RESP_CLEAN",
-	"TSIx_L2_EXT_SNOOP_RESP_DATA",
-	"TSIx_L2_EXT_SNOOP_INTERNAL",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-	"",
-};
-
-#endif /* _KBASE_GATOR_HWCNT_NAMES_TSIX_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gpu_id.h b/drivers/gpu/arm/midgard/mali_kbase_gpu_id.h
index 42f0111..a3377b2 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_gpu_id.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_gpu_id.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -58,9 +58,6 @@
 #define GPU_ID2_ARCH_MINOR                (0xF << GPU_ID2_ARCH_MINOR_SHIFT)
 #define GPU_ID2_ARCH_MAJOR                (0xF << GPU_ID2_ARCH_MAJOR_SHIFT)
 #define GPU_ID2_PRODUCT_MODEL  (GPU_ID2_ARCH_MAJOR | GPU_ID2_PRODUCT_MAJOR)
-#define GPU_ID2_VERSION        (GPU_ID2_VERSION_MAJOR | \
-								GPU_ID2_VERSION_MINOR | \
-								GPU_ID2_VERSION_STATUS)
 
 /* Helper macro to create a partial GPU_ID (new format) that defines
    a product ignoring its version. */
@@ -98,15 +95,8 @@
 		(((product_id) << GPU_ID2_PRODUCT_MAJOR_SHIFT) & \
 		    GPU_ID2_PRODUCT_MODEL)
 
-#define GPU_ID2_PRODUCT_TMIX              GPU_ID2_MODEL_MAKE(6u, 0)
-#define GPU_ID2_PRODUCT_THEX              GPU_ID2_MODEL_MAKE(6u, 1)
-#define GPU_ID2_PRODUCT_TSIX              GPU_ID2_MODEL_MAKE(7u, 0)
-#ifdef MALI_INCLUDE_TKAX
-#define GPU_ID2_PRODUCT_TKAX              GPU_ID2_MODEL_MAKE(9u, 0)
-#endif /* MALI_INCLUDE_TKAX */
-#ifdef MALI_INCLUDE_TTRX
-#define GPU_ID2_PRODUCT_TTRX              GPU_ID2_MODEL_MAKE(10u, 0)
-#endif /* MALI_INCLUDE_TTRX */
+#define GPU_ID2_PRODUCT_TMIX              GPU_ID2_MODEL_MAKE(6, 0)
+#define GPU_ID2_PRODUCT_THEX              GPU_ID2_MODEL_MAKE(6, 1)
 
 /* Values for GPU_ID_VERSION_STATUS field for PRODUCT_ID GPU_ID_PI_T60X */
 #define GPU_ID_S_15DEV0                   0x1
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gpuprops.c b/drivers/gpu/arm/midgard/mali_kbase_gpuprops.c
index e2f4209..7f77dba 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_gpuprops.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_gpuprops.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2015 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,7 +26,6 @@
 #include <mali_kbase_gpuprops.h>
 #include <mali_kbase_config_defaults.h>
 #include <mali_kbase_hwaccess_gpuprops.h>
-#include "mali_kbase_ioctl.h"
 #include <linux/clk.h>
 
 /**
@@ -186,22 +185,9 @@ static void kbase_gpuprops_get_props(base_gpu_props * const gpu_props, struct kb
 
 	gpu_props->raw_props.as_present = regdump.as_present;
 	gpu_props->raw_props.js_present = regdump.js_present;
-	gpu_props->raw_props.shader_present =
-		((u64) regdump.shader_present_hi << 32) +
-		regdump.shader_present_lo;
-	gpu_props->raw_props.tiler_present =
-		((u64) regdump.tiler_present_hi << 32) +
-		regdump.tiler_present_lo;
-	gpu_props->raw_props.l2_present =
-		((u64) regdump.l2_present_hi << 32) +
-		regdump.l2_present_lo;
-#ifdef CONFIG_MALI_CORESTACK
-	gpu_props->raw_props.stack_present =
-		((u64) regdump.stack_present_hi << 32) +
-		regdump.stack_present_lo;
-#else /* CONFIG_MALI_CORESTACK */
-	gpu_props->raw_props.stack_present = 0;
-#endif /* CONFIG_MALI_CORESTACK */
+	gpu_props->raw_props.shader_present = ((u64) regdump.shader_present_hi << 32) + regdump.shader_present_lo;
+	gpu_props->raw_props.tiler_present = ((u64) regdump.tiler_present_hi << 32) + regdump.tiler_present_lo;
+	gpu_props->raw_props.l2_present = ((u64) regdump.l2_present_hi << 32) + regdump.l2_present_lo;
 
 	for (i = 0; i < GPU_MAX_JOB_SLOTS; i++)
 		gpu_props->raw_props.js_features[i] = regdump.js_features[i];
@@ -215,14 +201,6 @@ static void kbase_gpuprops_get_props(base_gpu_props * const gpu_props, struct kb
 	gpu_props->raw_props.thread_features = regdump.thread_features;
 }
 
-void kbase_gpuprops_update_core_props_gpu_id(base_gpu_props * const gpu_props)
-{
-	gpu_props->core_props.version_status = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 0U, 4);
-	gpu_props->core_props.minor_revision = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 4U, 8);
-	gpu_props->core_props.major_revision = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 12U, 4);
-	gpu_props->core_props.product_id = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 16U, 16);
-}
-
 /**
  * kbase_gpuprops_calculate_props - Calculate the derived properties
  * @gpu_props: The &base_gpu_props structure
@@ -236,7 +214,10 @@ static void kbase_gpuprops_calculate_props(base_gpu_props * const gpu_props, str
 	int i;
 
 	/* Populate the base_gpu_props structure */
-	kbase_gpuprops_update_core_props_gpu_id(gpu_props);
+	gpu_props->core_props.version_status = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 0U, 4);
+	gpu_props->core_props.minor_revision = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 4U, 8);
+	gpu_props->core_props.major_revision = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 12U, 4);
+	gpu_props->core_props.product_id = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 16U, 16);
 	gpu_props->core_props.log2_program_counter_size = KBASE_GPU_PC_SIZE_LOG2;
 	gpu_props->core_props.gpu_available_memory_size = totalram_pages << PAGE_SHIFT;
 
@@ -328,183 +309,6 @@ void kbase_gpuprops_set_features(struct kbase_device *kbdev)
 	/*
 	 * Copy the raw value from the register, later this will get turned
 	 * into the selected coherency mode.
-	 * Additionally, add non-coherent mode, as this is always supported.
 	 */
-	gpu_props->raw_props.coherency_mode = regdump.coherency_features |
-		COHERENCY_FEATURE_BIT(COHERENCY_NONE);
-}
-
-static struct {
-	u32 type;
-	size_t offset;
-	int size;
-} gpu_property_mapping[] = {
-#define PROP(name, member) \
-	{KBASE_GPUPROP_ ## name, offsetof(struct mali_base_gpu_props, member), \
-		sizeof(((struct mali_base_gpu_props *)0)->member)}
-	PROP(PRODUCT_ID,                  core_props.product_id),
-	PROP(VERSION_STATUS,              core_props.version_status),
-	PROP(MINOR_REVISION,              core_props.minor_revision),
-	PROP(MAJOR_REVISION,              core_props.major_revision),
-	PROP(GPU_SPEED_MHZ,               core_props.gpu_speed_mhz),
-	PROP(GPU_FREQ_KHZ_MAX,            core_props.gpu_freq_khz_max),
-	PROP(GPU_FREQ_KHZ_MIN,            core_props.gpu_freq_khz_min),
-	PROP(LOG2_PROGRAM_COUNTER_SIZE,   core_props.log2_program_counter_size),
-	PROP(TEXTURE_FEATURES_0,          core_props.texture_features[0]),
-	PROP(TEXTURE_FEATURES_1,          core_props.texture_features[1]),
-	PROP(TEXTURE_FEATURES_2,          core_props.texture_features[2]),
-	PROP(GPU_AVAILABLE_MEMORY_SIZE,   core_props.gpu_available_memory_size),
-
-	PROP(L2_LOG2_LINE_SIZE,           l2_props.log2_line_size),
-	PROP(L2_LOG2_CACHE_SIZE,          l2_props.log2_cache_size),
-	PROP(L2_NUM_L2_SLICES,            l2_props.num_l2_slices),
-
-	PROP(TILER_BIN_SIZE_BYTES,        tiler_props.bin_size_bytes),
-	PROP(TILER_MAX_ACTIVE_LEVELS,     tiler_props.max_active_levels),
-
-	PROP(MAX_THREADS,                 thread_props.max_threads),
-	PROP(MAX_WORKGROUP_SIZE,          thread_props.max_workgroup_size),
-	PROP(MAX_BARRIER_SIZE,            thread_props.max_barrier_size),
-	PROP(MAX_REGISTERS,               thread_props.max_registers),
-	PROP(MAX_TASK_QUEUE,              thread_props.max_task_queue),
-	PROP(MAX_THREAD_GROUP_SPLIT,      thread_props.max_thread_group_split),
-	PROP(IMPL_TECH,                   thread_props.impl_tech),
-
-	PROP(RAW_SHADER_PRESENT,          raw_props.shader_present),
-	PROP(RAW_TILER_PRESENT,           raw_props.tiler_present),
-	PROP(RAW_L2_PRESENT,              raw_props.l2_present),
-	PROP(RAW_STACK_PRESENT,           raw_props.stack_present),
-	PROP(RAW_L2_FEATURES,             raw_props.l2_features),
-	PROP(RAW_SUSPEND_SIZE,            raw_props.suspend_size),
-	PROP(RAW_MEM_FEATURES,            raw_props.mem_features),
-	PROP(RAW_MMU_FEATURES,            raw_props.mmu_features),
-	PROP(RAW_AS_PRESENT,              raw_props.as_present),
-	PROP(RAW_JS_PRESENT,              raw_props.js_present),
-	PROP(RAW_JS_FEATURES_0,           raw_props.js_features[0]),
-	PROP(RAW_JS_FEATURES_1,           raw_props.js_features[1]),
-	PROP(RAW_JS_FEATURES_2,           raw_props.js_features[2]),
-	PROP(RAW_JS_FEATURES_3,           raw_props.js_features[3]),
-	PROP(RAW_JS_FEATURES_4,           raw_props.js_features[4]),
-	PROP(RAW_JS_FEATURES_5,           raw_props.js_features[5]),
-	PROP(RAW_JS_FEATURES_6,           raw_props.js_features[6]),
-	PROP(RAW_JS_FEATURES_7,           raw_props.js_features[7]),
-	PROP(RAW_JS_FEATURES_8,           raw_props.js_features[8]),
-	PROP(RAW_JS_FEATURES_9,           raw_props.js_features[9]),
-	PROP(RAW_JS_FEATURES_10,          raw_props.js_features[10]),
-	PROP(RAW_JS_FEATURES_11,          raw_props.js_features[11]),
-	PROP(RAW_JS_FEATURES_12,          raw_props.js_features[12]),
-	PROP(RAW_JS_FEATURES_13,          raw_props.js_features[13]),
-	PROP(RAW_JS_FEATURES_14,          raw_props.js_features[14]),
-	PROP(RAW_JS_FEATURES_15,          raw_props.js_features[15]),
-	PROP(RAW_TILER_FEATURES,          raw_props.tiler_features),
-	PROP(RAW_TEXTURE_FEATURES_0,      raw_props.texture_features[0]),
-	PROP(RAW_TEXTURE_FEATURES_1,      raw_props.texture_features[1]),
-	PROP(RAW_TEXTURE_FEATURES_2,      raw_props.texture_features[2]),
-	PROP(RAW_GPU_ID,                  raw_props.gpu_id),
-	PROP(RAW_THREAD_MAX_THREADS,      raw_props.thread_max_threads),
-	PROP(RAW_THREAD_MAX_WORKGROUP_SIZE,
-			raw_props.thread_max_workgroup_size),
-	PROP(RAW_THREAD_MAX_BARRIER_SIZE, raw_props.thread_max_barrier_size),
-	PROP(RAW_THREAD_FEATURES,         raw_props.thread_features),
-	PROP(RAW_COHERENCY_MODE,          raw_props.coherency_mode),
-
-	PROP(COHERENCY_NUM_GROUPS,        coherency_info.num_groups),
-	PROP(COHERENCY_NUM_CORE_GROUPS,   coherency_info.num_core_groups),
-	PROP(COHERENCY_COHERENCY,         coherency_info.coherency),
-	PROP(COHERENCY_GROUP_0,           coherency_info.group[0].core_mask),
-	PROP(COHERENCY_GROUP_1,           coherency_info.group[1].core_mask),
-	PROP(COHERENCY_GROUP_2,           coherency_info.group[2].core_mask),
-	PROP(COHERENCY_GROUP_3,           coherency_info.group[3].core_mask),
-	PROP(COHERENCY_GROUP_4,           coherency_info.group[4].core_mask),
-	PROP(COHERENCY_GROUP_5,           coherency_info.group[5].core_mask),
-	PROP(COHERENCY_GROUP_6,           coherency_info.group[6].core_mask),
-	PROP(COHERENCY_GROUP_7,           coherency_info.group[7].core_mask),
-	PROP(COHERENCY_GROUP_8,           coherency_info.group[8].core_mask),
-	PROP(COHERENCY_GROUP_9,           coherency_info.group[9].core_mask),
-	PROP(COHERENCY_GROUP_10,          coherency_info.group[10].core_mask),
-	PROP(COHERENCY_GROUP_11,          coherency_info.group[11].core_mask),
-	PROP(COHERENCY_GROUP_12,          coherency_info.group[12].core_mask),
-	PROP(COHERENCY_GROUP_13,          coherency_info.group[13].core_mask),
-	PROP(COHERENCY_GROUP_14,          coherency_info.group[14].core_mask),
-	PROP(COHERENCY_GROUP_15,          coherency_info.group[15].core_mask),
-
-#undef PROP
-};
-
-int kbase_gpuprops_populate_user_buffer(struct kbase_device *kbdev)
-{
-	struct kbase_gpu_props *kprops = &kbdev->gpu_props;
-	struct mali_base_gpu_props *props = &kprops->props;
-	u32 count = ARRAY_SIZE(gpu_property_mapping);
-	u32 i;
-	u32 size = 0;
-	u8 *p;
-
-	for (i = 0; i < count; i++) {
-		/* 4 bytes for the ID, and the size of the property */
-		size += 4 + gpu_property_mapping[i].size;
-	}
-
-	kprops->prop_buffer_size = size;
-	kprops->prop_buffer = kmalloc(size, GFP_KERNEL);
-
-	if (!kprops->prop_buffer) {
-		kprops->prop_buffer_size = 0;
-		return -ENOMEM;
-	}
-
-	p = kprops->prop_buffer;
-
-#define WRITE_U8(v) (*p++ = (v) & 0xFF)
-#define WRITE_U16(v) do { WRITE_U8(v); WRITE_U8((v) >> 8); } while (0)
-#define WRITE_U32(v) do { WRITE_U16(v); WRITE_U16((v) >> 16); } while (0)
-#define WRITE_U64(v) do { WRITE_U32(v); WRITE_U32((v) >> 32); } while (0)
-
-	for (i = 0; i < count; i++) {
-		u32 type = gpu_property_mapping[i].type;
-		u8 type_size;
-		void *field = ((u8 *)props) + gpu_property_mapping[i].offset;
-
-		switch (gpu_property_mapping[i].size) {
-		case 1:
-			type_size = KBASE_GPUPROP_VALUE_SIZE_U8;
-			break;
-		case 2:
-			type_size = KBASE_GPUPROP_VALUE_SIZE_U16;
-			break;
-		case 4:
-			type_size = KBASE_GPUPROP_VALUE_SIZE_U32;
-			break;
-		case 8:
-			type_size = KBASE_GPUPROP_VALUE_SIZE_U64;
-			break;
-		default:
-			dev_err(kbdev->dev,
-				"Invalid gpu_property_mapping type=%d size=%d",
-				type, gpu_property_mapping[i].size);
-			return -EINVAL;
-		}
-
-		WRITE_U32((type<<2) | type_size);
-
-		switch (type_size) {
-		case KBASE_GPUPROP_VALUE_SIZE_U8:
-			WRITE_U8(*((u8 *)field));
-			break;
-		case KBASE_GPUPROP_VALUE_SIZE_U16:
-			WRITE_U16(*((u16 *)field));
-			break;
-		case KBASE_GPUPROP_VALUE_SIZE_U32:
-			WRITE_U32(*((u32 *)field));
-			break;
-		case KBASE_GPUPROP_VALUE_SIZE_U64:
-			WRITE_U64(*((u64 *)field));
-			break;
-		default: /* Cannot be reached */
-			WARN_ON(1);
-			return -EINVAL;
-		}
-	}
-
-	return 0;
+	gpu_props->raw_props.coherency_mode = regdump.coherency_features;
 }
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gpuprops.h b/drivers/gpu/arm/midgard/mali_kbase_gpuprops.h
index 57b3eaf..f3c95cc 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_gpuprops.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_gpuprops.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2015,2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2015 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -61,24 +61,4 @@ void kbase_gpuprops_set_features(struct kbase_device *kbdev);
  */
 int kbase_gpuprops_uk_get_props(struct kbase_context *kctx, struct kbase_uk_gpuprops * const kbase_props);
 
-/**
- * kbase_gpuprops_populate_user_buffer - Populate the GPU properties buffer
- * @kbdev: The kbase device
- *
- * Fills kbdev->gpu_props->prop_buffer with the GPU properties for user
- * space to read.
- */
-int kbase_gpuprops_populate_user_buffer(struct kbase_device *kbdev);
-
-/**
- * kbase_gpuprops_update_core_props_gpu_id - break down gpu id value
- * @gpu_props: the &base_gpu_props structure
- *
- * Break down gpu_id value stored in base_gpu_props::raw_props.gpu_id into
- * separate fields (version_status, minor_revision, major_revision, product_id)
- * stored in base_gpu_props::core_props.
- */
-void kbase_gpuprops_update_core_props_gpu_id(base_gpu_props * const gpu_props);
-
-
 #endif				/* _KBASE_GPUPROPS_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_gpuprops_types.h b/drivers/gpu/arm/midgard/mali_kbase_gpuprops_types.h
index 10794fc..f42e91b 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_gpuprops_types.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_gpuprops_types.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -51,8 +51,6 @@ struct kbase_gpuprops_regdump {
 	u32 tiler_present_hi;
 	u32 l2_present_lo;
 	u32 l2_present_hi;
-	u32 stack_present_lo;
-	u32 stack_present_hi;
 	u32 coherency_features;
 };
 
@@ -82,11 +80,13 @@ struct kbase_gpu_props {
 	struct kbase_gpu_mem_props mem;
 	struct kbase_gpu_mmu_props mmu;
 
+	/**
+	 * Implementation specific irq throttle value (us), should be adjusted during integration.
+	 */
+	int irq_throttle_time_us;
+
 	/* Properties shared with userspace */
 	base_gpu_props props;
-
-	u32 prop_buffer_size;
-	void *prop_buffer;
 };
 
 #endif				/* _KBASE_GPUPROPS_TYPES_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_hw.c b/drivers/gpu/arm/midgard/mali_kbase_hw.c
index 9a390d2..1d7e5e9 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_hw.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_hw.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -45,19 +45,6 @@ void kbase_hw_set_features_mask(struct kbase_device *kbdev)
 		case GPU_ID2_PRODUCT_THEX:
 			features = base_hw_features_tHEx;
 			break;
-		case GPU_ID2_PRODUCT_TSIX:
-			features = base_hw_features_tSIx;
-			break;
-#ifdef MALI_INCLUDE_TKAX
-		case GPU_ID2_PRODUCT_TKAX:
-			features = base_hw_features_tKAx;
-			break;
-#endif /* MALI_INCLUDE_TKAX */
-#ifdef MALI_INCLUDE_TTRX
-		case GPU_ID2_PRODUCT_TTRX:
-			features = base_hw_features_tTRx;
-			break;
-#endif /* MALI_INCLUDE_TTRX */
 		default:
 			features = base_hw_features_generic;
 			break;
@@ -97,142 +84,6 @@ void kbase_hw_set_features_mask(struct kbase_device *kbdev)
 		set_bit(*features, &kbdev->hw_features_mask[0]);
 }
 
-/**
- * kbase_hw_get_issues_for_new_id - Get the hardware issues for a new GPU ID
- * @kbdev: Device pointer
- *
- * Return: pointer to an array of hardware issues, terminated by
- * BASE_HW_ISSUE_END.
- *
- * This function can only be used on new-format GPU IDs, i.e. those for which
- * GPU_ID_IS_NEW_FORMAT evaluates as true. The GPU ID is read from the @kbdev.
- *
- * In debugging versions of the driver, unknown versions of a known GPU will
- * be treated as the most recent known version not later than the actual
- * version. In such circumstances, the GPU ID in @kbdev will also be replaced
- * with the most recent known version.
- *
- * Note: The GPU configuration must have been read by kbase_gpuprops_get_props()
- * before calling this function.
- */
-static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(
-					struct kbase_device *kbdev)
-{
-	const enum base_hw_issue *issues = NULL;
-
-	struct base_hw_product {
-		u32 product_model;
-		struct {
-			u32 version;
-			const enum base_hw_issue *issues;
-		} map[7];
-	};
-
-	static const struct base_hw_product base_hw_products[] = {
-		{GPU_ID2_PRODUCT_TMIX,
-		 {{GPU_ID2_VERSION_MAKE(0, 0, 1),
-		   base_hw_issues_tMIx_r0p0_05dev0},
-		  {GPU_ID2_VERSION_MAKE(0, 0, 2), base_hw_issues_tMIx_r0p0},
-		  {U32_MAX /* sentinel value */, NULL} } },
-
-		{GPU_ID2_PRODUCT_THEX,
-		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tHEx_r0p0},
-		  {GPU_ID2_VERSION_MAKE(0, 0, 1), base_hw_issues_tHEx_r0p0},
-		  {GPU_ID2_VERSION_MAKE(0, 1, 0), base_hw_issues_tHEx_r0p1},
-		  {U32_MAX, NULL} } },
-
-		{GPU_ID2_PRODUCT_TSIX,
-		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tSIx_r0p0},
-		  {GPU_ID2_VERSION_MAKE(0, 0, 1), base_hw_issues_tSIx_r0p0},
-		  {GPU_ID2_VERSION_MAKE(0, 1, 0), base_hw_issues_tSIx_r0p1},
-		  {GPU_ID2_VERSION_MAKE(0, 1, 1), base_hw_issues_tSIx_r0p1},
-		  {GPU_ID2_VERSION_MAKE(1, 0, 0), base_hw_issues_tSIx_r1p0},
-		  {GPU_ID2_VERSION_MAKE(1, 0, 1), base_hw_issues_tSIx_r1p0},
-		  {U32_MAX, NULL} } },
-
-
-#ifdef MALI_INCLUDE_TKAX
-		{GPU_ID2_PRODUCT_TKAX,
-		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tKAx_r0p0},
-		  {U32_MAX, NULL} } },
-#endif /* MALI_INCLUDE_TKAX */
-
-#ifdef MALI_INCLUDE_TTRX
-		{GPU_ID2_PRODUCT_TTRX,
-		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tTRx_r0p0},
-		  {U32_MAX, NULL} } },
-#endif /* MALI_INCLUDE_TTRX */
-	};
-
-	u32 gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
-	const u32 product_model = gpu_id & GPU_ID2_PRODUCT_MODEL;
-	const struct base_hw_product *product = NULL;
-	size_t p;
-
-	/* Stop when we reach the end of the products array. */
-	for (p = 0; p < ARRAY_SIZE(base_hw_products); ++p) {
-		if (product_model == base_hw_products[p].product_model) {
-			product = &base_hw_products[p];
-			break;
-		}
-	}
-
-	if (product != NULL) {
-		/* Found a matching product. */
-		const u32 version = gpu_id & GPU_ID2_VERSION;
-		u32 fallback_version = 0;
-		const enum base_hw_issue *fallback_issues = NULL;
-		size_t v;
-
-		/* Stop when we reach the end of the map. */
-		for (v = 0; product->map[v].version != U32_MAX; ++v) {
-
-			if (version == product->map[v].version) {
-				/* Exact match so stop. */
-				issues = product->map[v].issues;
-				break;
-			}
-
-			/* Check whether this is a candidate for most recent
-				known version not later than the actual
-				version. */
-			if ((version > product->map[v].version) &&
-				(product->map[v].version >= fallback_version)) {
-				fallback_version = product->map[v].version;
-				fallback_issues = product->map[v].issues;
-			}
-		}
-
-		if ((issues == NULL) && (fallback_issues != NULL)) {
-			/* Fall back to the issue set of the most recent known
-				version not later than the actual version. */
-			issues = fallback_issues;
-
-			dev_info(kbdev->dev,
-				"r%dp%d status %d is unknown; treating as r%dp%d status %d",
-				(gpu_id & GPU_ID2_VERSION_MAJOR) >>
-					GPU_ID2_VERSION_MAJOR_SHIFT,
-				(gpu_id & GPU_ID2_VERSION_MINOR) >>
-					GPU_ID2_VERSION_MINOR_SHIFT,
-				(gpu_id & GPU_ID2_VERSION_STATUS) >>
-					GPU_ID2_VERSION_STATUS_SHIFT,
-				(fallback_version & GPU_ID2_VERSION_MAJOR) >>
-					GPU_ID2_VERSION_MAJOR_SHIFT,
-				(fallback_version & GPU_ID2_VERSION_MINOR) >>
-					GPU_ID2_VERSION_MINOR_SHIFT,
-				(fallback_version & GPU_ID2_VERSION_STATUS) >>
-					GPU_ID2_VERSION_STATUS_SHIFT);
-
-			gpu_id &= ~GPU_ID2_VERSION;
-			gpu_id |= fallback_version;
-			kbdev->gpu_props.props.raw_props.gpu_id = gpu_id;
-
-			kbase_gpuprops_update_core_props_gpu_id(&kbdev->gpu_props.props);
-		}
-	}
-	return issues;
-}
-
 int kbase_hw_set_issues_mask(struct kbase_device *kbdev)
 {
 	const enum base_hw_issue *issues;
@@ -247,17 +98,26 @@ int kbase_hw_set_issues_mask(struct kbase_device *kbdev)
 
 	if (impl_tech != IMPLEMENTATION_MODEL) {
 		if (GPU_ID_IS_NEW_FORMAT(product_id)) {
-			issues = kbase_hw_get_issues_for_new_id(kbdev);
-			if (issues == NULL) {
-				dev_err(kbdev->dev,
-					"Unknown GPU ID %x", gpu_id);
-				return -EINVAL;
+			switch (gpu_id) {
+			case GPU_ID2_MAKE(6, 0, 10, 0, 0, 0, 1):
+				issues = base_hw_issues_tMIx_r0p0_05dev0;
+				break;
+			case GPU_ID2_MAKE(6, 0, 10, 0, 0, 0, 2):
+				issues = base_hw_issues_tMIx_r0p0;
+				break;
+			default:
+				if ((gpu_id & GPU_ID2_PRODUCT_MODEL) ==
+							GPU_ID2_PRODUCT_TMIX) {
+					issues = base_hw_issues_tMIx_r0p0;
+				} else if ((gpu_id & GPU_ID2_PRODUCT_MODEL) ==
+							GPU_ID2_PRODUCT_THEX) {
+					issues = base_hw_issues_tHEx_r0p0;
+				} else {
+					dev_err(kbdev->dev,
+						"Unknown GPU ID %x", gpu_id);
+					return -EINVAL;
+				}
 			}
-
-			/* The GPU ID might have been replaced with the last
-			   known version of the same GPU. */
-			gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
-
 		} else {
 			switch (gpu_id) {
 			case GPU_ID_MAKE(GPU_ID_PI_T60X, 0, 0, GPU_ID_S_15DEV0):
@@ -364,19 +224,6 @@ int kbase_hw_set_issues_mask(struct kbase_device *kbdev)
 			case GPU_ID2_PRODUCT_THEX:
 				issues = base_hw_issues_model_tHEx;
 				break;
-			case GPU_ID2_PRODUCT_TSIX:
-				issues = base_hw_issues_model_tSIx;
-				break;
-#ifdef MALI_INCLUDE_TKAX
-			case GPU_ID2_PRODUCT_TKAX:
-				issues = base_hw_issues_model_tKAx;
-				break;
-#endif /* MALI_INCLUDE_TKAX */
-#ifdef MALI_INCLUDE_TTRX
-			case GPU_ID2_PRODUCT_TTRX:
-				issues = base_hw_issues_model_tTRx;
-				break;
-#endif /* MALI_INCLUDE_TTRX */
 			default:
 				dev_err(kbdev->dev,
 					"Unknown GPU ID %x", gpu_id);
@@ -416,35 +263,7 @@ int kbase_hw_set_issues_mask(struct kbase_device *kbdev)
 		}
 	}
 
-	if (GPU_ID_IS_NEW_FORMAT(product_id)) {
-		dev_info(kbdev->dev,
-			"GPU identified as 0x%x arch %d.%d.%d r%dp%d status %d",
-			(gpu_id & GPU_ID2_PRODUCT_MAJOR) >>
-				GPU_ID2_PRODUCT_MAJOR_SHIFT,
-			(gpu_id & GPU_ID2_ARCH_MAJOR) >>
-				GPU_ID2_ARCH_MAJOR_SHIFT,
-			(gpu_id & GPU_ID2_ARCH_MINOR) >>
-				GPU_ID2_ARCH_MINOR_SHIFT,
-			(gpu_id & GPU_ID2_ARCH_REV) >>
-				GPU_ID2_ARCH_REV_SHIFT,
-			(gpu_id & GPU_ID2_VERSION_MAJOR) >>
-				GPU_ID2_VERSION_MAJOR_SHIFT,
-			(gpu_id & GPU_ID2_VERSION_MINOR) >>
-				GPU_ID2_VERSION_MINOR_SHIFT,
-			(gpu_id & GPU_ID2_VERSION_STATUS) >>
-				GPU_ID2_VERSION_STATUS_SHIFT);
-	} else {
-		dev_info(kbdev->dev,
-			"GPU identified as 0x%04x r%dp%d status %d",
-			(gpu_id & GPU_ID_VERSION_PRODUCT_ID) >>
-				GPU_ID_VERSION_PRODUCT_ID_SHIFT,
-			(gpu_id & GPU_ID_VERSION_MAJOR) >>
-				GPU_ID_VERSION_MAJOR_SHIFT,
-			(gpu_id & GPU_ID_VERSION_MINOR) >>
-				GPU_ID_VERSION_MINOR_SHIFT,
-			(gpu_id & GPU_ID_VERSION_STATUS) >>
-				GPU_ID_VERSION_STATUS_SHIFT);
-	}
+	dev_info(kbdev->dev, "GPU identified as 0x%04x r%dp%d status %d", (gpu_id & GPU_ID_VERSION_PRODUCT_ID) >> GPU_ID_VERSION_PRODUCT_ID_SHIFT, (gpu_id & GPU_ID_VERSION_MAJOR) >> GPU_ID_VERSION_MAJOR_SHIFT, (gpu_id & GPU_ID_VERSION_MINOR) >> GPU_ID_VERSION_MINOR_SHIFT, (gpu_id & GPU_ID_VERSION_STATUS) >> GPU_ID_VERSION_STATUS_SHIFT);
 
 	for (; *issues != BASE_HW_ISSUE_END; issues++)
 		set_bit(*issues, &kbdev->hw_issues_mask[0]);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_hw.h b/drivers/gpu/arm/midgard/mali_kbase_hw.h
index 754250c..fce7d29 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_hw.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_hw.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2015 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -40,20 +40,7 @@
 	test_bit(feature, &(kbdev)->hw_features_mask[0])
 
 /**
- * kbase_hw_set_issues_mask - Set the hardware issues mask based on the GPU ID
- * @kbdev: Device pointer
- *
- * Return: 0 if the GPU ID was recognized, otherwise -EINVAL.
- *
- * The GPU ID is read from the @kbdev.
- *
- * In debugging versions of the driver, unknown versions of a known GPU with a
- * new-format ID will be treated as the most recent known version not later
- * than the actual version. In such circumstances, the GPU ID in @kbdev will
- * also be replaced with the most recent known version.
- *
- * Note: The GPU configuration must have been read by
- * kbase_gpuprops_get_props() before calling this function.
+ * @brief Set the HW issues mask depending on the GPU ID
  */
 int kbase_hw_set_issues_mask(struct kbase_device *kbdev);
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_hwaccess_jm.h b/drivers/gpu/arm/midgard/mali_kbase_hwaccess_jm.h
index 750fda2..c2c3909 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_hwaccess_jm.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_hwaccess_jm.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -45,22 +45,33 @@ void kbase_backend_run_atom(struct kbase_device *kbdev,
 void kbase_backend_slot_update(struct kbase_device *kbdev);
 
 /**
- * kbase_backend_find_and_release_free_address_space() - Release a free AS
+ * kbase_backend_find_free_address_space() - Find a free address space.
  * @kbdev:	Device pointer
  * @kctx:	Context pointer
  *
- * This function can evict an idle context from the runpool, freeing up the
- * address space it was using.
+ * If no address spaces are currently free, then this function can evict an
+ * idle context from the runpool, freeing up the address space it was using.
  *
  * The address space is marked as in use. The caller must either assign a
  * context using kbase_gpu_use_ctx(), or release it using
- * kbase_ctx_sched_release()
+ * kbase_gpu_release_free_address_space()
  *
  * Return: Number of free address space, or KBASEP_AS_NR_INVALID if none
  *	   available
  */
-int kbase_backend_find_and_release_free_address_space(
-		struct kbase_device *kbdev, struct kbase_context *kctx);
+int kbase_backend_find_free_address_space(struct kbase_device *kbdev,
+						struct kbase_context *kctx);
+
+/**
+ * kbase_backend_release_free_address_space() - Release an address space.
+ * @kbdev:	Device pointer
+ * @as_nr:	Address space to release
+ *
+ * The address space must have been returned by
+ * kbase_gpu_find_free_address_space().
+ */
+void kbase_backend_release_free_address_space(struct kbase_device *kbdev,
+						int as_nr);
 
 /**
  * kbase_backend_use_ctx() - Activate a currently unscheduled context, using the
@@ -122,19 +133,6 @@ void kbase_backend_release_ctx_noirq(struct kbase_device *kbdev,
 						struct kbase_context *kctx);
 
 /**
- * kbase_backend_cacheclean - Perform a cache clean if the given atom requires
- *                            one
- * @kbdev:	Device pointer
- * @katom:	Pointer to the failed atom
- *
- * On some GPUs, the GPU cache must be cleaned following a failed atom. This
- * function performs a clean if it is required by @katom.
- */
-void kbase_backend_cacheclean(struct kbase_device *kbdev,
-		struct kbase_jd_atom *katom);
-
-
-/**
  * kbase_backend_complete_wq() - Perform backend-specific actions required on
  *				 completing an atom.
  * @kbdev:	Device pointer
@@ -376,6 +374,4 @@ bool kbase_reset_gpu_active(struct kbase_device *kbdev);
 void kbase_job_slot_hardstop(struct kbase_context *kctx, int js,
 				struct kbase_jd_atom *target_katom);
 
-extern struct protected_mode_ops kbase_native_protected_ops;
-
 #endif /* _KBASE_HWACCESS_JM_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_ioctl.h b/drivers/gpu/arm/midgard/mali_kbase_ioctl.h
deleted file mode 100644
index dcbed9c..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_ioctl.h
+++ /dev/null
@@ -1,656 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KBASE_IOCTL_H_
-#define _KBASE_IOCTL_H_
-
-#ifdef __cpluscplus
-extern "C" {
-#endif
-
-#include <linux/types.h>
-
-#define KBASE_IOCTL_TYPE 0x80
-
-#ifdef ANDROID
-/* Android's definition of ioctl is incorrect, specifying the type argument as
- * 'int'. This creates a warning when using _IOWR (as the top bit is set). Work
- * round this by redefining _IOC to include a case to 'int'.
- */
-#undef _IOC
-#define _IOC(dir, type, nr, size) \
-	((int)(((dir) << _IOC_DIRSHIFT) | ((type) << _IOC_TYPESHIFT) | \
-	((nr) << _IOC_NRSHIFT) | ((size) << _IOC_SIZESHIFT)))
-#endif
-
-/**
- * struct kbase_ioctl_version_check - Check version compatibility with kernel
- *
- * @major: Major version number
- * @minor: Minor version number
- */
-struct kbase_ioctl_version_check {
-	__u16 major;
-	__u16 minor;
-};
-
-#define KBASE_IOCTL_VERSION_CHECK \
-	_IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
-
-/**
- * struct kbase_ioctl_set_flags - Set kernel context creation flags
- *
- * @create_flags: Flags - see base_context_create_flags
- */
-struct kbase_ioctl_set_flags {
-	__u32 create_flags;
-};
-
-#define KBASE_IOCTL_SET_FLAGS \
-	_IOW(KBASE_IOCTL_TYPE, 1, struct kbase_ioctl_set_flags)
-
-/**
- * struct kbase_ioctl_job_submit - Submit jobs/atoms to the kernel
- *
- * @addr: Memory address of an array of struct base_jd_atom_v2
- * @nr_atoms: Number of entries in the array
- * @stride: sizeof(struct base_jd_atom_v2)
- */
-struct kbase_ioctl_job_submit {
-	union kbase_pointer addr;
-	__u32 nr_atoms;
-	__u32 stride;
-};
-
-#define KBASE_IOCTL_JOB_SUBMIT \
-	_IOW(KBASE_IOCTL_TYPE, 2, struct kbase_ioctl_job_submit)
-
-/**
- * struct kbase_ioctl_get_gpuprops - Read GPU properties from the kernel
- *
- * @buffer: Pointer to the buffer to store properties into
- * @size: Size of the buffer
- * @flags: Flags - must be zero for now
- *
- * The ioctl will return the number of bytes stored into @buffer or an error
- * on failure (e.g. @size is too small). If @size is specified as 0 then no
- * data will be written but the return value will be the number of bytes needed
- * for all the properties.
- *
- * @flags may be used in the future to request a different format for the
- * buffer. With @flags == 0 the following format is used.
- *
- * The buffer will be filled with pairs of values, a u32 key identifying the
- * property followed by the value. The size of the value is identified using
- * the bottom bits of the key. The value then immediately followed the key and
- * is tightly packed (there is no padding). All keys and values are
- * little-endian.
- *
- * 00 = u8
- * 01 = u16
- * 10 = u32
- * 11 = u64
- */
-struct kbase_ioctl_get_gpuprops {
-	union kbase_pointer buffer;
-	__u32 size;
-	__u32 flags;
-};
-
-#define KBASE_IOCTL_GET_GPUPROPS \
-	_IOW(KBASE_IOCTL_TYPE, 3, struct kbase_ioctl_get_gpuprops)
-
-#define KBASE_IOCTL_POST_TERM \
-	_IO(KBASE_IOCTL_TYPE, 4)
-
-/**
- * union kbase_ioctl_mem_alloc - Allocate memory on the GPU
- *
- * @va_pages: The number of pages of virtual address space to reserve
- * @commit_pages: The number of physical pages to allocate
- * @extent: The number of extra pages to allocate on each GPU fault which grows
- *          the region
- * @flags: Flags
- * @gpu_va: The GPU virtual address which is allocated
- *
- * @in: Input parameters
- * @out: Output parameters
- */
-union kbase_ioctl_mem_alloc {
-	struct {
-		__u64 va_pages;
-		__u64 commit_pages;
-		__u64 extent;
-		__u64 flags;
-	} in;
-	struct {
-		__u64 flags;
-		__u64 gpu_va;
-	} out;
-};
-
-#define KBASE_IOCTL_MEM_ALLOC \
-	_IOWR(KBASE_IOCTL_TYPE, 5, union kbase_ioctl_mem_alloc)
-
-/**
- * struct kbase_ioctl_mem_query - Query properties of a GPU memory region
- * @gpu_addr: A GPU address contained within the region
- * @query: The type of query
- * @value: The result of the query
- *
- * Use a %KBASE_MEM_QUERY_xxx flag as input for @query.
- *
- * @in: Input parameters
- * @out: Output parameters
- */
-union kbase_ioctl_mem_query {
-	struct {
-		__u64 gpu_addr;
-		__u64 query;
-	} in;
-	struct {
-		__u64 value;
-	} out;
-};
-
-#define KBASE_IOCTL_MEM_QUERY \
-	_IOWR(KBASE_IOCTL_TYPE, 6, union kbase_ioctl_mem_query)
-
-#define KBASE_MEM_QUERY_COMMIT_SIZE	1
-#define KBASE_MEM_QUERY_VA_SIZE		2
-#define KBASE_MEM_QUERY_FLAGS		3
-
-/**
- * struct kbase_ioctl_mem_free - Free a memory region
- * @gpu_addr: Handle to the region to free
- */
-struct kbase_ioctl_mem_free {
-	__u64 gpu_addr;
-};
-
-#define KBASE_IOCTL_MEM_FREE \
-	_IOW(KBASE_IOCTL_TYPE, 7, struct kbase_ioctl_mem_free)
-
-/**
- * struct kbase_ioctl_hwcnt_reader_setup - Setup HWC dumper/reader
- * @buffer_count: requested number of dumping buffers
- * @jm_bm:        counters selection bitmask (JM)
- * @shader_bm:    counters selection bitmask (Shader)
- * @tiler_bm:     counters selection bitmask (Tiler)
- * @mmu_l2_bm:    counters selection bitmask (MMU_L2)
- *
- * A fd is returned from the ioctl if successful, or a negative value on error
- */
-struct kbase_ioctl_hwcnt_reader_setup {
-	__u32 buffer_count;
-	__u32 jm_bm;
-	__u32 shader_bm;
-	__u32 tiler_bm;
-	__u32 mmu_l2_bm;
-};
-
-#define KBASE_IOCTL_HWCNT_READER_SETUP \
-	_IOW(KBASE_IOCTL_TYPE, 8, struct kbase_ioctl_hwcnt_reader_setup)
-
-/**
- * struct kbase_ioctl_hwcnt_enable - Enable hardware counter collection
- * @dump_buffer:  GPU address to write counters to
- * @jm_bm:        counters selection bitmask (JM)
- * @shader_bm:    counters selection bitmask (Shader)
- * @tiler_bm:     counters selection bitmask (Tiler)
- * @mmu_l2_bm:    counters selection bitmask (MMU_L2)
- */
-struct kbase_ioctl_hwcnt_enable {
-	__u64 dump_buffer;
-	__u32 jm_bm;
-	__u32 shader_bm;
-	__u32 tiler_bm;
-	__u32 mmu_l2_bm;
-};
-
-#define KBASE_IOCTL_HWCNT_ENABLE \
-	_IOW(KBASE_IOCTL_TYPE, 9, struct kbase_ioctl_hwcnt_enable)
-
-#define KBASE_IOCTL_HWCNT_DUMP \
-	_IO(KBASE_IOCTL_TYPE, 10)
-
-#define KBASE_IOCTL_HWCNT_CLEAR \
-	_IO(KBASE_IOCTL_TYPE, 11)
-
-/**
- * struct kbase_ioctl_disjoint_query - Query the disjoint counter
- * @counter:   A counter of disjoint events in the kernel
- */
-struct kbase_ioctl_disjoint_query {
-	__u32 counter;
-};
-
-#define KBASE_IOCTL_DISJOINT_QUERY \
-	_IOR(KBASE_IOCTL_TYPE, 12, struct kbase_ioctl_disjoint_query)
-
-/**
- * struct kbase_ioctl_get_ddk_version - Query the kernel version
- * @version_buffer: Buffer to receive the kernel version string
- * @size: Size of the buffer
- *
- * The ioctl will return the number of bytes written into version_buffer
- * (which includes a NULL byte) or a negative error code
- */
-struct kbase_ioctl_get_ddk_version {
-	union kbase_pointer version_buffer;
-	__u32 size;
-};
-
-#define KBASE_IOCTL_GET_DDK_VERSION \
-	_IOW(KBASE_IOCTL_TYPE, 13, struct kbase_ioctl_get_ddk_version)
-
-/**
- * struct kbase_ioctl_mem_jit_init - Initialise the JIT memory allocator
- *
- * @va_pages: Number of VA pages to reserve for JIT
- *
- * Note that depending on the VA size of the application and GPU, the value
- * specified in @va_pages may be ignored.
- */
-struct kbase_ioctl_mem_jit_init {
-	__u64 va_pages;
-};
-
-#define KBASE_IOCTL_MEM_JIT_INIT \
-	_IOW(KBASE_IOCTL_TYPE, 14, struct kbase_ioctl_mem_jit_init)
-
-/**
- * struct kbase_ioctl_mem_sync - Perform cache maintenance on memory
- *
- * @handle: GPU memory handle (GPU VA)
- * @user_addr: The address where it is mapped in user space
- * @size: The number of bytes to synchronise
- * @type: The direction to synchronise: 0 is sync to memory (clean),
- * 1 is sync from memory (invalidate). Use the BASE_SYNCSET_OP_xxx constants.
- * @padding: Padding to round up to a multiple of 8 bytes, must be zero
- */
-struct kbase_ioctl_mem_sync {
-	__u64 handle;
-	__u64 user_addr;
-	__u64 size;
-	__u8 type;
-	__u8 padding[7];
-};
-
-#define KBASE_IOCTL_MEM_SYNC \
-	_IOW(KBASE_IOCTL_TYPE, 15, struct kbase_ioctl_mem_sync)
-
-/**
- * union kbase_ioctl_mem_find_cpu_offset - Find the offset of a CPU pointer
- *
- * @gpu_addr: The GPU address of the memory region
- * @cpu_addr: The CPU address to locate
- * @size: A size in bytes to validate is contained within the region
- * @offset: The offset from the start of the memory region to @cpu_addr
- *
- * @in: Input parameters
- * @out: Output parameters
- */
-union kbase_ioctl_mem_find_cpu_offset {
-	struct {
-		__u64 gpu_addr;
-		__u64 cpu_addr;
-		__u64 size;
-	} in;
-	struct {
-		__u64 offset;
-	} out;
-};
-
-#define KBASE_IOCTL_MEM_FIND_CPU_OFFSET \
-	_IOWR(KBASE_IOCTL_TYPE, 16, union kbase_ioctl_mem_find_cpu_offset)
-
-/**
- * struct kbase_ioctl_get_context_id - Get the kernel context ID
- *
- * @id: The kernel context ID
- */
-struct kbase_ioctl_get_context_id {
-	int id; /* This should really be __u32, but see GPUCORE-10048 */
-};
-
-#define KBASE_IOCTL_GET_CONTEXT_ID \
-	_IOR(KBASE_IOCTL_TYPE, 17, struct kbase_ioctl_get_context_id)
-
-/**
- * struct kbase_ioctl_tlstream_acquire - Acquire a tlstream fd
- *
- * @flags: Flags
- *
- * The ioctl returns a file descriptor when successful
- */
-struct kbase_ioctl_tlstream_acquire {
-	__u32 flags;
-};
-
-#define KBASE_IOCTL_TLSTREAM_ACQUIRE \
-	_IOW(KBASE_IOCTL_TYPE, 18, struct kbase_ioctl_tlstream_acquire)
-
-#define KBASE_IOCTL_TLSTREAM_FLUSH \
-	_IO(KBASE_IOCTL_TYPE, 19)
-
-/**
- * struct kbase_ioctl_mem_commit - Change the amount of memory backing a region
- *
- * @gpu_addr: The memory region to modify
- * @pages:    The number of physical pages that should be present
- *
- * The ioctl may return on the following error codes or 0 for success:
- *   -ENOMEM: Out of memory
- *   -EINVAL: Invalid arguments
- */
-struct kbase_ioctl_mem_commit {
-	__u64 gpu_addr;
-	__u64 pages;
-};
-
-#define KBASE_IOCTL_MEM_COMMIT \
-	_IOW(KBASE_IOCTL_TYPE, 20, struct kbase_ioctl_mem_commit)
-
-/**
- * union kbase_ioctl_mem_alias - Create an alias of memory regions
- * @flags: Flags, see BASE_MEM_xxx
- * @stride: Bytes between start of each memory region
- * @nents: The number of regions to pack together into the alias
- * @aliasing_info: Pointer to an array of struct base_mem_aliasing_info
- * @gpu_va: Address of the new alias
- * @va_pages: Size of the new alias
- *
- * @in: Input parameters
- * @out: Output parameters
- */
-union kbase_ioctl_mem_alias {
-	struct {
-		__u64 flags;
-		__u64 stride;
-		__u64 nents;
-		union kbase_pointer aliasing_info;
-	} in;
-	struct {
-		__u64 flags;
-		__u64 gpu_va;
-		__u64 va_pages;
-	} out;
-};
-
-#define KBASE_IOCTL_MEM_ALIAS \
-	_IOWR(KBASE_IOCTL_TYPE, 21, union kbase_ioctl_mem_alias)
-
-/**
- * union kbase_ioctl_mem_import - Import memory for use by the GPU
- * @flags: Flags, see BASE_MEM_xxx
- * @phandle: Handle to the external memory
- * @type: Type of external memory, see base_mem_import_type
- * @padding: Amount of extra VA pages to append to the imported buffer
- * @gpu_va: Address of the new alias
- * @va_pages: Size of the new alias
- *
- * @in: Input parameters
- * @out: Output parameters
- */
-union kbase_ioctl_mem_import {
-	struct {
-		__u64 flags;
-		union kbase_pointer phandle;
-		__u32 type;
-		__u32 padding;
-	} in;
-	struct {
-		__u64 flags;
-		__u64 gpu_va;
-		__u64 va_pages;
-	} out;
-};
-
-#define KBASE_IOCTL_MEM_IMPORT \
-	_IOWR(KBASE_IOCTL_TYPE, 22, union kbase_ioctl_mem_import)
-
-/**
- * struct kbase_ioctl_mem_flags_change - Change the flags for a memory region
- * @gpu_va: The GPU region to modify
- * @flags: The new flags to set
- * @mask: Mask of the flags to modify
- */
-struct kbase_ioctl_mem_flags_change {
-	__u64 gpu_va;
-	__u64 flags;
-	__u64 mask;
-};
-
-#define KBASE_IOCTL_MEM_FLAGS_CHANGE \
-	_IOW(KBASE_IOCTL_TYPE, 23, struct kbase_ioctl_mem_flags_change)
-
-/**
- * struct kbase_ioctl_stream_create - Create a synchronisation stream
- * @name: A name to identify this stream. Must be NULL-terminated.
- *
- * Note that this is also called a "timeline", but is named stream to avoid
- * confusion with other uses of the word.
- *
- * Unused bytes in @name (after the first NULL byte) must be also be NULL bytes.
- *
- * The ioctl returns a file descriptor.
- */
-struct kbase_ioctl_stream_create {
-	char name[32];
-};
-
-#define KBASE_IOCTL_STREAM_CREATE \
-	_IOW(KBASE_IOCTL_TYPE, 24, struct kbase_ioctl_stream_create)
-
-/**
- * struct kbase_ioctl_fence_validate - Validate a fd refers to a fence
- * @fd: The file descriptor to validate
- */
-struct kbase_ioctl_fence_validate {
-	int fd;
-};
-
-#define KBASE_IOCTL_FENCE_VALIDATE \
-	_IOW(KBASE_IOCTL_TYPE, 25, struct kbase_ioctl_fence_validate)
-
-/**
- * struct kbase_ioctl_get_profiling_controls - Get the profiling controls
- * @count: The size of @buffer in u32 words
- * @buffer: The buffer to receive the profiling controls
- */
-struct kbase_ioctl_get_profiling_controls {
-	union kbase_pointer buffer;
-	__u32 count;
-};
-
-#define KBASE_IOCTL_GET_PROFILING_CONTROLS \
-	_IOW(KBASE_IOCTL_TYPE, 26, struct kbase_ioctl_get_profiling_controls)
-
-/**
- * struct kbase_ioctl_mem_profile_add - Provide profiling information to kernel
- * @buffer: Pointer to the information
- * @len: Length
- * @padding: Padding
- *
- * The data provided is accessible through a debugfs file
- */
-struct kbase_ioctl_mem_profile_add {
-	union kbase_pointer buffer;
-	__u32 len;
-	__u32 padding;
-};
-
-#define KBASE_IOCTL_MEM_PROFILE_ADD \
-	_IOW(KBASE_IOCTL_TYPE, 27, struct kbase_ioctl_mem_profile_add)
-
-/**
- * struct kbase_ioctl_soft_event_update - Update the status of a soft-event
- * @event: GPU address of the event which has been updated
- * @new_status: The new status to set
- * @flags: Flags for future expansion
- */
-struct kbase_ioctl_soft_event_update {
-	__u64 event;
-	__u32 new_status;
-	__u32 flags;
-};
-
-#define KBASE_IOCTL_SOFT_EVENT_UPDATE \
-	_IOW(KBASE_IOCTL_TYPE, 28, struct kbase_ioctl_soft_event_update)
-
-/***************
- * test ioctls *
- ***************/
-#if MALI_UNIT_TEST
-/* These ioctls are purely for test purposes and are not used in the production
- * driver, they therefore may change without notice
- */
-
-#define KBASE_IOCTL_TEST_TYPE (KBASE_IOCTL_TYPE + 1)
-
-/**
- * struct kbase_ioctl_tlstream_test - Start a timeline stream test
- *
- * @tpw_count: number of trace point writers in each context
- * @msg_delay: time delay between tracepoints from one writer in milliseconds
- * @msg_count: number of trace points written by one writer
- * @aux_msg:   if non-zero aux messages will be included
- */
-struct kbase_ioctl_tlstream_test {
-	__u32 tpw_count;
-	__u32 msg_delay;
-	__u32 msg_count;
-	__u32 aux_msg;
-};
-
-#define KBASE_IOCTL_TLSTREAM_TEST \
-	_IOW(KBASE_IOCTL_TEST_TYPE, 1, struct kbase_ioctl_tlstream_test)
-
-/**
- * struct kbase_ioctl_tlstream_stats - Read tlstream stats for test purposes
- * @bytes_collected: number of bytes read by user
- * @bytes_generated: number of bytes generated by tracepoints
- */
-struct kbase_ioctl_tlstream_stats {
-	__u32 bytes_collected;
-	__u32 bytes_generated;
-};
-
-#define KBASE_IOCTL_TLSTREAM_STATS \
-	_IOR(KBASE_IOCTL_TEST_TYPE, 2, struct kbase_ioctl_tlstream_stats)
-
-#endif
-
-/**********************************
- * Definitions for GPU properties *
- **********************************/
-#define KBASE_GPUPROP_VALUE_SIZE_U8	(0x0)
-#define KBASE_GPUPROP_VALUE_SIZE_U16	(0x1)
-#define KBASE_GPUPROP_VALUE_SIZE_U32	(0x2)
-#define KBASE_GPUPROP_VALUE_SIZE_U64	(0x3)
-
-#define KBASE_GPUPROP_PRODUCT_ID			1
-#define KBASE_GPUPROP_VERSION_STATUS			2
-#define KBASE_GPUPROP_MINOR_REVISION			3
-#define KBASE_GPUPROP_MAJOR_REVISION			4
-#define KBASE_GPUPROP_GPU_SPEED_MHZ			5
-#define KBASE_GPUPROP_GPU_FREQ_KHZ_MAX			6
-#define KBASE_GPUPROP_GPU_FREQ_KHZ_MIN			7
-#define KBASE_GPUPROP_LOG2_PROGRAM_COUNTER_SIZE		8
-#define KBASE_GPUPROP_TEXTURE_FEATURES_0		9
-#define KBASE_GPUPROP_TEXTURE_FEATURES_1		10
-#define KBASE_GPUPROP_TEXTURE_FEATURES_2		11
-#define KBASE_GPUPROP_GPU_AVAILABLE_MEMORY_SIZE		12
-
-#define KBASE_GPUPROP_L2_LOG2_LINE_SIZE			13
-#define KBASE_GPUPROP_L2_LOG2_CACHE_SIZE		14
-#define KBASE_GPUPROP_L2_NUM_L2_SLICES			15
-
-#define KBASE_GPUPROP_TILER_BIN_SIZE_BYTES		16
-#define KBASE_GPUPROP_TILER_MAX_ACTIVE_LEVELS		17
-
-#define KBASE_GPUPROP_MAX_THREADS			18
-#define KBASE_GPUPROP_MAX_WORKGROUP_SIZE		19
-#define KBASE_GPUPROP_MAX_BARRIER_SIZE			20
-#define KBASE_GPUPROP_MAX_REGISTERS			21
-#define KBASE_GPUPROP_MAX_TASK_QUEUE			22
-#define KBASE_GPUPROP_MAX_THREAD_GROUP_SPLIT		23
-#define KBASE_GPUPROP_IMPL_TECH				24
-
-#define KBASE_GPUPROP_RAW_SHADER_PRESENT		25
-#define KBASE_GPUPROP_RAW_TILER_PRESENT			26
-#define KBASE_GPUPROP_RAW_L2_PRESENT			27
-#define KBASE_GPUPROP_RAW_STACK_PRESENT			28
-#define KBASE_GPUPROP_RAW_L2_FEATURES			29
-#define KBASE_GPUPROP_RAW_SUSPEND_SIZE			30
-#define KBASE_GPUPROP_RAW_MEM_FEATURES			31
-#define KBASE_GPUPROP_RAW_MMU_FEATURES			32
-#define KBASE_GPUPROP_RAW_AS_PRESENT			33
-#define KBASE_GPUPROP_RAW_JS_PRESENT			34
-#define KBASE_GPUPROP_RAW_JS_FEATURES_0			35
-#define KBASE_GPUPROP_RAW_JS_FEATURES_1			36
-#define KBASE_GPUPROP_RAW_JS_FEATURES_2			37
-#define KBASE_GPUPROP_RAW_JS_FEATURES_3			38
-#define KBASE_GPUPROP_RAW_JS_FEATURES_4			39
-#define KBASE_GPUPROP_RAW_JS_FEATURES_5			40
-#define KBASE_GPUPROP_RAW_JS_FEATURES_6			41
-#define KBASE_GPUPROP_RAW_JS_FEATURES_7			42
-#define KBASE_GPUPROP_RAW_JS_FEATURES_8			43
-#define KBASE_GPUPROP_RAW_JS_FEATURES_9			44
-#define KBASE_GPUPROP_RAW_JS_FEATURES_10		45
-#define KBASE_GPUPROP_RAW_JS_FEATURES_11		46
-#define KBASE_GPUPROP_RAW_JS_FEATURES_12		47
-#define KBASE_GPUPROP_RAW_JS_FEATURES_13		48
-#define KBASE_GPUPROP_RAW_JS_FEATURES_14		49
-#define KBASE_GPUPROP_RAW_JS_FEATURES_15		50
-#define KBASE_GPUPROP_RAW_TILER_FEATURES		51
-#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_0		52
-#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_1		53
-#define KBASE_GPUPROP_RAW_TEXTURE_FEATURES_2		54
-#define KBASE_GPUPROP_RAW_GPU_ID			55
-#define KBASE_GPUPROP_RAW_THREAD_MAX_THREADS		56
-#define KBASE_GPUPROP_RAW_THREAD_MAX_WORKGROUP_SIZE	57
-#define KBASE_GPUPROP_RAW_THREAD_MAX_BARRIER_SIZE	58
-#define KBASE_GPUPROP_RAW_THREAD_FEATURES		59
-#define KBASE_GPUPROP_RAW_COHERENCY_MODE		60
-
-#define KBASE_GPUPROP_COHERENCY_NUM_GROUPS		61
-#define KBASE_GPUPROP_COHERENCY_NUM_CORE_GROUPS		62
-#define KBASE_GPUPROP_COHERENCY_COHERENCY		63
-#define KBASE_GPUPROP_COHERENCY_GROUP_0			64
-#define KBASE_GPUPROP_COHERENCY_GROUP_1			65
-#define KBASE_GPUPROP_COHERENCY_GROUP_2			66
-#define KBASE_GPUPROP_COHERENCY_GROUP_3			67
-#define KBASE_GPUPROP_COHERENCY_GROUP_4			68
-#define KBASE_GPUPROP_COHERENCY_GROUP_5			69
-#define KBASE_GPUPROP_COHERENCY_GROUP_6			70
-#define KBASE_GPUPROP_COHERENCY_GROUP_7			71
-#define KBASE_GPUPROP_COHERENCY_GROUP_8			72
-#define KBASE_GPUPROP_COHERENCY_GROUP_9			73
-#define KBASE_GPUPROP_COHERENCY_GROUP_10		74
-#define KBASE_GPUPROP_COHERENCY_GROUP_11		75
-#define KBASE_GPUPROP_COHERENCY_GROUP_12		76
-#define KBASE_GPUPROP_COHERENCY_GROUP_13		77
-#define KBASE_GPUPROP_COHERENCY_GROUP_14		78
-#define KBASE_GPUPROP_COHERENCY_GROUP_15		79
-
-#ifdef __cpluscplus
-}
-#endif
-
-#endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_ipa.c b/drivers/gpu/arm/midgard/mali_kbase_ipa.c
new file mode 100644
index 0000000..c579d0a
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_ipa.c
@@ -0,0 +1,431 @@
+/*
+ *
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+#include <linux/of.h>
+#include <linux/sysfs.h>
+
+#include <mali_kbase.h>
+
+#define NR_IPA_GROUPS 8
+
+struct kbase_ipa_context;
+
+/**
+ * struct ipa_group - represents a single IPA group
+ * @name:               name of the IPA group
+ * @capacitance:        capacitance constant for IPA group
+ * @calc_power:         function to calculate power for IPA group
+ */
+struct ipa_group {
+	const char *name;
+	u32 capacitance;
+	u32 (*calc_power)(struct kbase_ipa_context *,
+			struct ipa_group *);
+};
+
+#include <mali_kbase_ipa_tables.h>
+
+/**
+ * struct kbase_ipa_context - IPA context per device
+ * @kbdev:              pointer to kbase device
+ * @groups:             array of IPA groups for this context
+ * @vinstr_cli:         vinstr client handle
+ * @vinstr_buffer:      buffer to dump hardware counters onto
+ * @ipa_lock:           protects the entire IPA context
+ */
+struct kbase_ipa_context {
+	struct kbase_device *kbdev;
+	struct ipa_group groups[NR_IPA_GROUPS];
+	struct kbase_vinstr_client *vinstr_cli;
+	void *vinstr_buffer;
+	struct mutex ipa_lock;
+};
+
+static ssize_t show_ipa_group(struct device *dev,
+		struct device_attribute *attr,
+		char *buf)
+{
+	struct kbase_device *kbdev = dev_get_drvdata(dev);
+	struct kbase_ipa_context *ctx = kbdev->ipa_ctx;
+	ssize_t count = -EINVAL;
+	size_t i;
+
+	mutex_lock(&ctx->ipa_lock);
+	for (i = 0; i < ARRAY_SIZE(ctx->groups); i++) {
+		if (!strcmp(ctx->groups[i].name, attr->attr.name)) {
+			count = snprintf(buf, PAGE_SIZE, "%lu\n",
+				(unsigned long)ctx->groups[i].capacitance);
+			break;
+		}
+	}
+	mutex_unlock(&ctx->ipa_lock);
+	return count;
+}
+
+static ssize_t set_ipa_group(struct device *dev,
+		struct device_attribute *attr,
+		const char *buf,
+		size_t count)
+{
+	struct kbase_device *kbdev = dev_get_drvdata(dev);
+	struct kbase_ipa_context *ctx = kbdev->ipa_ctx;
+	unsigned long capacitance;
+	size_t i;
+	int err;
+
+	err = kstrtoul(buf, 0, &capacitance);
+	if (err < 0)
+		return err;
+	if (capacitance > U32_MAX)
+		return -ERANGE;
+
+	mutex_lock(&ctx->ipa_lock);
+	for (i = 0; i < ARRAY_SIZE(ctx->groups); i++) {
+		if (!strcmp(ctx->groups[i].name, attr->attr.name)) {
+			ctx->groups[i].capacitance = capacitance;
+			mutex_unlock(&ctx->ipa_lock);
+			return count;
+		}
+	}
+	mutex_unlock(&ctx->ipa_lock);
+	return -EINVAL;
+}
+
+static DEVICE_ATTR(group0, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group1, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group2, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group3, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group4, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group5, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group6, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+static DEVICE_ATTR(group7, S_IRUGO | S_IWUSR, show_ipa_group, set_ipa_group);
+
+static struct attribute *kbase_ipa_attrs[] = {
+	&dev_attr_group0.attr,
+	&dev_attr_group1.attr,
+	&dev_attr_group2.attr,
+	&dev_attr_group3.attr,
+	&dev_attr_group4.attr,
+	&dev_attr_group5.attr,
+	&dev_attr_group6.attr,
+	&dev_attr_group7.attr,
+	NULL,
+};
+
+static struct attribute_group kbase_ipa_attr_group = {
+	.name = "ipa",
+	.attrs = kbase_ipa_attrs,
+};
+
+static void init_ipa_groups(struct kbase_ipa_context *ctx)
+{
+	memcpy(ctx->groups, ipa_groups_def, sizeof(ctx->groups));
+}
+
+#if defined(CONFIG_OF) && (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0))
+static int update_ipa_groups_from_dt(struct kbase_ipa_context *ctx)
+{
+	struct kbase_device *kbdev = ctx->kbdev;
+	struct device_node *np, *child;
+	struct ipa_group *group;
+	size_t nr_groups;
+	size_t i;
+	int err;
+
+	np = of_get_child_by_name(kbdev->dev->of_node, "ipa-groups");
+	if (!np)
+		return 0;
+
+	nr_groups = 0;
+	for_each_available_child_of_node(np, child)
+		nr_groups++;
+	if (!nr_groups || nr_groups > ARRAY_SIZE(ctx->groups)) {
+		dev_err(kbdev->dev, "invalid number of IPA groups: %zu", nr_groups);
+		err = -EINVAL;
+		goto err0;
+	}
+
+	for_each_available_child_of_node(np, child) {
+		const char *name;
+		u32 capacitance;
+
+		name = of_get_property(child, "label", NULL);
+		if (!name) {
+			dev_err(kbdev->dev, "label missing for IPA group");
+			err = -EINVAL;
+			goto err0;
+		}
+		err = of_property_read_u32(child, "capacitance",
+				&capacitance);
+		if (err < 0) {
+			dev_err(kbdev->dev, "capacitance missing for IPA group");
+			goto err0;
+		}
+
+		for (i = 0; i < ARRAY_SIZE(ctx->groups); i++) {
+			group = &ctx->groups[i];
+			if (!strcmp(group->name, name)) {
+				group->capacitance = capacitance;
+				break;
+			}
+		}
+	}
+
+	of_node_put(np);
+	return 0;
+err0:
+	of_node_put(np);
+	return err;
+}
+#else
+static int update_ipa_groups_from_dt(struct kbase_ipa_context *ctx)
+{
+	return 0;
+}
+#endif
+
+static int reset_ipa_groups(struct kbase_ipa_context *ctx)
+{
+	init_ipa_groups(ctx);
+	return update_ipa_groups_from_dt(ctx);
+}
+
+static inline u32 read_hwcnt(struct kbase_ipa_context *ctx,
+	u32 offset)
+{
+	u8 *p = ctx->vinstr_buffer;
+
+	return *(u32 *)&p[offset];
+}
+
+static inline u32 add_saturate(u32 a, u32 b)
+{
+	if (U32_MAX - a < b)
+		return U32_MAX;
+	return a + b;
+}
+
+/*
+ * Calculate power estimation based on hardware counter `c'
+ * across all shader cores.
+ */
+static u32 calc_power_sc_single(struct kbase_ipa_context *ctx,
+	struct ipa_group *group, u32 c)
+{
+	struct kbase_device *kbdev = ctx->kbdev;
+	u64 core_mask;
+	u32 base = 0, r = 0;
+
+	core_mask = kbdev->gpu_props.props.coherency_info.group[0].core_mask;
+	while (core_mask != 0ull) {
+		if ((core_mask & 1ull) != 0ull) {
+			u64 n = read_hwcnt(ctx, base + c);
+			u32 d = read_hwcnt(ctx, GPU_ACTIVE);
+			u32 s = group->capacitance;
+
+			r = add_saturate(r, div_u64(n * s, d));
+		}
+		base += NR_CNT_PER_BLOCK * NR_BYTES_PER_CNT;
+		core_mask >>= 1;
+	}
+	return r;
+}
+
+/*
+ * Calculate power estimation based on hardware counter `c1'
+ * and `c2' across all shader cores.
+ */
+static u32 calc_power_sc_double(struct kbase_ipa_context *ctx,
+	struct ipa_group *group, u32 c1, u32 c2)
+{
+	struct kbase_device *kbdev = ctx->kbdev;
+	u64 core_mask;
+	u32 base = 0, r = 0;
+
+	core_mask = kbdev->gpu_props.props.coherency_info.group[0].core_mask;
+	while (core_mask != 0ull) {
+		if ((core_mask & 1ull) != 0ull) {
+			u64 n = read_hwcnt(ctx, base + c1);
+			u32 d = read_hwcnt(ctx, GPU_ACTIVE);
+			u32 s = group->capacitance;
+
+			r = add_saturate(r, div_u64(n * s, d));
+			n = read_hwcnt(ctx, base + c2);
+			r = add_saturate(r, div_u64(n * s, d));
+		}
+		base += NR_CNT_PER_BLOCK * NR_BYTES_PER_CNT;
+		core_mask >>= 1;
+	}
+	return r;
+}
+
+static u32 calc_power_single(struct kbase_ipa_context *ctx,
+	struct ipa_group *group, u32 c)
+{
+	u64 n = read_hwcnt(ctx, c);
+	u32 d = read_hwcnt(ctx, GPU_ACTIVE);
+	u32 s = group->capacitance;
+
+	return div_u64(n * s, d);
+}
+
+static u32 calc_power_group0(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_single(ctx, group, L2_ANY_LOOKUP);
+}
+
+static u32 calc_power_group1(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_single(ctx, group, TILER_ACTIVE);
+}
+
+static u32 calc_power_group2(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_sc_single(ctx, group, FRAG_ACTIVE);
+}
+
+static u32 calc_power_group3(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_sc_double(ctx, group, VARY_SLOT_32,
+			VARY_SLOT_16);
+}
+
+static u32 calc_power_group4(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_sc_single(ctx, group, TEX_COORD_ISSUE);
+}
+
+static u32 calc_power_group5(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_sc_single(ctx, group, EXEC_INSTR_COUNT);
+}
+
+static u32 calc_power_group6(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_sc_double(ctx, group, BEATS_RD_LSC,
+			BEATS_WR_LSC);
+}
+
+static u32 calc_power_group7(struct kbase_ipa_context *ctx,
+		struct ipa_group *group)
+{
+	return calc_power_sc_single(ctx, group, EXEC_CORE_ACTIVE);
+}
+
+static int attach_vinstr(struct kbase_ipa_context *ctx)
+{
+	struct kbase_device *kbdev = ctx->kbdev;
+	struct kbase_uk_hwcnt_reader_setup setup;
+	size_t dump_size;
+
+	dump_size = kbase_vinstr_dump_size(kbdev);
+	ctx->vinstr_buffer = kzalloc(dump_size, GFP_KERNEL);
+	if (!ctx->vinstr_buffer) {
+		dev_err(kbdev->dev, "Failed to allocate IPA dump buffer");
+		return -1;
+	}
+
+	setup.jm_bm = ~0u;
+	setup.shader_bm = ~0u;
+	setup.tiler_bm = ~0u;
+	setup.mmu_l2_bm = ~0u;
+	ctx->vinstr_cli = kbase_vinstr_hwcnt_kernel_setup(kbdev->vinstr_ctx,
+			&setup, ctx->vinstr_buffer);
+	if (!ctx->vinstr_cli) {
+		dev_err(kbdev->dev, "Failed to register IPA with vinstr core");
+		kfree(ctx->vinstr_buffer);
+		ctx->vinstr_buffer = NULL;
+		return -1;
+	}
+	return 0;
+}
+
+static void detach_vinstr(struct kbase_ipa_context *ctx)
+{
+	if (ctx->vinstr_cli)
+		kbase_vinstr_detach_client(ctx->vinstr_cli);
+	ctx->vinstr_cli = NULL;
+	kfree(ctx->vinstr_buffer);
+	ctx->vinstr_buffer = NULL;
+}
+
+struct kbase_ipa_context *kbase_ipa_init(struct kbase_device *kbdev)
+{
+	struct kbase_ipa_context *ctx;
+	int err;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return NULL;
+
+	mutex_init(&ctx->ipa_lock);
+	ctx->kbdev = kbdev;
+
+	err = reset_ipa_groups(ctx);
+	if (err < 0)
+		goto err0;
+
+	err = sysfs_create_group(&kbdev->dev->kobj, &kbase_ipa_attr_group);
+	if (err < 0)
+		goto err0;
+
+	return ctx;
+err0:
+	kfree(ctx);
+	return NULL;
+}
+
+void kbase_ipa_term(struct kbase_ipa_context *ctx)
+{
+	struct kbase_device *kbdev = ctx->kbdev;
+
+	detach_vinstr(ctx);
+	sysfs_remove_group(&kbdev->dev->kobj, &kbase_ipa_attr_group);
+	kfree(ctx);
+}
+
+u32 kbase_ipa_dynamic_power(struct kbase_ipa_context *ctx, int *err)
+{
+	struct ipa_group *group;
+	u32 power = 0;
+	size_t i;
+
+	mutex_lock(&ctx->ipa_lock);
+	if (!ctx->vinstr_cli) {
+		*err = attach_vinstr(ctx);
+		if (*err < 0)
+			goto err0;
+	}
+	*err = kbase_vinstr_hwc_dump(ctx->vinstr_cli,
+			BASE_HWCNT_READER_EVENT_MANUAL);
+	if (*err)
+		goto err0;
+	for (i = 0; i < ARRAY_SIZE(ctx->groups); i++) {
+		group = &ctx->groups[i];
+		power = add_saturate(power, group->calc_power(ctx, group));
+	}
+err0:
+	mutex_unlock(&ctx->ipa_lock);
+	return power;
+}
+KBASE_EXPORT_TEST_API(kbase_ipa_dynamic_power);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_ipa.h b/drivers/gpu/arm/midgard/mali_kbase_ipa.h
new file mode 100644
index 0000000..e2234d1
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_ipa.h
@@ -0,0 +1,41 @@
+/*
+ *
+ * (C) COPYRIGHT 2015 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+struct kbase_ipa_context;
+
+/**
+ * kbase_ipa_init - initialize the kbase ipa core
+ * @kbdev:      kbase device
+ *
+ * Return:      pointer to the IPA context or NULL on failure
+ */
+struct kbase_ipa_context *kbase_ipa_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_ipa_term - terminate the kbase ipa core
+ * @ctx:        pointer to the IPA context
+ */
+void kbase_ipa_term(struct kbase_ipa_context *ctx);
+
+/**
+ * kbase_ipa_dynamic_power - calculate power
+ * @ctx:        pointer to the IPA context
+ * @err:        0 on success, negative on failure
+ *
+ * Return:      returns power consumption as mw @ 1GHz @ 1V
+ */
+u32 kbase_ipa_dynamic_power(struct kbase_ipa_context *ctx, int *err);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_ipa_tables.h b/drivers/gpu/arm/midgard/mali_kbase_ipa_tables.h
new file mode 100644
index 0000000..101abfe
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_ipa_tables.h
@@ -0,0 +1,104 @@
+/*
+ *
+ * (C) COPYRIGHT 2015 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#define NR_BYTES_PER_CNT  4
+#define NR_CNT_PER_BLOCK 64
+
+#define JM_BASE    (0 * NR_CNT_PER_BLOCK * NR_BYTES_PER_CNT)
+#define TILER_BASE (1 * NR_CNT_PER_BLOCK * NR_BYTES_PER_CNT)
+#define MMU_BASE   (2 * NR_CNT_PER_BLOCK * NR_BYTES_PER_CNT)
+#define SC0_BASE   (3 * NR_CNT_PER_BLOCK * NR_BYTES_PER_CNT)
+
+#define GPU_ACTIVE       (JM_BASE    + NR_BYTES_PER_CNT *  6)
+#define TILER_ACTIVE     (TILER_BASE + NR_BYTES_PER_CNT * 45)
+#define L2_ANY_LOOKUP    (MMU_BASE   + NR_BYTES_PER_CNT * 25)
+#define FRAG_ACTIVE      (SC0_BASE   + NR_BYTES_PER_CNT *  4)
+#define EXEC_CORE_ACTIVE (SC0_BASE   + NR_BYTES_PER_CNT * 26)
+#define EXEC_INSTR_COUNT (SC0_BASE   + NR_BYTES_PER_CNT * 28)
+#define TEX_COORD_ISSUE  (SC0_BASE   + NR_BYTES_PER_CNT * 40)
+#define VARY_SLOT_32     (SC0_BASE   + NR_BYTES_PER_CNT * 50)
+#define VARY_SLOT_16     (SC0_BASE   + NR_BYTES_PER_CNT * 51)
+#define BEATS_RD_LSC     (SC0_BASE   + NR_BYTES_PER_CNT * 56)
+#define BEATS_WR_LSC     (SC0_BASE   + NR_BYTES_PER_CNT * 61)
+
+static u32 calc_power_group0(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group1(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group2(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group3(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group4(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group5(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group6(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+static u32 calc_power_group7(struct kbase_ipa_context *ctx,
+		struct ipa_group *group);
+
+static struct ipa_group ipa_groups_def[] = {
+	/* L2 */
+	{
+		.name = "group0",
+		.capacitance = 687,
+		.calc_power = calc_power_group0,
+	},
+	/* TILER */
+	{
+		.name = "group1",
+		.capacitance = 0,
+		.calc_power = calc_power_group1,
+	},
+	/* FRAG */
+	{
+		.name = "group2",
+		.capacitance = 23,
+		.calc_power = calc_power_group2,
+	},
+	/* VARY */
+	{
+		.name = "group3",
+		.capacitance = 108,
+		.calc_power = calc_power_group3,
+	},
+	/* TEX */
+	{
+		.name = "group4",
+		.capacitance = 128,
+		.calc_power = calc_power_group4,
+	},
+	/* EXEC INSTR */
+	{
+		.name = "group5",
+		.capacitance = 249,
+		.calc_power = calc_power_group5,
+	},
+	/* LSC */
+	{
+		.name = "group6",
+		.capacitance = 0,
+		.calc_power = calc_power_group6,
+	},
+	/* EXEC OVERHEAD */
+	{
+		.name = "group7",
+		.capacitance = 29,
+		.calc_power = calc_power_group7,
+	},
+};
diff --git a/drivers/gpu/arm/midgard/mali_kbase_jd.c b/drivers/gpu/arm/midgard/mali_kbase_jd.c
index f39f1b0..81952e2 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_jd.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_jd.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -290,18 +290,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 	struct kbase_dma_fence_resv_info info = {
 		.dma_fence_resv_count = 0,
 	};
-#ifdef CONFIG_SYNC
-	/*
-	 * When both dma-buf fence and Android native sync is enabled, we
-	 * disable dma-buf fence for contexts that are using Android native
-	 * fences.
-	 */
-	const bool implicit_sync = !kbase_ctx_flag(katom->kctx,
-						   KCTX_NO_IMPLICIT_SYNC);
-#else /* CONFIG_SYNC */
-	const bool implicit_sync = true;
-#endif /* CONFIG_SYNC */
-#endif /* CONFIG_MALI_DMA_FENCE */
+#endif
 	struct base_external_resource *input_extres;
 
 	KBASE_DEBUG_ASSERT(katom);
@@ -353,22 +342,20 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 #endif				/* CONFIG_KDS */
 
 #ifdef CONFIG_MALI_DMA_FENCE
-	if (implicit_sync) {
-		info.resv_objs = kmalloc_array(katom->nr_extres,
-					sizeof(struct reservation_object *),
-					GFP_KERNEL);
-		if (!info.resv_objs) {
-			err_ret_val = -ENOMEM;
-			goto early_err_out;
-		}
+	info.resv_objs = kmalloc_array(katom->nr_extres,
+				       sizeof(struct reservation_object *),
+				       GFP_KERNEL);
+	if (!info.resv_objs) {
+		err_ret_val = -ENOMEM;
+		goto early_err_out;
+	}
 
-		info.dma_fence_excl_bitmap =
-				kcalloc(BITS_TO_LONGS(katom->nr_extres),
-					sizeof(unsigned long), GFP_KERNEL);
-		if (!info.dma_fence_excl_bitmap) {
-			err_ret_val = -ENOMEM;
-			goto early_err_out;
-		}
+	info.dma_fence_excl_bitmap = kcalloc(BITS_TO_LONGS(katom->nr_extres),
+					     sizeof(unsigned long),
+					     GFP_KERNEL);
+	if (!info.dma_fence_excl_bitmap) {
+		err_ret_val = -ENOMEM;
+		goto early_err_out;
 	}
 #endif /* CONFIG_MALI_DMA_FENCE */
 
@@ -413,8 +400,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 		}
 
 #ifdef CONFIG_MALI_DMA_FENCE
-		if (implicit_sync &&
-		    reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
+		if (reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
 			struct reservation_object *resv;
 
 			resv = reg->gpu_alloc->imported.umm.dma_buf->resv;
@@ -465,18 +451,16 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 #endif				/* CONFIG_KDS */
 
 #ifdef CONFIG_MALI_DMA_FENCE
-	if (implicit_sync) {
-		if (info.dma_fence_resv_count) {
-			int ret;
-
-			ret = kbase_dma_fence_wait(katom, &info);
-			if (ret < 0)
-				goto failed_dma_fence_setup;
-		}
+	if (info.dma_fence_resv_count) {
+		int ret;
 
-		kfree(info.resv_objs);
-		kfree(info.dma_fence_excl_bitmap);
+		ret = kbase_dma_fence_wait(katom, &info);
+		if (ret < 0)
+			goto failed_dma_fence_setup;
 	}
+
+	kfree(info.resv_objs);
+	kfree(info.dma_fence_excl_bitmap);
 #endif /* CONFIG_MALI_DMA_FENCE */
 
 	/* all done OK */
@@ -531,10 +515,8 @@ failed_kds_setup:
 	kfree(kds_access_bitmap);
 #endif				/* CONFIG_KDS */
 #ifdef CONFIG_MALI_DMA_FENCE
-	if (implicit_sync) {
-		kfree(info.resv_objs);
-		kfree(info.dma_fence_excl_bitmap);
-	}
+	kfree(info.resv_objs);
+	kfree(info.dma_fence_excl_bitmap);
 #endif
 	return err_ret_val;
 }
@@ -596,7 +578,7 @@ static inline void jd_resolve_dep(struct list_head *out_list,
 #ifdef CONFIG_MALI_DMA_FENCE
 			int dep_count;
 
-			dep_count = kbase_fence_dep_count_read(dep_atom);
+			dep_count = atomic_read(&dep_atom->dma_fence.dep_count);
 			if (likely(dep_count == -1)) {
 				dep_satisfied = true;
 			} else {
@@ -735,8 +717,8 @@ static void jd_try_submitting_deps(struct list_head *out_list,
 #ifdef CONFIG_MALI_DMA_FENCE
 				int dep_count;
 
-				dep_count = kbase_fence_dep_count_read(
-								dep_atom);
+				dep_count = atomic_read(
+						&dep_atom->dma_fence.dep_count);
 				if (likely(dep_count == -1)) {
 					dep_satisfied = true;
 				} else {
@@ -975,11 +957,8 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 	 * the scheduler: 'not ready to run' and 'dependency-only' jobs. */
 	jctx->job_nr++;
 
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
 	katom->start_timestamp.tv64 = 0;
-#else
-	katom->start_timestamp = 0;
-#endif
+	katom->time_spent_us = 0;
 	katom->udata = user_atom->udata;
 	katom->kctx = kctx;
 	katom->nr_extres = user_atom->nr_extres;
@@ -1011,9 +990,11 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 	katom->kds_rset = NULL;
 #endif				/* CONFIG_KDS */
 #ifdef CONFIG_MALI_DMA_FENCE
-	kbase_fence_dep_count_set(katom, -1);
+	atomic_set(&katom->dma_fence.dep_count, -1);
 #endif
 
+	kbase_tlstream_tl_attrib_atom_state(katom, TL_ATOM_STATE_IDLE);
+
 	/* Don't do anything if there is a mess up with dependencies.
 	   This is done in a separate cycle to check both the dependencies at ones, otherwise
 	   it will be extra complexity to deal with 1st dependency ( just added to the list )
@@ -1032,13 +1013,11 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 				/* Wrong dependency setup. Atom will be sent
 				 * back to user space. Do not record any
 				 * dependencies. */
-				KBASE_TLSTREAM_TL_NEW_ATOM(
+				kbase_tlstream_tl_new_atom(
 						katom,
 						kbase_jd_atom_id(kctx, katom));
-				KBASE_TLSTREAM_TL_RET_ATOM_CTX(
+				kbase_tlstream_tl_ret_atom_ctx(
 						katom, kctx);
-				KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(katom,
-						TL_ATOM_STATE_IDLE);
 
 				ret = jd_done_nolock(katom, NULL);
 				goto out;
@@ -1079,12 +1058,10 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 			/* This atom is going through soft replay or
 			 * will be sent back to user space. Do not record any
 			 * dependencies. */
-			KBASE_TLSTREAM_TL_NEW_ATOM(
+			kbase_tlstream_tl_new_atom(
 					katom,
 					kbase_jd_atom_id(kctx, katom));
-			KBASE_TLSTREAM_TL_RET_ATOM_CTX(katom, kctx);
-			KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(katom,
-					TL_ATOM_STATE_IDLE);
+			kbase_tlstream_tl_ret_atom_ctx(katom, kctx);
 
 			if ((katom->core_req & BASE_JD_REQ_SOFT_JOB_TYPE)
 					 == BASE_JD_REQ_SOFT_REPLAY) {
@@ -1129,16 +1106,15 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 	katom->sched_priority = sched_prio;
 
 	/* Create a new atom recording all dependencies it was set up with. */
-	KBASE_TLSTREAM_TL_NEW_ATOM(
+	kbase_tlstream_tl_new_atom(
 			katom,
 			kbase_jd_atom_id(kctx, katom));
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(katom, TL_ATOM_STATE_IDLE);
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY(katom, katom->sched_priority);
-	KBASE_TLSTREAM_TL_RET_ATOM_CTX(katom, kctx);
+	kbase_tlstream_tl_attrib_atom_priority(katom, katom->sched_priority);
+	kbase_tlstream_tl_ret_atom_ctx(katom, kctx);
 	for (i = 0; i < 2; i++)
 		if (BASE_JD_DEP_TYPE_INVALID != kbase_jd_katom_dep_type(
 					&katom->dep[i])) {
-			KBASE_TLSTREAM_TL_DEP_ATOM_ATOM(
+			kbase_tlstream_tl_dep_atom_atom(
 					(void *)kbase_jd_katom_dep_atom(
 						&katom->dep[i]),
 					(void *)katom);
@@ -1150,7 +1126,7 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 			struct kbase_jd_atom *dep_atom =
 				&jctx->atoms[dep_atom_number];
 
-			KBASE_TLSTREAM_TL_RDEP_ATOM_ATOM(
+			kbase_tlstream_tl_rdep_atom_atom(
 					(void *)dep_atom,
 					(void *)katom);
 		}
@@ -1237,7 +1213,7 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 
 
 #ifdef CONFIG_MALI_DMA_FENCE
-	if (kbase_fence_dep_count_read(katom) != -1) {
+	if (atomic_read(&katom->dma_fence.dep_count) != -1) {
 		ret = false;
 		goto out;
 	}
@@ -1274,20 +1250,26 @@ bool jd_submit_atom(struct kbase_context *kctx, const struct base_jd_atom_v2 *us
 	return ret;
 }
 
+#ifdef BASE_LEGACY_UK6_SUPPORT
 int kbase_jd_submit(struct kbase_context *kctx,
-		void __user *user_addr, u32 nr_atoms, u32 stride,
-		bool uk6_atom)
+		const struct kbase_uk_job_submit *submit_data,
+		int uk6_atom)
+#else
+int kbase_jd_submit(struct kbase_context *kctx,
+		const struct kbase_uk_job_submit *submit_data)
+#endif /* BASE_LEGACY_UK6_SUPPORT */
 {
 	struct kbase_jd_context *jctx = &kctx->jctx;
 	int err = 0;
 	int i;
 	bool need_to_try_schedule_context = false;
 	struct kbase_device *kbdev;
+	void __user *user_addr;
 	u32 latest_flush;
 
 	/*
-	 * kbase_jd_submit isn't expected to fail and so all errors with the
-	 * jobs are reported by immediately failing them (through event system)
+	 * kbase_jd_submit isn't expected to fail and so all errors with the jobs
+	 * are reported by immediately falling them (through event system)
 	 */
 	kbdev = kctx->kbdev;
 
@@ -1298,25 +1280,29 @@ int kbase_jd_submit(struct kbase_context *kctx,
 		return -EINVAL;
 	}
 
-	if (stride != sizeof(base_jd_atom_v2)) {
+#ifdef BASE_LEGACY_UK6_SUPPORT
+	if ((uk6_atom && submit_data->stride !=
+			sizeof(struct base_jd_atom_v2_uk6)) ||
+			submit_data->stride != sizeof(base_jd_atom_v2)) {
+#else
+	if (submit_data->stride != sizeof(base_jd_atom_v2)) {
+#endif /* BASE_LEGACY_UK6_SUPPORT */
 		dev_err(kbdev->dev, "Stride passed to job_submit doesn't match kernel");
 		return -EINVAL;
 	}
 
-	KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_add_return(nr_atoms,
-				&kctx->timeline.jd_atoms_in_flight));
+	user_addr = get_compat_pointer(kctx, &submit_data->addr);
+
+	KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_add_return(submit_data->nr_atoms, &kctx->timeline.jd_atoms_in_flight));
 
 	/* All atoms submitted in this call have the same flush ID */
 	latest_flush = kbase_backend_get_current_flush_id(kbdev);
 
-	for (i = 0; i < nr_atoms; i++) {
+	for (i = 0; i < submit_data->nr_atoms; i++) {
 		struct base_jd_atom_v2 user_atom;
 		struct kbase_jd_atom *katom;
 
 #ifdef BASE_LEGACY_UK6_SUPPORT
-		BUILD_BUG_ON(sizeof(struct base_jd_atom_v2_uk6) !=
-				sizeof(base_jd_atom_v2));
-
 		if (uk6_atom) {
 			struct base_jd_atom_v2_uk6 user_atom_v6;
 			base_jd_dep_type dep_types[2] = {BASE_JD_DEP_TYPE_DATA, BASE_JD_DEP_TYPE_DATA};
@@ -1326,7 +1312,7 @@ int kbase_jd_submit(struct kbase_context *kctx,
 				err = -EINVAL;
 				KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx,
 					atomic_sub_return(
-					nr_atoms - i,
+					submit_data->nr_atoms - i,
 					&kctx->timeline.jd_atoms_in_flight));
 				break;
 			}
@@ -1358,17 +1344,14 @@ int kbase_jd_submit(struct kbase_context *kctx,
 			user_atom.device_nr = user_atom_v6.device_nr;
 		} else {
 #endif /* BASE_LEGACY_UK6_SUPPORT */
-			if (copy_from_user(&user_atom, user_addr,
-						sizeof(user_atom)) != 0) {
-				err = -EINVAL;
-				KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx,
-					atomic_sub_return(nr_atoms - i,
-					&kctx->timeline.jd_atoms_in_flight));
-				break;
-			}
+		if (copy_from_user(&user_atom, user_addr, sizeof(user_atom)) != 0) {
+			err = -EINVAL;
+			KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_sub_return(submit_data->nr_atoms - i, &kctx->timeline.jd_atoms_in_flight));
+			break;
+		}
 #ifdef BASE_LEGACY_UK6_SUPPORT
 		}
-#endif
+#endif /* BASE_LEGACY_UK6_SUPPORT */
 
 #ifdef BASE_LEGACY_UK10_2_SUPPORT
 		if (KBASE_API_VERSION(10, 3) > kctx->api_version)
@@ -1376,7 +1359,7 @@ int kbase_jd_submit(struct kbase_context *kctx,
 					      & 0x7fff);
 #endif /* BASE_LEGACY_UK10_2_SUPPORT */
 
-		user_addr = (void __user *)((uintptr_t) user_addr + stride);
+		user_addr = (void __user *)((uintptr_t) user_addr + submit_data->stride);
 
 		mutex_lock(&jctx->lock);
 #ifndef compiletime_assert
@@ -1476,7 +1459,7 @@ void kbase_jd_done_worker(struct work_struct *data)
 	 * Begin transaction on JD context and JS context
 	 */
 	mutex_lock(&jctx->lock);
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(katom, TL_ATOM_STATE_DONE);
+	kbase_tlstream_tl_attrib_atom_state(katom, TL_ATOM_STATE_DONE);
 	mutex_lock(&js_devdata->queue_mutex);
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
 
@@ -1731,12 +1714,15 @@ KBASE_EXPORT_TEST_API(kbase_jd_done);
 void kbase_jd_cancel(struct kbase_device *kbdev, struct kbase_jd_atom *katom)
 {
 	struct kbase_context *kctx;
+	struct kbasep_js_kctx_info *js_kctx_info;
 
 	KBASE_DEBUG_ASSERT(NULL != kbdev);
 	KBASE_DEBUG_ASSERT(NULL != katom);
 	kctx = katom->kctx;
 	KBASE_DEBUG_ASSERT(NULL != kctx);
 
+	js_kctx_info = &kctx->jctx.sched_info;
+
 	KBASE_TRACE_ADD(kbdev, JD_CANCEL, kctx, katom, katom->jc, 0);
 
 	/* This should only be done from a context that is not scheduled */
@@ -1843,9 +1829,8 @@ int kbase_jd_init(struct kbase_context *kctx)
 		kctx->jctx.atoms[i].event_code = BASE_JD_EVENT_JOB_INVALID;
 		kctx->jctx.atoms[i].status = KBASE_JD_ATOM_STATE_UNUSED;
 
-#if defined(CONFIG_MALI_DMA_FENCE) || defined(CONFIG_SYNC_FILE)
-		kctx->jctx.atoms[i].dma_fence.context =
-						dma_fence_context_alloc(1);
+#ifdef CONFIG_MALI_DMA_FENCE
+		kctx->jctx.atoms[i].dma_fence.context = fence_context_alloc(1);
 		atomic_set(&kctx->jctx.atoms[i].dma_fence.seqno, 0);
 		INIT_LIST_HEAD(&kctx->jctx.atoms[i].dma_fence.callbacks);
 #endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.c b/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.c
index c8b37c4..6437e42 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -15,122 +15,14 @@
 
 
 
-#ifdef CONFIG_DEBUG_FS
-
 #include <linux/seq_file.h>
-#include <mali_kbase.h>
-#include <mali_kbase_jd_debugfs.h>
-#include <mali_kbase_dma_fence.h>
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-#include <mali_kbase_sync.h>
-#endif
-
-struct kbase_jd_debugfs_depinfo {
-	u8 id;
-	char type;
-};
 
-static void kbase_jd_debugfs_fence_info(struct kbase_jd_atom *atom,
-					struct seq_file *sfile)
-{
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-	struct kbase_sync_fence_info info;
-	int res;
-
-	switch (atom->core_req & BASE_JD_REQ_SOFT_JOB_TYPE) {
-	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
-		res = kbase_sync_fence_out_info_get(atom, &info);
-		if (0 == res) {
-			seq_printf(sfile, "Sa([%p]%d) ",
-				   info.fence, info.status);
-			break;
-		}
-	case BASE_JD_REQ_SOFT_FENCE_WAIT:
-		res = kbase_sync_fence_in_info_get(atom, &info);
-		if (0 == res) {
-			seq_printf(sfile, "Wa([%p]%d) ",
-				   info.fence, info.status);
-			break;
-		}
-	default:
-		break;
-	}
-#endif /* CONFIG_SYNC || CONFIG_SYNC_FILE */
-
-#ifdef CONFIG_MALI_DMA_FENCE
-	if (atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES) {
-		struct kbase_fence_cb *cb;
-
-		if (atom->dma_fence.fence) {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-			struct fence *fence = atom->dma_fence.fence;
-#else
-			struct dma_fence *fence = atom->dma_fence.fence;
-#endif
-
-			seq_printf(sfile,
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 8, 0))
-					"Sd(%u#%u: %s) ",
-#else
-					"Sd(%llu#%u: %s) ",
-#endif
-					fence->context,
-					fence->seqno,
-					dma_fence_is_signaled(fence) ?
-						"signaled" : "active");
-		}
-
-		list_for_each_entry(cb, &atom->dma_fence.callbacks,
-				    node) {
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-			struct fence *fence = cb->fence;
-#else
-			struct dma_fence *fence = cb->fence;
-#endif
-
-			seq_printf(sfile,
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 8, 0))
-					"Wd(%u#%u: %s) ",
-#else
-					"Wd(%llu#%u: %s) ",
-#endif
-					fence->context,
-					fence->seqno,
-					dma_fence_is_signaled(fence) ?
-						"signaled" : "active");
-		}
-	}
-#endif /* CONFIG_MALI_DMA_FENCE */
+#include <mali_kbase.h>
 
-}
+#include <mali_kbase_jd_debugfs.h>
 
-static void kbasep_jd_debugfs_atom_deps(
-		struct kbase_jd_debugfs_depinfo *deps,
-		struct kbase_jd_atom *atom)
-{
-	struct kbase_context *kctx = atom->kctx;
-	int i;
+#ifdef CONFIG_DEBUG_FS
 
-	for (i = 0; i < 2; i++)	{
-		deps[i].id = (unsigned)(atom->dep[i].atom ?
-				kbase_jd_atom_id(kctx, atom->dep[i].atom) : 0);
-
-		switch (atom->dep[i].dep_type) {
-		case BASE_JD_DEP_TYPE_INVALID:
-			deps[i].type = ' ';
-			break;
-		case BASE_JD_DEP_TYPE_DATA:
-			deps[i].type = 'D';
-			break;
-		case BASE_JD_DEP_TYPE_ORDER:
-			deps[i].type = '>';
-			break;
-		default:
-			deps[i].type = '?';
-			break;
-		}
-	}
-}
 /**
  * kbasep_jd_debugfs_atoms_show - Show callback for the JD atoms debugfs file.
  * @sfile: The debugfs entry
@@ -159,7 +51,7 @@ static int kbasep_jd_debugfs_atoms_show(struct seq_file *sfile, void *data)
 			BASE_UK_VERSION_MINOR);
 
 	/* Print table heading */
-	seq_puts(sfile, " ID, Core req, St, CR,   Predeps,           Start time, Additional info...\n");
+	seq_puts(sfile, "atom id,core reqs,status,coreref status,predeps,start time,time on gpu\n");
 
 	atoms = kctx->jctx.atoms;
 	/* General atom states */
@@ -169,7 +61,6 @@ static int kbasep_jd_debugfs_atoms_show(struct seq_file *sfile, void *data)
 	for (i = 0; i != BASE_JD_ATOM_COUNT; ++i) {
 		struct kbase_jd_atom *atom = &atoms[i];
 		s64 start_timestamp = 0;
-		struct kbase_jd_debugfs_depinfo deps[2];
 
 		if (atom->status == KBASE_JD_ATOM_STATE_UNUSED)
 			continue;
@@ -181,20 +72,17 @@ static int kbasep_jd_debugfs_atoms_show(struct seq_file *sfile, void *data)
 			start_timestamp = ktime_to_ns(
 					ktime_sub(ktime_get(), atom->start_timestamp));
 
-		kbasep_jd_debugfs_atom_deps(deps, atom);
-
 		seq_printf(sfile,
-				"%3u, %8x, %2u, %2u, %c%3u %c%3u, %20lld, ",
-				i, atom->core_req, atom->status,
-				atom->coreref_state,
-				deps[0].type, deps[0].id,
-				deps[1].type, deps[1].id,
-				start_timestamp);
-
-
-		kbase_jd_debugfs_fence_info(atom, sfile);
-
-		seq_puts(sfile, "\n");
+				"%i,%u,%u,%u,%u %u,%lli,%llu\n",
+				i, atom->core_req, atom->status, atom->coreref_state,
+				(unsigned)(atom->dep[0].atom ?
+						atom->dep[0].atom - atoms : 0),
+				(unsigned)(atom->dep[1].atom ?
+						atom->dep[1].atom - atoms : 0),
+				(signed long long)start_timestamp,
+				(unsigned long long)(atom->time_spent_us ?
+					atom->time_spent_us * 1000 : start_timestamp)
+				);
 	}
 	spin_unlock_irqrestore(&kctx->kbdev->hwaccess_lock, irq_flags);
 	mutex_unlock(&kctx->jctx.lock);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.h b/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.h
index 0935f1d..090f816 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_jd_debugfs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2015 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,7 +27,7 @@
 
 #include <mali_kbase.h>
 
-#define MALI_JD_DEBUGFS_VERSION 2
+#define MALI_JD_DEBUGFS_VERSION 1
 
 /**
  * kbasep_jd_debugfs_ctx_init() - Add debugfs entries for JD system
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js.c b/drivers/gpu/arm/midgard/mali_kbase_js.c
index b053ce1..60a7373 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_js.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_js.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,7 +27,6 @@
 #endif
 #include <mali_kbase_tlstream.h>
 #include <mali_kbase_hw.h>
-#include <mali_kbase_ctx_sched.h>
 
 #include <mali_kbase_defs.h>
 #include <mali_kbase_config_defaults.h>
@@ -76,14 +75,53 @@ static int kbase_js_get_slot(struct kbase_device *kbdev,
 				struct kbase_jd_atom *katom);
 
 static void kbase_js_foreach_ctx_job(struct kbase_context *kctx,
-		kbasep_js_ctx_job_cb callback);
+		kbasep_js_policy_ctx_job_cb callback);
 
 /* Helper for trace subcodes */
 #if KBASE_TRACE_ENABLE
 static int kbasep_js_trace_get_refcnt(struct kbase_device *kbdev,
 		struct kbase_context *kctx)
 {
-	return atomic_read(&kctx->refcount);
+	unsigned long flags;
+	struct kbasep_js_device_data *js_devdata;
+	int as_nr;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		struct kbasep_js_per_as_data *js_per_as_data;
+
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		refcnt = js_per_as_data->as_busy_refcount;
+	}
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return refcnt;
+}
+
+static int kbasep_js_trace_get_refcnt_nolock(struct kbase_device *kbdev,
+						struct kbase_context *kctx)
+{
+	struct kbasep_js_device_data *js_devdata;
+	int as_nr;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		struct kbasep_js_per_as_data *js_per_as_data;
+
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		refcnt = js_per_as_data->as_busy_refcount;
+	}
+
+	return refcnt;
 }
 #else				/* KBASE_TRACE_ENABLE  */
 static int kbasep_js_trace_get_refcnt(struct kbase_device *kbdev,
@@ -93,9 +131,33 @@ static int kbasep_js_trace_get_refcnt(struct kbase_device *kbdev,
 	CSTD_UNUSED(kctx);
 	return 0;
 }
+static int kbasep_js_trace_get_refcnt_nolock(struct kbase_device *kbdev,
+						struct kbase_context *kctx)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(kctx);
+	return 0;
+}
 #endif				/* KBASE_TRACE_ENABLE  */
 
 /*
+ * Private types
+ */
+enum {
+	JS_DEVDATA_INIT_NONE = 0,
+	JS_DEVDATA_INIT_CONSTANTS = (1 << 0),
+	JS_DEVDATA_INIT_POLICY = (1 << 1),
+	JS_DEVDATA_INIT_ALL = ((1 << 2) - 1)
+};
+
+enum {
+	JS_KCTX_INIT_NONE = 0,
+	JS_KCTX_INIT_CONSTANTS = (1 << 0),
+	JS_KCTX_INIT_POLICY = (1 << 1),
+	JS_KCTX_INIT_ALL = ((1 << 2) - 1)
+};
+
+/*
  * Private functions
  */
 
@@ -136,11 +198,12 @@ static void kbase_js_sync_timers(struct kbase_device *kbdev)
 	mutex_unlock(&kbdev->js_data.runpool_mutex);
 }
 
-/* Hold the mmu_hw_mutex and hwaccess_lock for this */
+/* Hold the hwaccess_lock for this */
 bool kbasep_js_runpool_retain_ctx_nolock(struct kbase_device *kbdev,
 		struct kbase_context *kctx)
 {
 	struct kbasep_js_device_data *js_devdata;
+	struct kbasep_js_per_as_data *js_per_as_data;
 	bool result = false;
 	int as_nr;
 
@@ -148,15 +211,19 @@ bool kbasep_js_runpool_retain_ctx_nolock(struct kbase_device *kbdev,
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 	js_devdata = &kbdev->js_data;
 
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
 	as_nr = kctx->as_nr;
-	if (atomic_read(&kctx->refcount) > 0) {
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		int new_refcnt;
+
 		KBASE_DEBUG_ASSERT(as_nr >= 0);
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		KBASE_DEBUG_ASSERT(js_per_as_data->kctx != NULL);
+
+		new_refcnt = ++(js_per_as_data->as_busy_refcount);
 
-		kbase_ctx_sched_retain_ctx_refcount(kctx);
 		KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_RETAIN_CTX_NOLOCK, kctx,
-				NULL, 0u, atomic_read(&kctx->refcount));
+				NULL, 0u, new_refcnt);
 		result = true;
 	}
 
@@ -231,7 +298,7 @@ jsctx_rb_none_to_pull(struct kbase_context *kctx, int js)
  */
 static void
 jsctx_queue_foreach_prio(struct kbase_context *kctx, int js, int prio,
-		kbasep_js_ctx_job_cb callback)
+		kbasep_js_policy_ctx_job_cb callback)
 {
 	struct jsctx_queue *queue = &kctx->jsctx_queue[prio][js];
 
@@ -268,7 +335,7 @@ jsctx_queue_foreach_prio(struct kbase_context *kctx, int js, int prio,
  */
 static inline void
 jsctx_queue_foreach(struct kbase_context *kctx, int js,
-		kbasep_js_ctx_job_cb callback)
+		kbasep_js_policy_ctx_job_cb callback)
 {
 	int prio;
 
@@ -419,12 +486,29 @@ static bool kbase_js_ctx_list_add_unpullable_nolock(struct kbase_device *kbdev,
 int kbasep_js_devdata_init(struct kbase_device * const kbdev)
 {
 	struct kbasep_js_device_data *jsdd;
+	int err;
 	int i;
+	u16 as_present;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 
 	jsdd = &kbdev->js_data;
 
+	KBASE_DEBUG_ASSERT(jsdd->init_status == JS_DEVDATA_INIT_NONE);
+
+	/* These two must be recalculated if nr_hw_address_spaces changes
+	 * (e.g. for HW workarounds) */
+	as_present = (1U << kbdev->nr_hw_address_spaces) - 1;
+	kbdev->nr_user_address_spaces = kbdev->nr_hw_address_spaces;
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
+		bool use_workaround;
+
+		use_workaround = DEFAULT_SECURE_BUT_LOSS_OF_PERFORMANCE;
+		if (use_workaround) {
+			dev_dbg(kbdev->dev, "GPU has HW ISSUE 8987, and driver configured for security workaround: 1 address space only");
+			kbdev->nr_user_address_spaces = 1;
+		}
+	}
 #ifdef CONFIG_MALI_DEBUG
 	/* Soft-stop will be disabled on a single context by default unless
 	 * softstop_always is set */
@@ -434,6 +518,8 @@ int kbasep_js_devdata_init(struct kbase_device * const kbdev)
 	jsdd->nr_user_contexts_running = 0;
 	jsdd->nr_contexts_pullable = 0;
 	atomic_set(&jsdd->nr_contexts_runnable, 0);
+	/* All ASs initially free */
+	jsdd->as_free = as_present;
 	/* No ctx allowed to submit */
 	jsdd->runpool_irq.submit_allowed = 0u;
 	memset(jsdd->runpool_irq.ctx_attr_ref_count, 0,
@@ -461,6 +547,10 @@ int kbasep_js_devdata_init(struct kbase_device * const kbdev)
 	jsdd->gpu_reset_ticks_cl = DEFAULT_JS_RESET_TICKS_CL;
 	jsdd->gpu_reset_ticks_dumping = DEFAULT_JS_RESET_TICKS_DUMPING;
 	jsdd->ctx_timeslice_ns = DEFAULT_JS_CTX_TIMESLICE_NS;
+	jsdd->cfs_ctx_runtime_init_slices =
+		DEFAULT_JS_CFS_CTX_RUNTIME_INIT_SLICES;
+	jsdd->cfs_ctx_runtime_min_slices =
+		DEFAULT_JS_CFS_CTX_RUNTIME_MIN_SLICES;
 	atomic_set(&jsdd->soft_job_timeout_ms, DEFAULT_JS_SOFT_JOB_TIMEOUT);
 
 	dev_dbg(kbdev->dev, "JS Config Attribs: ");
@@ -484,6 +574,10 @@ int kbasep_js_devdata_init(struct kbase_device * const kbdev)
 			jsdd->gpu_reset_ticks_dumping);
 	dev_dbg(kbdev->dev, "\tctx_timeslice_ns:%u",
 			jsdd->ctx_timeslice_ns);
+	dev_dbg(kbdev->dev, "\tcfs_ctx_runtime_init_slices:%u",
+			jsdd->cfs_ctx_runtime_init_slices);
+	dev_dbg(kbdev->dev, "\tcfs_ctx_runtime_min_slices:%u",
+			jsdd->cfs_ctx_runtime_min_slices);
 	dev_dbg(kbdev->dev, "\tsoft_job_timeout:%i",
 		atomic_read(&jsdd->soft_job_timeout_ms));
 
@@ -497,24 +591,39 @@ int kbasep_js_devdata_init(struct kbase_device * const kbdev)
 	}
 
 #if KBASE_DISABLE_SCHEDULING_SOFT_STOPS
-	dev_dbg(kbdev->dev, "Job Scheduling Soft-stops disabled, ignoring value for soft_stop_ticks==%u at %uns per tick. Other soft-stops may still occur.",
+	dev_dbg(kbdev->dev, "Job Scheduling Policy Soft-stops disabled, ignoring value for soft_stop_ticks==%u at %uns per tick. Other soft-stops may still occur.",
 			jsdd->soft_stop_ticks,
 			jsdd->scheduling_period_ns);
 #endif
 #if KBASE_DISABLE_SCHEDULING_HARD_STOPS
-	dev_dbg(kbdev->dev, "Job Scheduling Hard-stops disabled, ignoring values for hard_stop_ticks_ss==%d and hard_stop_ticks_dumping==%u at %uns per tick. Other hard-stops may still occur.",
+	dev_dbg(kbdev->dev, "Job Scheduling Policy Hard-stops disabled, ignoring values for hard_stop_ticks_ss==%d and hard_stop_ticks_dumping==%u at %uns per tick. Other hard-stops may still occur.",
 			jsdd->hard_stop_ticks_ss,
 			jsdd->hard_stop_ticks_dumping,
 			jsdd->scheduling_period_ns);
 #endif
 #if KBASE_DISABLE_SCHEDULING_SOFT_STOPS && KBASE_DISABLE_SCHEDULING_HARD_STOPS
-	dev_dbg(kbdev->dev, "Note: The JS tick timer (if coded) will still be run, but do nothing.");
+	dev_dbg(kbdev->dev, "Note: The JS policy's tick timer (if coded) will still be run, but do nothing.");
 #endif
 
+	/* setup the number of irq throttle cycles base on given time */
+	{
+		int time_us = kbdev->gpu_props.irq_throttle_time_us;
+		int cycles = kbasep_js_convert_us_to_gpu_ticks_max_freq(kbdev,
+				time_us);
+
+		atomic_set(&kbdev->irq_throttle_cycles, cycles);
+	}
+
+	/* Clear the AS data, including setting NULL pointers */
+	memset(&jsdd->runpool_irq.per_as_data[0], 0,
+			sizeof(jsdd->runpool_irq.per_as_data));
+
 	for (i = 0; i < kbdev->gpu_props.num_job_slots; ++i)
 		jsdd->js_reqs[i] = core_reqs_from_jsn_features(
 			kbdev->gpu_props.props.raw_props.js_features[i]);
 
+	jsdd->init_status |= JS_DEVDATA_INIT_CONSTANTS;
+
 	/* On error, we could continue on: providing none of the below resources
 	 * rely on the ones above */
 
@@ -523,11 +632,20 @@ int kbasep_js_devdata_init(struct kbase_device * const kbdev)
 	spin_lock_init(&kbdev->hwaccess_lock);
 	sema_init(&jsdd->schedule_sem, 1);
 
+	err = kbasep_js_policy_init(kbdev);
+	if (!err)
+		jsdd->init_status |= JS_DEVDATA_INIT_POLICY;
+
 	for (i = 0; i < kbdev->gpu_props.num_job_slots; ++i) {
 		INIT_LIST_HEAD(&jsdd->ctx_list_pullable[i]);
 		INIT_LIST_HEAD(&jsdd->ctx_list_unpullable[i]);
 	}
 
+	/* On error, do no cleanup; this will be handled by the caller(s), since
+	 * we've designed this resource to be safe to terminate on init-fail */
+	if (jsdd->init_status != JS_DEVDATA_INIT_ALL)
+		return -EINVAL;
+
 	return 0;
 }
 
@@ -539,26 +657,33 @@ void kbasep_js_devdata_halt(struct kbase_device *kbdev)
 void kbasep_js_devdata_term(struct kbase_device *kbdev)
 {
 	struct kbasep_js_device_data *js_devdata;
-	s8 zero_ctx_attr_ref_count[KBASEP_JS_CTX_ATTR_COUNT] = { 0, };
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 
 	js_devdata = &kbdev->js_data;
 
-	/* The caller must de-register all contexts before calling this
-	 */
-	KBASE_DEBUG_ASSERT(js_devdata->nr_all_contexts_running == 0);
-	KBASE_DEBUG_ASSERT(memcmp(
-	        js_devdata->runpool_irq.ctx_attr_ref_count,
-	        zero_ctx_attr_ref_count,
-	        sizeof(zero_ctx_attr_ref_count)) == 0);
-	CSTD_UNUSED(zero_ctx_attr_ref_count);
+	if ((js_devdata->init_status & JS_DEVDATA_INIT_CONSTANTS)) {
+		s8 zero_ctx_attr_ref_count[KBASEP_JS_CTX_ATTR_COUNT] = { 0, };
+		/* The caller must de-register all contexts before calling this
+		 */
+		KBASE_DEBUG_ASSERT(js_devdata->nr_all_contexts_running == 0);
+		KBASE_DEBUG_ASSERT(memcmp(
+				js_devdata->runpool_irq.ctx_attr_ref_count,
+				zero_ctx_attr_ref_count,
+				sizeof(zero_ctx_attr_ref_count)) == 0);
+		CSTD_UNUSED(zero_ctx_attr_ref_count);
+	}
+	if ((js_devdata->init_status & JS_DEVDATA_INIT_POLICY))
+		kbasep_js_policy_term(&js_devdata->policy);
+
+	js_devdata->init_status = JS_DEVDATA_INIT_NONE;
 }
 
 int kbasep_js_kctx_init(struct kbase_context * const kctx)
 {
 	struct kbase_device *kbdev;
 	struct kbasep_js_kctx_info *js_kctx_info;
+	int err;
 	int i, j;
 
 	KBASE_DEBUG_ASSERT(kctx != NULL);
@@ -570,6 +695,7 @@ int kbasep_js_kctx_init(struct kbase_context * const kctx)
 		INIT_LIST_HEAD(&kctx->jctx.sched_info.ctx.ctx_list_entry[i]);
 
 	js_kctx_info = &kctx->jctx.sched_info;
+	KBASE_DEBUG_ASSERT(js_kctx_info->init_status == JS_KCTX_INIT_NONE);
 
 	js_kctx_info->ctx.nr_jobs = 0;
 	kbase_ctx_flag_clear(kctx, KCTX_SCHEDULED);
@@ -581,12 +707,23 @@ int kbasep_js_kctx_init(struct kbase_context * const kctx)
 	 * flags are set */
 	kbase_ctx_flag_set(kctx, KCTX_SUBMIT_DISABLED);
 
+	js_kctx_info->init_status |= JS_KCTX_INIT_CONSTANTS;
+
 	/* On error, we could continue on: providing none of the below resources
 	 * rely on the ones above */
 	mutex_init(&js_kctx_info->ctx.jsctx_mutex);
 
 	init_waitqueue_head(&js_kctx_info->ctx.is_scheduled_wait);
 
+	err = kbasep_js_policy_init_ctx(kbdev, kctx);
+	if (!err)
+		js_kctx_info->init_status |= JS_KCTX_INIT_POLICY;
+
+	/* On error, do no cleanup; this will be handled by the caller(s), since
+	 * we've designed this resource to be safe to terminate on init-fail */
+	if (js_kctx_info->init_status != JS_KCTX_INIT_ALL)
+		return -EINVAL;
+
 	for (i = 0; i < KBASE_JS_ATOM_SCHED_PRIO_COUNT; i++) {
 		for (j = 0; j < BASE_JM_MAX_NR_SLOTS; j++) {
 			INIT_LIST_HEAD(&kctx->jsctx_queue[i][j].x_dep_head);
@@ -601,6 +738,7 @@ void kbasep_js_kctx_term(struct kbase_context *kctx)
 {
 	struct kbase_device *kbdev;
 	struct kbasep_js_kctx_info *js_kctx_info;
+	union kbasep_js_policy *js_policy;
 	int js;
 	bool update_ctx_count = false;
 
@@ -609,11 +747,14 @@ void kbasep_js_kctx_term(struct kbase_context *kctx)
 	kbdev = kctx->kbdev;
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 
+	js_policy = &kbdev->js_data.policy;
 	js_kctx_info = &kctx->jctx.sched_info;
 
-	/* The caller must de-register all jobs before calling this */
-	KBASE_DEBUG_ASSERT(!kbase_ctx_flag(kctx, KCTX_SCHEDULED));
-	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.nr_jobs == 0);
+	if ((js_kctx_info->init_status & JS_KCTX_INIT_CONSTANTS)) {
+		/* The caller must de-register all jobs before calling this */
+		KBASE_DEBUG_ASSERT(!kbase_ctx_flag(kctx, KCTX_SCHEDULED));
+		KBASE_DEBUG_ASSERT(js_kctx_info->ctx.nr_jobs == 0);
+	}
 
 	mutex_lock(&kbdev->js_data.queue_mutex);
 	mutex_lock(&kctx->jctx.sched_info.ctx.jsctx_mutex);
@@ -631,6 +772,11 @@ void kbasep_js_kctx_term(struct kbase_context *kctx)
 	mutex_unlock(&kctx->jctx.sched_info.ctx.jsctx_mutex);
 	mutex_unlock(&kbdev->js_data.queue_mutex);
 
+	if ((js_kctx_info->init_status & JS_KCTX_INIT_POLICY))
+		kbasep_js_policy_term_ctx(js_policy, kctx);
+
+	js_kctx_info->init_status = JS_KCTX_INIT_NONE;
+
 	if (update_ctx_count) {
 		mutex_lock(&kbdev->js_data.runpool_mutex);
 		kbase_backend_ctx_count_changed(kbdev);
@@ -915,8 +1061,6 @@ static bool kbase_js_ctx_pullable(struct kbase_context *kctx, int js,
 	katom = jsctx_rb_peek(kctx, js);
 	if (!katom)
 		return false; /* No pullable atoms */
-	if (kctx->blocked_js[js][katom->sched_priority])
-		return false;
 	if (atomic_read(&katom->blocked))
 		return false; /* next atom blocked */
 	if (katom->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED) {
@@ -1069,6 +1213,7 @@ bool kbasep_js_add_job(struct kbase_context *kctx,
 	struct kbasep_js_kctx_info *js_kctx_info;
 	struct kbase_device *kbdev;
 	struct kbasep_js_device_data *js_devdata;
+	union kbasep_js_policy *js_policy;
 
 	bool enqueue_required = false;
 	bool timer_sync = false;
@@ -1079,6 +1224,7 @@ bool kbasep_js_add_job(struct kbase_context *kctx,
 
 	kbdev = kctx->kbdev;
 	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
 	js_kctx_info = &kctx->jctx.sched_info;
 
 	mutex_lock(&js_devdata->queue_mutex);
@@ -1113,13 +1259,13 @@ bool kbasep_js_add_job(struct kbase_context *kctx,
 		goto out_unlock;
 	}
 
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(atom, TL_ATOM_STATE_READY);
+	kbase_tlstream_tl_attrib_atom_state(atom, TL_ATOM_STATE_READY);
 	KBASE_TIMELINE_ATOM_READY(kctx, kbase_jd_atom_id(kctx, atom));
 
 	enqueue_required = kbase_js_dep_resolved_submit(kctx, atom);
 
 	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_ADD_JOB, kctx, atom, atom->jc,
-				kbasep_js_trace_get_refcnt(kbdev, kctx));
+				kbasep_js_trace_get_refcnt_nolock(kbdev, kctx));
 
 	/* Context Attribute Refcounting */
 	kbasep_js_ctx_attr_ctx_retain_atom(kbdev, kctx, atom);
@@ -1153,11 +1299,11 @@ bool kbasep_js_add_job(struct kbase_context *kctx,
 					false);
 		} else if (js_kctx_info->ctx.nr_jobs == 1) {
 			/* Handle Refcount going from 0 to 1: schedule the
-			 * context on the Queue */
+			 * context on the Policy Queue */
 			KBASE_DEBUG_ASSERT(!kbase_ctx_flag(kctx, KCTX_SCHEDULED));
 			dev_dbg(kbdev->dev, "JS: Enqueue Context %p", kctx);
 
-			/* Queue was updated - caller must try to
+			/* Policy Queue was updated - caller must try to
 			 * schedule the head context */
 			WARN_ON(!enqueue_required);
 		}
@@ -1175,12 +1321,14 @@ void kbasep_js_remove_job(struct kbase_device *kbdev,
 {
 	struct kbasep_js_kctx_info *js_kctx_info;
 	struct kbasep_js_device_data *js_devdata;
+	union kbasep_js_policy *js_policy;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 	KBASE_DEBUG_ASSERT(atom != NULL);
 
 	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
 	js_kctx_info = &kctx->jctx.sched_info;
 
 	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_REMOVE_JOB, kctx, atom, atom->jc,
@@ -1233,11 +1381,11 @@ bool kbasep_js_runpool_retain_ctx(struct kbase_device *kbdev,
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	js_devdata = &kbdev->js_data;
 
-	mutex_lock(&kbdev->mmu_hw_mutex);
+	/* KBASE_TRACE_ADD_REFCOUNT( kbdev, JS_RETAIN_CTX, kctx, NULL, 0,
+	   kbasep_js_trace_get_refcnt(kbdev, kctx)); */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	result = kbasep_js_runpool_retain_ctx_nolock(kbdev, kctx);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-	mutex_unlock(&kbdev->mmu_hw_mutex);
 
 	return result;
 }
@@ -1248,23 +1396,48 @@ struct kbase_context *kbasep_js_runpool_lookup_ctx(struct kbase_device *kbdev,
 	unsigned long flags;
 	struct kbasep_js_device_data *js_devdata;
 	struct kbase_context *found_kctx = NULL;
+	struct kbasep_js_per_as_data *js_per_as_data;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(0 <= as_nr && as_nr < BASE_MAX_NR_AS);
 	js_devdata = &kbdev->js_data;
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-	found_kctx = kbdev->as_to_kctx[as_nr];
+	found_kctx = js_per_as_data->kctx;
 
 	if (found_kctx != NULL)
-		kbase_ctx_sched_retain_ctx_refcount(found_kctx);
+		++(js_per_as_data->as_busy_refcount);
 
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	return found_kctx;
 }
 
+struct kbase_context *kbasep_js_runpool_lookup_ctx_nolock(
+		struct kbase_device *kbdev, int as_nr)
+{
+	struct kbasep_js_device_data *js_devdata;
+	struct kbase_context *found_kctx = NULL;
+	struct kbasep_js_per_as_data *js_per_as_data;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(0 <= as_nr && as_nr < BASE_MAX_NR_AS);
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	js_devdata = &kbdev->js_data;
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+	found_kctx = js_per_as_data->kctx;
+
+	if (found_kctx != NULL)
+		++(js_per_as_data->as_busy_refcount);
+
+	return found_kctx;
+}
+
 /**
  * kbasep_js_release_result - Try running more jobs after releasing a context
  *                            and/or atom
@@ -1350,6 +1523,8 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 	unsigned long flags;
 	struct kbasep_js_device_data *js_devdata;
 	struct kbasep_js_kctx_info *js_kctx_info;
+	union kbasep_js_policy *js_policy;
+	struct kbasep_js_per_as_data *js_per_as_data;
 
 	kbasep_js_release_result release_result = 0u;
 	bool runpool_ctx_attr_change = false;
@@ -1361,13 +1536,21 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 	js_kctx_info = &kctx->jctx.sched_info;
 	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
 
 	/* Ensure context really is scheduled in */
 	KBASE_DEBUG_ASSERT(kbase_ctx_flag(kctx, KCTX_SCHEDULED));
 
+	/* kctx->as_nr and js_per_as_data are only read from here. The caller's
+	 * js_ctx_mutex provides a barrier that ensures they are up-to-date.
+	 *
+	 * They will not change whilst we're reading them, because the refcount
+	 * is non-zero (and we ASSERT on that last fact).
+	 */
 	kctx_as_nr = kctx->as_nr;
 	KBASE_DEBUG_ASSERT(kctx_as_nr != KBASEP_AS_NR_INVALID);
-	KBASE_DEBUG_ASSERT(atomic_read(&kctx->refcount) > 0);
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[kctx_as_nr];
+	KBASE_DEBUG_ASSERT(js_per_as_data->as_busy_refcount > 0);
 
 	/*
 	 * Transaction begins on AS and runpool_irq
@@ -1376,14 +1559,14 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 	 */
 	current_as = &kbdev->as[kctx_as_nr];
 	mutex_lock(&kbdev->pm.lock);
+	mutex_lock(&kbdev->mmu_hw_mutex);
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
 	KBASE_DEBUG_ASSERT(kctx_as_nr == kctx->as_nr);
-	KBASE_DEBUG_ASSERT(atomic_read(&kctx->refcount) > 0);
+	KBASE_DEBUG_ASSERT(js_per_as_data->as_busy_refcount > 0);
 
 	/* Update refcount */
-	kbase_ctx_sched_release_ctx(kctx);
-	new_ref_count = atomic_read(&kctx->refcount);
+	new_ref_count = --(js_per_as_data->as_busy_refcount);
 
 	/* Release the atom if it finished (i.e. wasn't soft-stopped) */
 	if (kbasep_js_has_atom_finished(katom_retained_state))
@@ -1393,7 +1576,7 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_RELEASE_CTX, kctx, NULL, 0u,
 			new_ref_count);
 
-	if (new_ref_count == 2 && kbase_ctx_flag(kctx, KCTX_PRIVILEGED) &&
+	if (new_ref_count == 1 && kbase_ctx_flag(kctx, KCTX_PRIVILEGED) &&
 			!kbase_pm_is_suspending(kbdev)) {
 		/* Context is kept scheduled into an address space even when
 		 * there are no jobs, in this case we have to handle the
@@ -1406,10 +1589,8 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 		kbasep_js_set_submit_allowed(js_devdata, kctx);
 	}
 
-	/* Make a set of checks to see if the context should be scheduled out.
-	 * Note that there'll always be at least 1 reference to the context
-	 * which was previously acquired by kbasep_js_schedule_ctx(). */
-	if (new_ref_count == 1 &&
+	/* Make a set of checks to see if the context should be scheduled out */
+	if (new_ref_count == 0 &&
 		(!kbasep_js_is_submit_allowed(js_devdata, kctx) ||
 							kbdev->pm.suspending)) {
 		int num_slots = kbdev->gpu_props.num_job_slots;
@@ -1417,14 +1598,14 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 
 		/* Last reference, and we've been told to remove this context
 		 * from the Run Pool */
-		dev_dbg(kbdev->dev, "JS: RunPool Remove Context %p because refcount=%d, jobs=%d, allowed=%d",
+		dev_dbg(kbdev->dev, "JS: RunPool Remove Context %p because as_busy_refcount=%d, jobs=%d, allowed=%d",
 				kctx, new_ref_count, js_kctx_info->ctx.nr_jobs,
 				kbasep_js_is_submit_allowed(js_devdata, kctx));
 
 #if defined(CONFIG_MALI_GATOR_SUPPORT)
 		kbase_trace_mali_mmu_as_released(kctx->as_nr);
 #endif
-		KBASE_TLSTREAM_TL_NRET_AS_CTX(&kbdev->as[kctx->as_nr], kctx);
+		kbase_tlstream_tl_nret_as_ctx(&kbdev->as[kctx->as_nr], kctx);
 
 		kbase_backend_release_ctx_irq(kbdev, kctx);
 
@@ -1470,11 +1651,12 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 
 		kbase_backend_release_ctx_noirq(kbdev, kctx);
 
+		mutex_unlock(&kbdev->mmu_hw_mutex);
 		mutex_unlock(&kbdev->pm.lock);
 
 		/* Note: Don't reuse kctx_as_nr now */
 
-		/* Synchronize with any timers */
+		/* Synchronize with any policy timers */
 		kbase_backend_ctx_count_changed(kbdev);
 
 		/* update book-keeping info */
@@ -1492,6 +1674,7 @@ static kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(
 				katom_retained_state, runpool_ctx_attr_change);
 
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		mutex_unlock(&kbdev->mmu_hw_mutex);
 		mutex_unlock(&kbdev->pm.lock);
 	}
 
@@ -1514,13 +1697,17 @@ void kbasep_js_runpool_requeue_or_kill_ctx(struct kbase_device *kbdev,
 		struct kbase_context *kctx, bool has_pm_ref)
 {
 	struct kbasep_js_device_data *js_devdata;
+	union kbasep_js_policy *js_policy;
+	struct kbasep_js_kctx_info *js_kctx_info;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+	js_policy = &kbdev->js_data.policy;
 	js_devdata = &kbdev->js_data;
 
 	/* This is called if and only if you've you've detached the context from
-	 * the Runpool Queue, and not added it back to the Runpool
+	 * the Runpool or the Policy Queue, and not added it back to the Runpool
 	 */
 	KBASE_DEBUG_ASSERT(!kbase_ctx_flag(kctx, KCTX_SCHEDULED));
 
@@ -1628,35 +1815,19 @@ static bool kbasep_js_schedule_ctx(struct kbase_device *kbdev,
 {
 	struct kbasep_js_device_data *js_devdata;
 	struct kbasep_js_kctx_info *js_kctx_info;
+	union kbasep_js_policy *js_policy;
 	struct kbase_as *new_address_space = NULL;
 	unsigned long flags;
 	bool kctx_suspended = false;
 	int as_nr;
 
 	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
 	js_kctx_info = &kctx->jctx.sched_info;
 
 	/* Pick available address space for this context */
-	mutex_lock(&kbdev->mmu_hw_mutex);
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	as_nr = kbase_ctx_sched_retain_ctx(kctx);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-	mutex_unlock(&kbdev->mmu_hw_mutex);
-	if (as_nr == KBASEP_AS_NR_INVALID) {
-		as_nr = kbase_backend_find_and_release_free_address_space(
-				kbdev, kctx);
-		if (as_nr != KBASEP_AS_NR_INVALID) {
-			/* Attempt to retain the context again, this should
-			 * succeed */
-			mutex_lock(&kbdev->mmu_hw_mutex);
-			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-			as_nr = kbase_ctx_sched_retain_ctx(kctx);
-			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-			mutex_unlock(&kbdev->mmu_hw_mutex);
+	as_nr = kbase_backend_find_free_address_space(kbdev, kctx);
 
-			WARN_ON(as_nr == KBASEP_AS_NR_INVALID);
-		}
-	}
 	if (as_nr == KBASEP_AS_NR_INVALID)
 		return false; /* No address spaces currently available */
 
@@ -1667,16 +1838,12 @@ static bool kbasep_js_schedule_ctx(struct kbase_device *kbdev,
 	 */
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
 	mutex_lock(&js_devdata->runpool_mutex);
-	mutex_lock(&kbdev->mmu_hw_mutex);
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
 	/* Check to see if context is dying due to kbase_job_zap_context() */
 	if (kbase_ctx_flag(kctx, KCTX_DYING)) {
 		/* Roll back the transaction so far and return */
-		kbase_ctx_sched_release_ctx(kctx);
+		kbase_backend_release_free_address_space(kbdev, as_nr);
 
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-		mutex_unlock(&kbdev->mmu_hw_mutex);
 		mutex_unlock(&js_devdata->runpool_mutex);
 		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
 
@@ -1689,17 +1856,21 @@ static bool kbasep_js_schedule_ctx(struct kbase_device *kbdev,
 
 	kbase_ctx_flag_set(kctx, KCTX_SCHEDULED);
 
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
 	/* Assign context to previously chosen address space */
 	if (!kbase_backend_use_ctx(kbdev, kctx, as_nr)) {
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		mutex_unlock(&kbdev->mmu_hw_mutex);
 		/* Roll back the transaction so far and return */
-		kbase_ctx_sched_release_ctx(kctx);
 		kbase_ctx_flag_clear(kctx, KCTX_SCHEDULED);
 
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-		mutex_unlock(&kbdev->mmu_hw_mutex);
+		kbase_backend_release_free_address_space(kbdev, as_nr);
+
 		mutex_unlock(&js_devdata->runpool_mutex);
-		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
 
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
 		return false;
 	}
 
@@ -1708,7 +1879,7 @@ static bool kbasep_js_schedule_ctx(struct kbase_device *kbdev,
 #if defined(CONFIG_MALI_GATOR_SUPPORT)
 	kbase_trace_mali_mmu_as_in_use(kctx->as_nr);
 #endif
-	KBASE_TLSTREAM_TL_RET_AS_CTX(&kbdev->as[kctx->as_nr], kctx);
+	kbase_tlstream_tl_ret_as_ctx(&kbdev->as[kctx->as_nr], kctx);
 
 	/* Cause any future waiter-on-termination to wait until the context is
 	 * descheduled */
@@ -1740,7 +1911,7 @@ static bool kbasep_js_schedule_ctx(struct kbase_device *kbdev,
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	mutex_unlock(&kbdev->mmu_hw_mutex);
 
-	/* Synchronize with any timers */
+	/* Synchronize with any policy timers */
 	kbase_backend_ctx_count_changed(kbdev);
 
 	mutex_unlock(&js_devdata->runpool_mutex);
@@ -1766,8 +1937,7 @@ static bool kbase_js_use_ctx(struct kbase_device *kbdev,
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-	if (kbase_ctx_flag(kctx, KCTX_SCHEDULED) &&
-			kbase_backend_use_ctx_sched(kbdev, kctx)) {
+	if (kbase_backend_use_ctx_sched(kbdev, kctx)) {
 		/* Context already has ASID - mark as active */
 		kbdev->hwaccess.active_kctx = kctx;
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
@@ -1869,12 +2039,14 @@ void kbasep_js_suspend(struct kbase_device *kbdev)
 	/* Retain each of the contexts, so we can cause it to leave even if it
 	 * had no refcount to begin with */
 	for (i = BASE_MAX_NR_AS - 1; i >= 0; --i) {
-		struct kbase_context *kctx = kbdev->as_to_kctx[i];
+		struct kbasep_js_per_as_data *js_per_as_data =
+			&js_devdata->runpool_irq.per_as_data[i];
+		struct kbase_context *kctx = js_per_as_data->kctx;
 
 		retained = retained << 1;
 
 		if (kctx) {
-			kbase_ctx_sched_retain_ctx_refcount(kctx);
+			++(js_per_as_data->as_busy_refcount);
 			retained |= 1u;
 			/* We can only cope with up to 1 privileged context -
 			 * the instrumented context. It'll be suspended by
@@ -1894,7 +2066,9 @@ void kbasep_js_suspend(struct kbase_device *kbdev)
 	for (i = 0;
 		 i < BASE_MAX_NR_AS;
 		 ++i, retained = retained >> 1) {
-		struct kbase_context *kctx = kbdev->as_to_kctx[i];
+		struct kbasep_js_per_as_data *js_per_as_data =
+			&js_devdata->runpool_irq.per_as_data[i];
+		struct kbase_context *kctx = js_per_as_data->kctx;
 
 		if (retained & 1u)
 			kbasep_js_runpool_release_ctx(kbdev, kctx);
@@ -2101,26 +2275,22 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, int js)
 {
 	struct kbase_jd_atom *katom;
 	struct kbasep_js_device_data *js_devdata;
-	struct kbase_device *kbdev;
 	int pulled;
 
 	KBASE_DEBUG_ASSERT(kctx);
 
-	kbdev = kctx->kbdev;
-
-	js_devdata = &kbdev->js_data;
-	lockdep_assert_held(&kbdev->hwaccess_lock);
+	js_devdata = &kctx->kbdev->js_data;
+	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
 
 	if (!kbasep_js_is_submit_allowed(js_devdata, kctx))
 		return NULL;
-	if (kbase_pm_is_suspending(kbdev))
+	if (kbase_pm_is_suspending(kctx->kbdev))
 		return NULL;
 
 	katom = jsctx_rb_peek(kctx, js);
 	if (!katom)
 		return NULL;
-	if (kctx->blocked_js[js][katom->sched_priority])
-		return NULL;
+
 	if (atomic_read(&katom->blocked))
 		return NULL;
 
@@ -2129,7 +2299,7 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, int js)
 	 * present on the same slot */
 	if (katom->pre_dep && atomic_read(&kctx->atoms_pulled_slot[js])) {
 		struct kbase_jd_atom *prev_atom =
-				kbase_backend_inspect_tail(kbdev, js);
+				kbase_backend_inspect_tail(kctx->kbdev, js);
 
 		if (prev_atom && prev_atom->kctx != kctx)
 			return NULL;
@@ -2141,7 +2311,7 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, int js)
 					katom->x_pre_dep->will_fail_event_code)
 			return NULL;
 		if ((katom->atom_flags & KBASE_KATOM_FLAG_FAIL_BLOCKER) &&
-				kbase_backend_nr_atoms_on_slot(kbdev, js))
+				kbase_backend_nr_atoms_on_slot(kctx->kbdev, js))
 			return NULL;
 	}
 
@@ -2151,17 +2321,15 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, int js)
 	if (pulled == 1 && !kctx->slots_pullable) {
 		WARN_ON(kbase_ctx_flag(kctx, KCTX_RUNNABLE_REF));
 		kbase_ctx_flag_set(kctx, KCTX_RUNNABLE_REF);
-		atomic_inc(&kbdev->js_data.nr_contexts_runnable);
+		atomic_inc(&kctx->kbdev->js_data.nr_contexts_runnable);
 	}
 	atomic_inc(&kctx->atoms_pulled_slot[katom->slot_nr]);
-	kctx->atoms_pulled_slot_pri[katom->slot_nr][katom->sched_priority]++;
 	jsctx_rb_pull(kctx, katom);
 
-	kbasep_js_runpool_retain_ctx_nolock(kbdev, kctx);
-
+	kbasep_js_runpool_retain_ctx_nolock(kctx->kbdev, kctx);
 	katom->atom_flags |= KBASE_KATOM_FLAG_HOLDING_CTX_REF;
 
-	katom->ticks = 0;
+	katom->sched_info.cfs.ticks = 0;
 
 	return katom;
 }
@@ -2177,7 +2345,6 @@ static void js_return_worker(struct work_struct *data)
 	struct kbasep_js_kctx_info *js_kctx_info = &kctx->jctx.sched_info;
 	struct kbasep_js_atom_retained_state retained_state;
 	int js = katom->slot_nr;
-	int prio = katom->sched_priority;
 	bool timer_sync = false;
 	bool context_idle = false;
 	unsigned long flags;
@@ -2185,7 +2352,7 @@ static void js_return_worker(struct work_struct *data)
 	u64 affinity = katom->affinity;
 	enum kbase_atom_coreref_state coreref_state = katom->coreref_state;
 
-	KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_EX(katom);
+	kbase_tlstream_tl_event_atom_softstop_ex(katom);
 
 	kbase_backend_complete_wq(kbdev, katom);
 
@@ -2204,26 +2371,10 @@ static void js_return_worker(struct work_struct *data)
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-	kctx->atoms_pulled_slot_pri[js][katom->sched_priority]--;
-
 	if (!atomic_read(&kctx->atoms_pulled_slot[js]) &&
 			jsctx_rb_none_to_pull(kctx, js))
 		timer_sync |= kbase_js_ctx_list_remove_nolock(kbdev, kctx, js);
 
-	/* If this slot has been blocked due to soft-stopped atoms, and all
-	 * atoms have now been processed, then unblock the slot */
-	if (!kctx->atoms_pulled_slot_pri[js][prio] &&
-			kctx->blocked_js[js][prio]) {
-		kctx->blocked_js[js][prio] = false;
-
-		/* Only mark the slot as pullable if the context is not idle -
-		 * that case is handled below */
-		if (atomic_read(&kctx->atoms_pulled) &&
-				kbase_js_ctx_pullable(kctx, js, true))
-			timer_sync |= kbase_js_ctx_list_add_pullable_nolock(
-					kbdev, kctx, js);
-	}
-
 	if (!atomic_read(&kctx->atoms_pulled)) {
 		if (!kctx->slots_pullable) {
 			WARN_ON(!kbase_ctx_flag(kctx, KCTX_RUNNABLE_REF));
@@ -2305,7 +2456,6 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx,
 	bool timer_sync = false;
 	int atom_slot;
 	bool context_idle = false;
-	int prio = katom->sched_priority;
 
 	kbdev = kctx->kbdev;
 	atom_slot = katom->slot_nr;
@@ -2321,7 +2471,6 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx,
 	if (katom->atom_flags & KBASE_KATOM_FLAG_JSCTX_IN_TREE) {
 		context_idle = !atomic_dec_return(&kctx->atoms_pulled);
 		atomic_dec(&kctx->atoms_pulled_slot[atom_slot]);
-		kctx->atoms_pulled_slot_pri[atom_slot][prio]--;
 
 		if (!atomic_read(&kctx->atoms_pulled) &&
 				!kctx->slots_pullable) {
@@ -2330,17 +2479,6 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx,
 			atomic_dec(&kbdev->js_data.nr_contexts_runnable);
 			timer_sync = true;
 		}
-
-		/* If this slot has been blocked due to soft-stopped atoms, and
-		 * all atoms have now been processed, then unblock the slot */
-		if (!kctx->atoms_pulled_slot_pri[atom_slot][prio]
-				&& kctx->blocked_js[atom_slot][prio]) {
-			kctx->blocked_js[atom_slot][prio] = false;
-			if (kbase_js_ctx_pullable(kctx, atom_slot, true))
-				timer_sync |=
-					kbase_js_ctx_list_add_pullable_nolock(
-						kbdev, kctx, atom_slot);
-		}
 	}
 	WARN_ON(!(katom->atom_flags & KBASE_KATOM_FLAG_JSCTX_IN_TREE));
 
@@ -2402,10 +2540,12 @@ struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom,
 	u64 microseconds_spent = 0;
 	struct kbase_device *kbdev;
 	struct kbase_context *kctx = katom->kctx;
+	union kbasep_js_policy *js_policy;
 	struct kbase_jd_atom *x_dep = katom->x_post_dep;
 
 	kbdev = kctx->kbdev;
 
+	js_policy = &kbdev->js_data.policy;
 
 	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
 
@@ -2441,6 +2581,8 @@ struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom,
 			microseconds_spent = KBASEP_JS_TICK_RESOLUTION_US;
 	}
 
+	/* Log the result of the job (completion status, and time spent). */
+	kbasep_js_policy_log_job_result(js_policy, katom, microseconds_spent);
 
 	kbase_jd_done(katom, katom->slot_nr, end_timestamp, 0);
 
@@ -2467,17 +2609,13 @@ struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom,
 void kbase_js_sched(struct kbase_device *kbdev, int js_mask)
 {
 	struct kbasep_js_device_data *js_devdata;
-	struct kbase_context *last_active;
 	bool timer_sync = false;
-	bool ctx_waiting = false;
 
 	js_devdata = &kbdev->js_data;
 
 	down(&js_devdata->schedule_sem);
 	mutex_lock(&js_devdata->queue_mutex);
 
-	last_active = kbdev->hwaccess.active_kctx;
-
 	while (js_mask) {
 		int js;
 
@@ -2555,40 +2693,18 @@ void kbase_js_sched(struct kbase_device *kbdev, int js_mask)
 				js_mask &= ~(1 << js);
 
 			if (!kbase_ctx_flag(kctx, KCTX_PULLED)) {
-				bool pullable = kbase_js_ctx_pullable(kctx, js,
-						true);
-
-				/* Failed to pull jobs - push to head of list.
-				 * Unless this context is already 'active', in
-				 * which case it's effectively already scheduled
-				 * so push it to the back of the list. */
-				if (pullable && kctx == last_active)
-					timer_sync |=
-					kbase_js_ctx_list_add_pullable_nolock(
-							kctx->kbdev,
-							kctx, js);
-				else if (pullable)
+				/* Failed to pull jobs - push to head of list */
+				if (kbase_js_ctx_pullable(kctx, js, true))
 					timer_sync |=
 					kbase_js_ctx_list_add_pullable_head_nolock(
-							kctx->kbdev,
-							kctx, js);
+								kctx->kbdev,
+								kctx, js);
 				else
 					timer_sync |=
 					kbase_js_ctx_list_add_unpullable_nolock(
 								kctx->kbdev,
 								kctx, js);
 
-				/* If this context is not the active context,
-				 * but the active context is pullable on this
-				 * slot, then we need to remove the active
-				 * marker to prevent it from submitting atoms in
-				 * the IRQ handler, which would prevent this
-				 * context from making progress. */
-				if (last_active && kctx != last_active &&
-						kbase_js_ctx_pullable(
-						last_active, js, true))
-					ctx_waiting = true;
-
 				if (context_idle) {
 					kbase_jm_idle_ctx(kbdev, kctx);
 					spin_unlock_irqrestore(
@@ -2627,9 +2743,6 @@ void kbase_js_sched(struct kbase_device *kbdev, int js_mask)
 	if (timer_sync)
 		kbase_js_sync_timers(kbdev);
 
-	if (kbdev->hwaccess.active_kctx == last_active && ctx_waiting)
-		kbdev->hwaccess.active_kctx = NULL;
-
 	mutex_unlock(&js_devdata->queue_mutex);
 	up(&js_devdata->schedule_sem);
 }
@@ -2649,7 +2762,7 @@ void kbase_js_zap_context(struct kbase_context *kctx)
 
 	/* First, atomically do the following:
 	 * - mark the context as dying
-	 * - try to evict it from the queue */
+	 * - try to evict it from the policy queue */
 	mutex_lock(&kctx->jctx.lock);
 	mutex_lock(&js_devdata->queue_mutex);
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
@@ -2659,15 +2772,15 @@ void kbase_js_zap_context(struct kbase_context *kctx)
 
 	/*
 	 * At this point we know:
-	 * - If eviction succeeded, it was in the queue, but now no
+	 * - If eviction succeeded, it was in the policy queue, but now no
 	 *   longer is
 	 *  - We must cancel the jobs here. No Power Manager active reference to
 	 *    release.
 	 *  - This happens asynchronously - kbase_jd_zap_context() will wait for
 	 *    those jobs to be killed.
-	 * - If eviction failed, then it wasn't in the queue. It is one
+	 * - If eviction failed, then it wasn't in the policy queue. It is one
 	 *   of the following:
-	 *  - a. it didn't have any jobs, and so is not in the Queue or
+	 *  - a. it didn't have any jobs, and so is not in the Policy Queue or
 	 *       the Run Pool (not scheduled)
 	 *   - Hence, no more work required to cancel jobs. No Power Manager
 	 *     active reference to release.
@@ -2687,7 +2800,7 @@ void kbase_js_zap_context(struct kbase_context *kctx)
 	 * submitting any more jobs. When it finally does leave,
 	 * kbasep_js_runpool_requeue_or_kill_ctx() will kill all remaining jobs
 	 * (because it is dying), release the Power Manager active reference,
-	 * and will not requeue the context in the queue.
+	 * and will not requeue the context in the policy queue.
 	 * kbase_jd_zap_context() will wait for those jobs to be killed.
 	 *  - Hence, work required just to make it leave the runpool. Cancelling
 	 *    jobs and releasing the Power manager active reference will be
@@ -2704,7 +2817,7 @@ void kbase_js_zap_context(struct kbase_context *kctx)
 		/* The following events require us to kill off remaining jobs
 		 * and update PM book-keeping:
 		 * - we evicted it correctly (it must have jobs to be in the
-		 *   Queue)
+		 *   Policy Queue)
 		 *
 		 * These events need no action, but take this path anyway:
 		 * - Case a: it didn't have any jobs, and was never in the Queue
@@ -2717,7 +2830,7 @@ void kbase_js_zap_context(struct kbase_context *kctx)
 
 		dev_dbg(kbdev->dev, "Zap: Ctx %p scheduled=0", kctx);
 
-		/* Only cancel jobs when we evicted from the
+		/* Only cancel jobs when we evicted from the policy
 		 * queue. No Power Manager active reference was held.
 		 *
 		 * Having is_dying set ensures that this kills, and
@@ -2784,7 +2897,22 @@ void kbase_js_zap_context(struct kbase_context *kctx)
 static inline int trace_get_refcnt(struct kbase_device *kbdev,
 					struct kbase_context *kctx)
 {
-	return atomic_read(&kctx->refcount);
+	struct kbasep_js_device_data *js_devdata;
+	int as_nr;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		struct kbasep_js_per_as_data *js_per_as_data;
+
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		refcnt = js_per_as_data->as_busy_refcount;
+	}
+
+	return refcnt;
 }
 
 /**
@@ -2806,7 +2934,7 @@ static inline int trace_get_refcnt(struct kbase_device *kbdev,
  * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
  */
 static void kbase_js_foreach_ctx_job(struct kbase_context *kctx,
-		kbasep_js_ctx_job_cb callback)
+		kbasep_js_policy_ctx_job_cb callback)
 {
 	struct kbase_device *kbdev;
 	struct kbasep_js_device_data *js_devdata;
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js.h b/drivers/gpu/arm/midgard/mali_kbase_js.h
index ddada8e..8969222 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_js.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_js.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,6 +26,7 @@
 #define _KBASE_JS_H_
 
 #include "mali_kbase_js_defs.h"
+#include "mali_kbase_js_policy.h"
 #include "mali_kbase_context.h"
 #include "mali_kbase_defs.h"
 #include "mali_kbase_debug.h"
@@ -46,7 +47,8 @@
  * @addtogroup kbase_js Job Scheduler Internal APIs
  * @{
  *
- * These APIs are Internal to KBase.
+ * These APIs are Internal to KBase and are available for use by the
+ * @ref kbase_js_policy "Job Scheduler Policy APIs"
  */
 
 /**
@@ -180,7 +182,9 @@ bool kbasep_js_add_job(struct kbase_context *kctx, struct kbase_jd_atom *atom);
  * It is a programming error to call this when:
  * - \a atom is not a job belonging to kctx.
  * - \a atom has already been removed from the Job Scheduler.
- * - \a atom is still in the runpool
+ * - \a atom is still in the runpool:
+ *  - it has not been removed with kbasep_js_policy_dequeue_job()
+ *  - or, it has not been removed with kbasep_js_policy_dequeue_job_irq()
  *
  * Do not use this for removing jobs being killed by kbase_jd_cancel() - use
  * kbasep_js_remove_cancelled_job() instead.
@@ -204,6 +208,8 @@ void kbasep_js_remove_job(struct kbase_device *kbdev, struct kbase_context *kctx
  * - \a atom has already been removed from the Job Scheduler.
  * - \a atom is still in the runpool:
  *  - it is not being killed with kbasep_jd_cancel()
+ *  - or, it has not been removed with kbasep_js_policy_dequeue_job()
+ *  - or, it has not been removed with kbasep_js_policy_dequeue_job_irq()
  *
  * The following locking conditions are made on the caller:
  * - it must hold kbasep_js_kctx_info::ctx::jsctx_mutex.
@@ -227,8 +233,7 @@ bool kbasep_js_remove_cancelled_job(struct kbase_device *kbdev,
  * @note This function can safely be called from IRQ context.
  *
  * The following locking conditions are made on the caller:
- * - it must \em not hold mmu_hw_mutex and hwaccess_lock, because they will be
- *   used internally.
+ * - it must \em not hold the hwaccess_lock, because it will be used internally.
  *
  * @return value != false if the retain succeeded, and the context will not be scheduled out.
  * @return false if the retain failed (because the context is being/has been scheduled out).
@@ -242,7 +247,7 @@ bool kbasep_js_runpool_retain_ctx(struct kbase_device *kbdev, struct kbase_conte
  * @note This function can safely be called from IRQ context.
  *
  * The following locks must be held by the caller:
- * - mmu_hw_mutex, hwaccess_lock
+ * - hwaccess_lock
  *
  * @return value != false if the retain succeeded, and the context will not be scheduled out.
  * @return false if the retain failed (because the context is being/has been scheduled out).
@@ -270,6 +275,28 @@ bool kbasep_js_runpool_retain_ctx_nolock(struct kbase_device *kbdev, struct kbas
 struct kbase_context *kbasep_js_runpool_lookup_ctx(struct kbase_device *kbdev, int as_nr);
 
 /**
+ * kbasep_js_runpool_lookup_ctx_nolock - Lookup a context in the Run Pool based
+ *         upon its current address space and ensure that is stays scheduled in.
+ * @kbdev: Device pointer
+ * @as_nr: Address space to lookup
+ *
+ * The context is refcounted as being busy to prevent it from scheduling
+ * out. It must be released with kbasep_js_runpool_release_ctx() when it is no
+ * longer required to stay scheduled in.
+ *
+ * Note: This function can safely be called from IRQ context.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must the hold the hwaccess_lock
+ *
+ * Return: a valid struct kbase_context on success, which has been refcounted as
+ *         being busy.
+ *         NULL on failure, indicating that no context was found in \a as_nr
+ */
+struct kbase_context *kbasep_js_runpool_lookup_ctx_nolock(
+		struct kbase_device *kbdev, int as_nr);
+
+/**
  * @brief Handling the requeuing/killing of a context that was evicted from the
  * policy queue or runpool.
  *
@@ -437,8 +464,9 @@ void kbase_js_try_run_jobs(struct kbase_device *kbdev);
  *
  * The emptying mechanism may take some time to complete, since it can wait for
  * jobs to complete naturally instead of forcing them to end quickly. However,
- * this is bounded by the Job Scheduler's Job Timeouts. Hence, this
- * function is guaranteed to complete in a finite time.
+ * this is bounded by the Job Scheduling Policy's Job Timeouts. Hence, this
+ * function is guaranteed to complete in a finite time whenever the Job
+ * Scheduling Policy implements Job Timeouts (such as those done by CFS).
  */
 void kbasep_js_suspend(struct kbase_device *kbdev);
 
@@ -761,9 +789,41 @@ static inline bool kbasep_js_get_atom_retry_submit_slot(const struct kbasep_js_a
 	return (bool) (js >= 0);
 }
 
+#if KBASE_DEBUG_DISABLE_ASSERTS == 0
+/**
+ * Debug Check the refcount of a context. Only use within ASSERTs
+ *
+ * Obtains hwaccess_lock
+ *
+ * @return negative value if the context is not scheduled in
+ * @return current refcount of the context if it is scheduled in. The refcount
+ * is not guarenteed to be kept constant.
+ */
+static inline int kbasep_js_debug_check_ctx_refcount(struct kbase_device *kbdev, struct kbase_context *kctx)
+{
+	unsigned long flags;
+	struct kbasep_js_device_data *js_devdata;
+	int result = -1;
+	int as_nr;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID)
+		result = js_devdata->runpool_irq.per_as_data[as_nr].as_busy_refcount;
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return result;
+}
+#endif				/* KBASE_DEBUG_DISABLE_ASSERTS == 0 */
+
 /**
  * @brief Variant of kbasep_js_runpool_lookup_ctx() that can be used when the
- * context is guaranteed to be already previously retained.
+ * context is guarenteed to be already previously retained.
  *
  * It is a programming error to supply the \a as_nr of a context that has not
  * been previously retained/has a busy refcount of zero. The only exception is
@@ -772,24 +832,97 @@ static inline bool kbasep_js_get_atom_retry_submit_slot(const struct kbasep_js_a
  * The following locking conditions are made on the caller:
  * - it must \em not hold the hwaccess_lock, because it will be used internally.
  *
- * @return a valid struct kbase_context on success, with a refcount that is guaranteed
+ * @return a valid struct kbase_context on success, with a refcount that is guarenteed
  * to be non-zero and unmodified by this function.
  * @return NULL on failure, indicating that no context was found in \a as_nr
  */
 static inline struct kbase_context *kbasep_js_runpool_lookup_ctx_noretain(struct kbase_device *kbdev, int as_nr)
 {
+	unsigned long flags;
+	struct kbasep_js_device_data *js_devdata;
 	struct kbase_context *found_kctx;
+	struct kbasep_js_per_as_data *js_per_as_data;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(0 <= as_nr && as_nr < BASE_MAX_NR_AS);
+	js_devdata = &kbdev->js_data;
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-	found_kctx = kbdev->as_to_kctx[as_nr];
-	KBASE_DEBUG_ASSERT(found_kctx == NULL ||
-			atomic_read(&found_kctx->refcount) > 0);
+	found_kctx = js_per_as_data->kctx;
+	KBASE_DEBUG_ASSERT(found_kctx == NULL || js_per_as_data->as_busy_refcount > 0);
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	return found_kctx;
 }
 
+/**
+ * This will provide a conversion from time (us) to ticks of the gpu clock
+ * based on the minimum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: when you need the number of cycles to guarantee you won't wait for
+ * longer than 'us' time (you might have a shorter wait).
+ */
+static inline u32 kbasep_js_convert_us_to_gpu_ticks_min_freq(struct kbase_device *kbdev, u32 us)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_min;
+
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return us * (gpu_freq / 1000);
+}
+
+/**
+ * This will provide a conversion from time (us) to ticks of the gpu clock
+ * based on the maximum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: When you need the number of cycles to guarantee you'll wait at least
+ * 'us' amount of time (but you might wait longer).
+ */
+static inline u32 kbasep_js_convert_us_to_gpu_ticks_max_freq(struct kbase_device *kbdev, u32 us)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_max;
+
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return us * (u32) (gpu_freq / 1000);
+}
+
+/**
+ * This will provide a conversion from ticks of the gpu clock to time (us)
+ * based on the minimum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: When you need to know the worst-case wait that 'ticks' cycles will
+ * take (you guarantee that you won't wait any longer than this, but it may
+ * be shorter).
+ */
+static inline u32 kbasep_js_convert_gpu_ticks_to_us_min_freq(struct kbase_device *kbdev, u32 ticks)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_min;
+
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return ticks / gpu_freq * 1000;
+}
+
+/**
+ * This will provide a conversion from ticks of the gpu clock to time (us)
+ * based on the maximum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: When you need to know the best-case wait for 'tick' cycles (you
+ * guarantee to be waiting for at least this long, but it may be longer).
+ */
+static inline u32 kbasep_js_convert_gpu_ticks_to_us_max_freq(struct kbase_device *kbdev, u32 ticks)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_max;
+
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return ticks / gpu_freq * 1000;
+}
+
 /*
  * The following locking conditions are made on the caller:
  * - The caller must hold the kbasep_js_kctx_info::ctx::jsctx_mutex.
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js_ctx_attr.c b/drivers/gpu/arm/midgard/mali_kbase_js_ctx_attr.c
index 321506a..455b661 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_js_ctx_attr.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_js_ctx_attr.c
@@ -195,10 +195,12 @@ static bool kbasep_js_ctx_attr_ctx_release_attr(struct kbase_device *kbdev, stru
 
 void kbasep_js_ctx_attr_set_initial_attrs(struct kbase_device *kbdev, struct kbase_context *kctx)
 {
+	struct kbasep_js_kctx_info *js_kctx_info;
 	bool runpool_state_changed = false;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
 
 	if (kbase_ctx_flag(kctx, KCTX_SUBMIT_DISABLED)) {
 		/* This context never submits, so don't track any scheduling attributes */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js_defs.h b/drivers/gpu/arm/midgard/mali_kbase_js_defs.h
index ba8b644..e6a9d41 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_js_defs.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_js_defs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -52,9 +52,27 @@ struct kbasep_atom_req {
 	u32 device_nr;
 };
 
+#include "mali_kbase_js_policy_cfs.h"
+
+/* Wrapper Interface - doxygen is elsewhere */
+union kbasep_js_policy {
+	struct kbasep_js_policy_cfs cfs;
+};
+
+/* Wrapper Interface - doxygen is elsewhere */
+union kbasep_js_policy_ctx_info {
+	struct kbasep_js_policy_cfs_ctx cfs;
+};
+
+/* Wrapper Interface - doxygen is elsewhere */
+union kbasep_js_policy_job_info {
+	struct kbasep_js_policy_cfs_job cfs;
+};
+
+
 /** Callback function run on all of a context's jobs registered with the Job
  * Scheduler */
-typedef void (*kbasep_js_ctx_job_cb)(struct kbase_device *kbdev, struct kbase_jd_atom *katom);
+typedef void (*kbasep_js_policy_ctx_job_cb)(struct kbase_device *kbdev, struct kbase_jd_atom *katom);
 
 /**
  * @brief Maximum number of jobs that can be submitted to a job slot whilst
@@ -67,6 +85,16 @@ typedef void (*kbasep_js_ctx_job_cb)(struct kbase_device *kbdev, struct kbase_jd
 #define KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ 2
 
 /**
+ * @brief the IRQ_THROTTLE time in microseconds
+ *
+ * This will be converted via the GPU's clock frequency into a cycle-count.
+ *
+ * @note we can make an estimate of the GPU's frequency by periodically
+ * sampling its CYCLE_COUNT register
+ */
+#define KBASE_JS_IRQ_THROTTLE_TIME_US 20
+
+/**
  * @brief Context attributes
  *
  * Each context attribute can be thought of as a boolean value that caches some
@@ -96,7 +124,9 @@ enum kbasep_js_ctx_attr {
 
 	/** Attribute indicating a context that contains Non-Compute jobs. That is,
 	 * the context has some jobs that are \b not of type @ref
-	 * BASE_JD_REQ_ONLY_COMPUTE.
+	 * BASE_JD_REQ_ONLY_COMPUTE. The context usually has
+	 * BASE_CONTEXT_HINT_COMPUTE \b clear, but this depends on the HW
+	 * workarounds in use in the Job Scheduling Policy.
 	 *
 	 * @note A context can be both 'Compute' and 'Non Compute' if it contains
 	 * both types of jobs.
@@ -142,6 +172,26 @@ enum {
 typedef u32 kbasep_js_atom_done_code;
 
 /**
+ * Data used by the scheduler that is unique for each Address Space.
+ *
+ * This is used in IRQ context and hwaccess_lock must be held whilst accessing
+ * this data (inculding reads and atomic decisions based on the read).
+ */
+struct kbasep_js_per_as_data {
+	/**
+	 * Ref count of whether this AS is busy, and must not be scheduled out
+	 *
+	 * When jobs are running this is always positive. However, it can still be
+	 * positive when no jobs are running. If all you need is a heuristic to
+	 * tell you whether jobs might be running, this should be sufficient.
+	 */
+	int as_busy_refcount;
+
+	/** Pointer to the current context on this address space, or NULL for no context */
+	struct kbase_context *kctx;
+};
+
+/**
  * @brief KBase Device Data Job Scheduler sub-structure
  *
  * This encapsulates the current context of the Job Scheduler on a particular
@@ -161,8 +211,10 @@ struct kbasep_js_device_data {
 	struct runpool_irq {
 		/** Bitvector indicating whether a currently scheduled context is allowed to submit jobs.
 		 * When bit 'N' is set in this, it indicates whether the context bound to address space
-		 * 'N' is allowed to submit jobs.
-		 */
+		 * 'N' (per_as_data[N].kctx) is allowed to submit jobs.
+		 *
+		 * It is placed here because it's much more memory efficient than having a u8 in
+		 * struct kbasep_js_per_as_data to store this flag  */
 		u16 submit_allowed;
 
 		/** Context Attributes:
@@ -181,6 +233,9 @@ struct kbasep_js_device_data {
 		 * is required on updating the variable) */
 		s8 ctx_attr_ref_count[KBASEP_JS_CTX_ATTR_COUNT];
 
+		/** Data that is unique for each AS */
+		struct kbasep_js_per_as_data per_as_data[BASE_MAX_NR_AS];
+
 		/*
 		 * Affinity management and tracking
 		 */
@@ -231,11 +286,21 @@ struct kbasep_js_device_data {
 	 */
 	struct list_head ctx_list_unpullable[BASE_JM_MAX_NR_SLOTS];
 
+	u16 as_free;				/**< Bitpattern of free Address Spaces */
+
 	/** Number of currently scheduled user contexts (excluding ones that are not submitting jobs) */
 	s8 nr_user_contexts_running;
 	/** Number of currently scheduled contexts (including ones that are not submitting jobs) */
 	s8 nr_all_contexts_running;
 
+	/**
+	 * Policy-specific information.
+	 *
+	 * Refer to the structure defined by the current policy to determine which
+	 * locks must be held when accessing this.
+	 */
+	union kbasep_js_policy policy;
+
 	/** Core Requirements to match up with base_js_atom's core_req memeber
 	 * @note This is a write-once member, and so no locking is required to read */
 	base_jd_core_req js_reqs[BASE_JM_MAX_NR_SLOTS];
@@ -250,6 +315,8 @@ struct kbasep_js_device_data {
 	u32 gpu_reset_ticks_cl;	     /*< Value for JS_RESET_TICKS_CL */
 	u32 gpu_reset_ticks_dumping; /*< Value for JS_RESET_TICKS_DUMPING */
 	u32 ctx_timeslice_ns;		 /**< Value for JS_CTX_TIMESLICE_NS */
+	u32 cfs_ctx_runtime_init_slices; /**< Value for DEFAULT_JS_CFS_CTX_RUNTIME_INIT_SLICES */
+	u32 cfs_ctx_runtime_min_slices;	 /**< Value for  DEFAULT_JS_CFS_CTX_RUNTIME_MIN_SLICES */
 
 	/**< Value for JS_SOFT_JOB_TIMEOUT */
 	atomic_t soft_job_timeout_ms;
@@ -282,6 +349,19 @@ struct kbasep_js_device_data {
  * scheduling information.
  */
 struct kbasep_js_kctx_info {
+	/**
+	 * Runpool substructure. This must only be accessed whilst the Run Pool
+	 * mutex ( kbasep_js_device_data::runpool_mutex ) is held.
+	 *
+	 * In addition, the hwaccess_lock may need to be held for certain
+	 * sub-members.
+	 *
+	 * @note some of the members could be moved into struct kbasep_js_device_data for
+	 * improved d-cache/tlb efficiency.
+	 */
+	struct {
+		union kbasep_js_policy_ctx_info policy_ctx;	/**< Policy-specific context */
+	} runpool;
 
 	/**
 	 * Job Scheduler Context information sub-structure. These members are
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js_policy.h b/drivers/gpu/arm/midgard/mali_kbase_js_policy.h
new file mode 100644
index 0000000..d1f3a0a
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_js_policy.h
@@ -0,0 +1,763 @@
+/*
+ *
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js_policy.h
+ * Job Scheduler Policy APIs.
+ */
+
+#ifndef _KBASE_JS_POLICY_H_
+#define _KBASE_JS_POLICY_H_
+
+/**
+ * @page page_kbase_js_policy Job Scheduling Policies
+ * The Job Scheduling system is described in the following:
+ * - @subpage page_kbase_js_policy_overview
+ * - @subpage page_kbase_js_policy_operation
+ *
+ * The API details are as follows:
+ * - @ref kbase_jm
+ * - @ref kbase_js
+ * - @ref kbase_js_policy
+ */
+
+/**
+ * @page page_kbase_js_policy_overview Overview of the Policy System
+ *
+ * The Job Scheduler Policy manages:
+ * - The assigning of KBase Contexts to GPU Address Spaces (\em ASs)
+ * - The choosing of Job Chains (\em Jobs) from a KBase context, to run on the
+ * GPU's Job Slots (\em JSs).
+ * - The amount of \em time a context is assigned to (<em>scheduled on</em>) an
+ * Address Space
+ * - The amount of \em time a Job spends running on the GPU
+ *
+ * The Policy implements this management via 2 components:
+ * - A Policy Queue, which manages a set of contexts that are ready to run,
+ * but not currently running.
+ * - A Policy Run Pool, which manages the currently running contexts (one per Address
+ * Space) and the jobs to run on the Job Slots.
+ *
+ * Each Graphics Process in the system has at least one KBase Context. Therefore,
+ * the Policy Queue can be seen as a queue of Processes waiting to run Jobs on
+ * the GPU.
+ *
+ * <!-- The following needs to be all on one line, due to doxygen's parser -->
+ * @dotfile policy_overview.dot "Diagram showing a very simplified overview of the Policy System. IRQ handling, soft/hard-stopping, contexts re-entering the system and Policy details are omitted"
+ *
+ * The main operations on the queue are:
+ * - Enqueuing a Context to it
+ * - Dequeuing a Context from it, to run it.
+ * - Note: requeuing a context is much the same as enqueuing a context, but
+ * occurs when a context is scheduled out of the system to allow other contexts
+ * to run.
+ *
+ * These operations have much the same meaning for the Run Pool - Jobs are
+ * dequeued to run on a Jobslot, and requeued when they are scheduled out of
+ * the GPU.
+ *
+ * @note This is an over-simplification of the Policy APIs - there are more
+ * operations than 'Enqueue'/'Dequeue', and a Dequeue from the Policy Queue
+ * takes at least two function calls: one to Dequeue from the Queue, one to add
+ * to the Run Pool.
+ *
+ * As indicated on the diagram, Jobs permanently leave the scheduling system
+ * when they are completed, otherwise they get dequeued/requeued until this
+ * happens. Similarly, Contexts leave the scheduling system when their jobs
+ * have all completed. However, Contexts may later return to the scheduling
+ * system (not shown on the diagram) if more Bags of Jobs are submitted to
+ * them.
+ */
+
+/**
+ * @page page_kbase_js_policy_operation Policy Operation
+ *
+ * We describe the actions that the Job Scheduler Core takes on the Policy in
+ * the following cases:
+ * - The IRQ Path
+ * - The Job Submission Path
+ * - The High Priority Job Submission Path
+ *
+ * This shows how the Policy APIs will be used by the Job Scheduler core.
+ *
+ * The following diagram shows an example Policy that contains a Low Priority
+ * queue, and a Real-time (High Priority) Queue. The RT queue is examined
+ * before the LowP one on dequeuing from the head. The Low Priority Queue is
+ * ordered by time, and the RT queue is ordered by time weighted by
+ * RT-priority. In addition, it shows that the Job Scheduler Core will start a
+ * Soft-Stop Timer (SS-Timer) when it dequeue's and submits a job. The
+ * Soft-Stop time is set by a global configuration value, and must be a value
+ * appropriate for the policy. For example, this could include "don't run a
+ * soft-stop timer" for a First-Come-First-Served (FCFS) policy.
+ *
+ * <!-- The following needs to be all on one line, due to doxygen's parser -->
+ * @dotfile policy_operation_diagram.dot "Diagram showing the objects managed by an Example Policy, and the operations made upon these objects by the Job Scheduler Core."
+ *
+ * @section sec_kbase_js_policy_operation_prio Dealing with Priority
+ *
+ * Priority applies separately to a context as a whole, and to the jobs within
+ * a context. The jobs specify a priority in the base_jd_atom::prio member, but
+ * it is independent of the context priority. That is, it only affects
+ * scheduling of atoms within a context. Refer to @ref base_jd_prio for more
+ * details. The meaning of the context's priority value is up to the policy
+ * itself, and could be a logarithmic scale instead of a linear scale (e.g. the
+ * policy could implement an increase/decrease in priority by 1 results in an
+ * increase/decrease in \em proportion of time spent scheduled in by 25%, an
+ * effective change in timeslice by 11%).
+ *
+ * It is up to the policy whether a boost in priority boosts the priority of
+ * the entire context (e.g. to such an extent where it may pre-empt other
+ * running contexts). If it chooses to do this, the Policy must make sure that
+ * only jobs from high-priority contexts are run, and that the context is
+ * scheduled out once only jobs from low priority contexts remain. This ensures
+ * that the low priority contexts do not gain from the priority boost, yet they
+ * still get scheduled correctly with respect to other low priority contexts.
+ *
+ *
+ * @section sec_kbase_js_policy_operation_irq IRQ Path
+ *
+ * The following happens on the IRQ path from the Job Scheduler Core:
+ * - Note the slot that completed (for later)
+ * - Log the time spent by the job (and implicitly, the time spent by the
+ * context)
+ *  - call kbasep_js_policy_log_job_result() <em>in the context of the irq
+ * handler.</em>
+ *  - This must happen regardless of whether the job completed successfully or
+ * not (otherwise the context gets away with DoS'ing the system with faulty jobs)
+ * - What was the result of the job?
+ *  - If Completed: job is just removed from the system
+ *  - If Hard-stop or failure: job is removed from the system
+ *  - If Soft-stop: queue the book-keeping work onto a work-queue: have a
+ * work-queue call kbasep_js_policy_enqueue_job()
+ * - Check the timeslice used by the owning context
+ *  - call kbasep_js_policy_should_remove_ctx() <em>in the context of the irq
+ * handler.</em>
+ *  - If this returns true, clear the "allowed" flag.
+ * - Check the ctx's flags for "allowed", "has jobs to run" and "is running
+ * jobs"
+ * - And so, should the context stay scheduled in?
+ *  - If No, push onto a work-queue the work of scheduling out the old context,
+ * and getting a new one. That is:
+ *   - kbasep_js_policy_runpool_remove_ctx() on old_ctx
+ *   - kbasep_js_policy_enqueue_ctx() on old_ctx
+ *   - kbasep_js_policy_dequeue_head_ctx() to get new_ctx
+ *   - kbasep_js_policy_runpool_add_ctx() on new_ctx
+ *   - (all of this work is deferred on a work-queue to keep the IRQ handler quick)
+ * - If there is space in the completed job slots' HEAD/NEXT registers, run the next job:
+ *  - kbasep_js_policy_dequeue_job() <em>in the context of the irq
+ * handler</em> with core_req set to that of the completing slot
+ *  - if this returned true, submit the job to the completed slot.
+ *  - This is repeated until kbasep_js_policy_dequeue_job() returns
+ * false, or the job slot has a job queued on both the HEAD and NEXT registers.
+ *  - If kbasep_js_policy_dequeue_job() returned false, submit some work to
+ * the work-queue to retry from outside of IRQ context (calling
+ * kbasep_js_policy_dequeue_job() from a work-queue).
+ *
+ * Since the IRQ handler submits new jobs \em and re-checks the IRQ_RAWSTAT,
+ * this sequence could loop a large number of times: this could happen if
+ * the jobs submitted completed on the GPU very quickly (in a few cycles), such
+ * as GPU NULL jobs. Then, the HEAD/NEXT registers will always be free to take
+ * more jobs, causing us to loop until we run out of jobs.
+ *
+ * To mitigate this, we must limit the number of jobs submitted per slot during
+ * the IRQ handler - for example, no more than 2 jobs per slot per IRQ should
+ * be sufficient (to fill up the HEAD + NEXT registers in normal cases). For
+ * Mali-T600 with 3 job slots, this means that up to 6 jobs could be submitted per
+ * slot. Note that IRQ Throttling can make this situation commonplace: 6 jobs
+ * could complete but the IRQ for each of them is delayed by the throttling. By
+ * the time you get the IRQ, all 6 jobs could've completed, meaning you can
+ * submit jobs to fill all 6 HEAD+NEXT registers again.
+ *
+ * @note As much work is deferred as possible, which includes the scheduling
+ * out of a context and scheduling in a new context. However, we can still make
+ * starting a single high-priorty context quick despite this:
+ * - On Mali-T600 family, there is one more AS than JSs.
+ * - This means we can very quickly schedule out one AS, no matter what the
+ * situation (because there will always be one AS that's not currently running
+ * on the job slot - it can only have a job in the NEXT register).
+ *  - Even with this scheduling out, fair-share can still be guaranteed e.g. by
+ * a timeline-based Completely Fair Scheduler.
+ * - When our high-priority context comes in, we can do this quick-scheduling
+ * out immediately, and then schedule in the high-priority context without having to block.
+ * - This all assumes that the context to schedule out is of lower
+ * priority. Otherwise, we will have to block waiting for some other low
+ * priority context to finish its jobs. Note that it's likely (but not
+ * impossible) that the high-priority context \b is running jobs, by virtue of
+ * it being high priority.
+ * - Therefore, we can give a high liklihood that on Mali-T600 at least one
+ * high-priority context can be started very quickly. For the general case, we
+ * can guarantee starting (no. ASs) - (no. JSs) high priority contexts
+ * quickly. In any case, there is a high likelihood that we're able to start
+ * more than one high priority context quickly.
+ *
+ * In terms of the functions used in the IRQ handler directly, these are the
+ * perfomance considerations:
+ * - kbase_js_policy_log_job_result():
+ *  - This is just adding to a 64-bit value (possibly even a 32-bit value if we
+ * only store the time the job's recently spent - see below on 'priority weighting')
+ *  - For priority weighting, a divide operation ('div') could happen, but
+ * this can happen in a deferred context (outside of IRQ) when scheduling out
+ * the ctx; as per our Engineering Specification, the contexts of different
+ * priority still stay scheduled in for the same timeslice, but higher priority
+ * ones scheduled back in more often.
+ *  - That is, the weighted and unweighted times must be stored separately, and
+ * the weighted time is only updated \em outside of IRQ context.
+ *  - Of course, this divide is more likely to be a 'multiply by inverse of the
+ * weight', assuming that the weight (priority) doesn't change.
+ * - kbasep_js_policy_should_remove_ctx():
+ *  - This is usually just a comparison of the stored time value against some
+ * maximum value.
+ *
+ * @note all deferred work can be wrapped up into one call - we usually need to
+ * indicate that a job/bag is done outside of IRQ context anyway.
+ *
+ *
+ *
+ * @section sec_kbase_js_policy_operation_submit Submission path
+ *
+ * Start with a Context with no jobs present, and assume equal priority of all
+ * contexts in the system. The following work all happens outside of IRQ
+ * Context :
+ * - As soon as job is made 'ready to 'run', then is must be registerd with the Job
+ * Scheduler Policy:
+ *  - 'Ready to run' means they've satisified their dependencies in the
+ * Kernel-side Job Dispatch system.
+ *  - Call kbasep_js_policy_enqueue_job()
+ *  - This indicates that the job should be scheduled (it is ready to run).
+ * - As soon as a ctx changes from having 0 jobs 'ready to run' to >0 jobs
+ * 'ready to run', we enqueue the context on the policy queue:
+ *  - Call kbasep_js_policy_enqueue_ctx()
+ *  - This indicates that the \em ctx should be scheduled (it is ready to run)
+ *
+ * Next, we need to handle adding a context to the Run Pool - if it's sensible
+ * to do so. This can happen due to two reasons:
+ * -# A context is enqueued as above, and there are ASs free for it to run on
+ * (e.g. it is the first context to be run, in which case it can be added to
+ * the Run Pool immediately after enqueuing on the Policy Queue)
+ * -# A previous IRQ caused another ctx to be scheduled out, requiring that the
+ * context at the head of the queue be scheduled in. Such steps would happen in
+ * a work queue (work deferred from the IRQ context).
+ *
+ * In both cases, we'd handle it as follows:
+ * - Get the context at the Head of the Policy Queue:
+ *  - Call kbasep_js_policy_dequeue_head_ctx()
+ * - Assign the Context an Address Space (Assert that there will be one free,
+ * given the above two reasons)
+ * - Add this context to the Run Pool:
+ *  - Call kbasep_js_policy_runpool_add_ctx()
+ * - Now see if a job should be run:
+ *  - Mostly, this will be done in the IRQ handler at the completion of a
+ * previous job.
+ *  - However, there are two cases where this cannot be done: a) The first job
+ * enqueued to the system (there is no previous IRQ to act upon) b) When jobs
+ * are submitted at a low enough rate to not fill up all Job Slots (or, not to
+ * fill both the 'HEAD' and 'NEXT' registers in the job-slots)
+ *  - Hence, on each ctx <b>and job</b> submission we should try to see if we
+ * can run a job:
+ *  - For each job slot that has free space (in NEXT or HEAD+NEXT registers):
+ *   - Call kbasep_js_policy_dequeue_job() with core_req set to that of the
+ * slot
+ *   - if we got one, submit it to the job slot.
+ *   - This is repeated until kbasep_js_policy_dequeue_job() returns
+ * false, or the job slot has a job queued on both the HEAD and NEXT registers.
+ *
+ * The above case shows that we should attempt to run jobs in cases where a) a ctx
+ * has been added to the Run Pool, and b) new jobs have been added to a context
+ * in the Run Pool:
+ * - In the latter case, the context is in the runpool because it's got a job
+ * ready to run, or is already running a job
+ * - We could just wait until the IRQ handler fires, but for certain types of
+ * jobs this can take comparatively a long time to complete, e.g. GLES FS jobs
+ * generally take much longer to run that GLES CS jobs, which are vertex shader
+ * jobs.
+ * - Therefore, when a new job appears in the ctx, we must check the job-slots
+ * to see if they're free, and run the jobs as before.
+ *
+ *
+ *
+ * @section sec_kbase_js_policy_operation_submit_hipri Submission path for High Priority Contexts
+ *
+ * For High Priority Contexts on Mali-T600, we can make sure that at least 1 of
+ * them can be scheduled in immediately to start high prioriy jobs. In general,
+ * (no. ASs) - (no JSs) high priority contexts may be started immediately. The
+ * following describes how this happens:
+ *
+ * Similar to the previous section, consider what happens with a high-priority
+ * context (a context with a priority higher than that of any in the Run Pool)
+ * that starts out with no jobs:
+ * - A job becomes ready to run on the context, and so we enqueue the context
+ * on the Policy's Queue.
+ * - However, we'd like to schedule in this context immediately, instead of
+ * waiting for one of the Run Pool contexts' timeslice to expire
+ * - The policy's Enqueue function must detect this (because it is the policy
+ * that embodies the concept of priority), and take appropriate action
+ *  - That is, kbasep_js_policy_enqueue_ctx() should check the Policy's Run
+ * Pool to see if a lower priority context should be scheduled out, and then
+ * schedule in the High Priority context.
+ *  - For Mali-T600, we can always pick a context to schedule out immediately
+ * (because there are more ASs than JSs), and so scheduling out a victim context
+ * and scheduling in the high priority context can happen immediately.
+ *   - If a policy implements fair-sharing, then this can still ensure the
+ * victim later on gets a fair share of the GPU.
+ *   - As a note, consider whether the victim can be of equal/higher priority
+ * than the incoming context:
+ *   - Usually, higher priority contexts will be the ones currently running
+ * jobs, and so the context with the lowest priority is usually not running
+ * jobs.
+ *   - This makes it likely that the victim context is low priority, but
+ * it's not impossible for it to be a high priority one:
+ *    - Suppose 3 high priority contexts are submitting only FS jobs, and one low
+ * priority context submitting CS jobs. Then, the context not running jobs will
+ * be one of the hi priority contexts (because only 2 FS jobs can be
+ * queued/running on the GPU HW for Mali-T600).
+ *   - The problem can be mitigated by extra action, but it's questionable
+ * whether we need to: we already have a high likelihood that there's at least
+ * one high priority context - that should be good enough.
+ *   - And so, this method makes sure that at least one high priority context
+ * can be started very quickly, but more than one high priority contexts could be
+ * delayed (up to one timeslice).
+ *   - To improve this, use a GPU with a higher number of Address Spaces vs Job
+ * Slots.
+ * - At this point, let's assume this high priority context has been scheduled
+ * in immediately. The next step is to ensure it can start some jobs quickly.
+ *  - It must do this by Soft-Stopping jobs on any of the Job Slots that it can
+ * submit to.
+ *  - The rest of the logic for starting the jobs is taken care of by the IRQ
+ * handler. All the policy needs to do is ensure that
+ * kbasep_js_policy_dequeue_job() will return the jobs from the high priority
+ * context.
+ *
+ * @note in SS state, we currently only use 2 job-slots (even for T608, but
+ * this might change in future). In this case, it's always possible to schedule
+ * out 2 ASs quickly (their jobs won't be in the HEAD registers). At the same
+ * time, this maximizes usage of the job-slots (only 2 are in use), because you
+ * can guarantee starting of the jobs from the High Priority contexts immediately too.
+ *
+ *
+ *
+ * @section sec_kbase_js_policy_operation_notes Notes
+ *
+ * - In this design, a separate 'init' is needed from dequeue/requeue, so that
+ * information can be retained between the dequeue/requeue calls. For example,
+ * the total time spent for a context/job could be logged between
+ * dequeue/requeuing, to implement Fair Sharing. In this case, 'init' just
+ * initializes that information to some known state.
+ *
+ *
+ *
+ */
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_js_policy Job Scheduler Policy APIs
+ * @{
+ *
+ * <b>Refer to @ref page_kbase_js_policy for an overview and detailed operation of
+ * the Job Scheduler Policy and its use from the Job Scheduler Core</b>.
+ */
+
+/**
+ * @brief Job Scheduler Policy structure
+ */
+union kbasep_js_policy;
+
+/**
+ * @brief Initialize the Job Scheduler Policy
+ */
+int kbasep_js_policy_init(struct kbase_device *kbdev);
+
+/**
+ * @brief Terminate the Job Scheduler Policy
+ */
+void kbasep_js_policy_term(union kbasep_js_policy *js_policy);
+
+/**
+ * @addtogroup kbase_js_policy_ctx Job Scheduler Policy, Context Management API
+ * @{
+ *
+ * <b>Refer to @ref page_kbase_js_policy for an overview and detailed operation of
+ * the Job Scheduler Policy and its use from the Job Scheduler Core</b>.
+ */
+
+/**
+ * @brief Job Scheduler Policy Ctx Info structure
+ *
+ * This structure is embedded in the struct kbase_context structure. It is used to:
+ * - track information needed for the policy to schedule the context (e.g. time
+ * used, OS priority etc.)
+ * - link together kbase_contexts into a queue, so that a struct kbase_context can be
+ * obtained as the container of the policy ctx info. This allows the API to
+ * return what "the next context" should be.
+ * - obtain other information already stored in the struct kbase_context for
+ * scheduling purposes (e.g process ID to get the priority of the originating
+ * process)
+ */
+union kbasep_js_policy_ctx_info;
+
+/**
+ * @brief Initialize a ctx for use with the Job Scheduler Policy
+ *
+ * This effectively initializes the union kbasep_js_policy_ctx_info structure within
+ * the struct kbase_context (itself located within the kctx->jctx.sched_info structure).
+ */
+int kbasep_js_policy_init_ctx(struct kbase_device *kbdev, struct kbase_context *kctx);
+
+/**
+ * @brief Terminate resources associated with using a ctx in the Job Scheduler
+ * Policy.
+ */
+void kbasep_js_policy_term_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx);
+
+/**
+ * @brief Enqueue a context onto the Job Scheduler Policy Queue
+ *
+ * If the context enqueued has a priority higher than any in the Run Pool, then
+ * it is the Policy's responsibility to decide whether to schedule out a low
+ * priority context from the Run Pool to allow the high priority context to be
+ * scheduled in.
+ *
+ * If the context has the privileged flag set, it will always be kept at the
+ * head of the queue.
+ *
+ * The caller will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * The caller will be holding kbasep_js_device_data::queue_mutex.
+ */
+void kbasep_js_policy_enqueue_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx);
+
+/**
+ * @brief Dequeue a context from the Head of the Job Scheduler Policy Queue
+ *
+ * The caller will be holding kbasep_js_device_data::queue_mutex.
+ *
+ * @return true if a context was available, and *kctx_ptr points to
+ * the kctx dequeued.
+ * @return false if no contexts were available.
+ */
+bool kbasep_js_policy_dequeue_head_ctx(union kbasep_js_policy *js_policy, struct kbase_context ** const kctx_ptr);
+
+/**
+ * @brief Evict a context from the Job Scheduler Policy Queue
+ *
+ * This is only called as part of destroying a kbase_context.
+ *
+ * There are many reasons why this might fail during the lifetime of a
+ * context. For example, the context is in the process of being scheduled. In
+ * that case a thread doing the scheduling might have a pointer to it, but the
+ * context is neither in the Policy Queue, nor is it in the Run
+ * Pool. Crucially, neither the Policy Queue, Run Pool, or the Context itself
+ * are locked.
+ *
+ * Hence to find out where in the system the context is, it is important to do
+ * more than just check the kbasep_js_kctx_info::ctx::is_scheduled member.
+ *
+ * The caller will be holding kbasep_js_device_data::queue_mutex.
+ *
+ * @return true if the context was evicted from the Policy Queue
+ * @return false if the context was not found in the Policy Queue
+ */
+bool kbasep_js_policy_try_evict_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx);
+
+/**
+ * @brief Call a function on all jobs belonging to a non-queued, non-running
+ * context, optionally detaching the jobs from the context as it goes.
+ *
+ * At the time of the call, the context is guarenteed to be not-currently
+ * scheduled on the Run Pool (is_scheduled == false), and not present in
+ * the Policy Queue. This is because one of the following functions was used
+ * recently on the context:
+ * - kbasep_js_policy_evict_ctx()
+ * - kbasep_js_policy_runpool_remove_ctx()
+ *
+ * In both cases, no subsequent call was made on the context to any of:
+ * - kbasep_js_policy_runpool_add_ctx()
+ * - kbasep_js_policy_enqueue_ctx()
+ *
+ * Due to the locks that might be held at the time of the call, the callback
+ * may need to defer work on a workqueue to complete its actions (e.g. when
+ * cancelling jobs)
+ *
+ * \a detach_jobs must only be set when cancelling jobs (which occurs as part
+ * of context destruction).
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ */
+void kbasep_js_policy_foreach_ctx_job(union kbasep_js_policy *js_policy, struct kbase_context *kctx,
+	kbasep_js_policy_ctx_job_cb callback, bool detach_jobs);
+
+/**
+ * @brief Add a context to the Job Scheduler Policy's Run Pool
+ *
+ * If the context enqueued has a priority higher than any in the Run Pool, then
+ * it is the Policy's responsibility to decide whether to schedule out low
+ * priority jobs that are currently running on the GPU.
+ *
+ * The number of contexts present in the Run Pool will never be more than the
+ * number of Address Spaces.
+ *
+ * The following guarentees are made about the state of the system when this
+ * is called:
+ * - kctx->as_nr member is valid
+ * - the context has its submit_allowed flag set
+ * - kbasep_js_device_data::runpool_irq::per_as_data[kctx->as_nr] is valid
+ * - The refcount of the context is guarenteed to be zero.
+ * - kbasep_js_kctx_info::ctx::is_scheduled will be true.
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_mutex.
+ * - it will be holding hwaccess_lock (a spinlock)
+ *
+ * Due to a spinlock being held, this function must not call any APIs that sleep.
+ */
+void kbasep_js_policy_runpool_add_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx);
+
+/**
+ * @brief Remove a context from the Job Scheduler Policy's Run Pool
+ *
+ * The kctx->as_nr member is valid and the context has its submit_allowed flag
+ * set when this is called. The state of
+ * kbasep_js_device_data::runpool_irq::per_as_data[kctx->as_nr] is also
+ * valid. The refcount of the context is guarenteed to be zero.
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_mutex.
+ * - it will be holding hwaccess_lock (a spinlock)
+ *
+ * Due to a spinlock being held, this function must not call any APIs that sleep.
+ */
+void kbasep_js_policy_runpool_remove_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx);
+
+/**
+ * @brief Indicate whether a context should be removed from the Run Pool
+ * (should be scheduled out).
+ *
+ * The hwaccess_lock will be held by the caller.
+ *
+ * @note This API is called from IRQ context.
+ */
+bool kbasep_js_policy_should_remove_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx);
+
+/**
+ * @brief Synchronize with any timers acting upon the runpool
+ *
+ * The policy should check whether any timers it owns should be running. If
+ * they should not, the policy must cancel such timers and ensure they are not
+ * re-run by the time this function finishes.
+ *
+ * In particular, the timers must not be running when there are no more contexts
+ * on the runpool, because the GPU could be powered off soon after this call.
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_mutex.
+ */
+void kbasep_js_policy_runpool_timers_sync(union kbasep_js_policy *js_policy);
+
+
+/**
+ * @brief Indicate whether a new context has an higher priority than the current context.
+ *
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held for \a new_ctx
+ *
+ * This function must not sleep, because an IRQ spinlock might be held whilst
+ * this is called.
+ *
+ * @note There is nothing to stop the priority of \a current_ctx changing
+ * during or immediately after this function is called (because its jsctx_mutex
+ * cannot be held). Therefore, this function should only be seen as a heuristic
+ * guide as to whether \a new_ctx is higher priority than \a current_ctx
+ */
+bool kbasep_js_policy_ctx_has_priority(union kbasep_js_policy *js_policy, struct kbase_context *current_ctx, struct kbase_context *new_ctx);
+
+	  /** @} *//* end group kbase_js_policy_ctx */
+
+/**
+ * @addtogroup kbase_js_policy_job Job Scheduler Policy, Job Chain Management API
+ * @{
+ *
+ * <b>Refer to @ref page_kbase_js_policy for an overview and detailed operation of
+ * the Job Scheduler Policy and its use from the Job Scheduler Core</b>.
+ */
+
+/**
+ * @brief Job Scheduler Policy Job Info structure
+ *
+ * This structure is embedded in the struct kbase_jd_atom structure. It is used to:
+ * - track information needed for the policy to schedule the job (e.g. time
+ * used, etc.)
+ * - link together jobs into a queue/buffer, so that a struct kbase_jd_atom can be
+ * obtained as the container of the policy job info. This allows the API to
+ * return what "the next job" should be.
+ */
+union kbasep_js_policy_job_info;
+
+/**
+ * @brief Initialize a job for use with the Job Scheduler Policy
+ *
+ * This function initializes the union kbasep_js_policy_job_info structure within the
+ * kbase_jd_atom. It will only initialize/allocate resources that are specific
+ * to the job.
+ *
+ * That is, this function makes \b no attempt to:
+ * - initialize any context/policy-wide information
+ * - enqueue the job on the policy.
+ *
+ * At some later point, the following functions must be called on the job, in this order:
+ * - kbasep_js_policy_register_job() to register the job and initialize policy/context wide data.
+ * - kbasep_js_policy_enqueue_job() to enqueue the job
+ *
+ * A job must only ever be initialized on the Policy once, and must be
+ * terminated on the Policy before the job is freed.
+ *
+ * The caller will not be holding any locks, and so this function will not
+ * modify any information in \a kctx or \a js_policy.
+ *
+ * @return 0 if initialization was correct.
+ */
+int kbasep_js_policy_init_job(const union kbasep_js_policy *js_policy, const struct kbase_context *kctx, struct kbase_jd_atom *katom);
+
+/**
+ * @brief Register context/policy-wide information for a job on the Job Scheduler Policy.
+ *
+ * Registers the job with the policy. This is used to track the job before it
+ * has been enqueued/requeued by kbasep_js_policy_enqueue_job(). Specifically,
+ * it is used to update information under a lock that could not be updated at
+ * kbasep_js_policy_init_job() time (such as context/policy-wide data).
+ *
+ * @note This function will not fail, and hence does not allocate any
+ * resources. Any failures that could occur on registration will be caught
+ * during kbasep_js_policy_init_job() instead.
+ *
+ * A job must only ever be registerd on the Policy once, and must be
+ * deregistered on the Policy on completion (whether or not that completion was
+ * success/failure).
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held.
+ */
+void kbasep_js_policy_register_job(union kbasep_js_policy *js_policy, struct kbase_context *kctx, struct kbase_jd_atom *katom);
+
+/**
+ * @brief De-register context/policy-wide information for a on the Job Scheduler Policy.
+ *
+ * This must be used before terminating the resources associated with using a
+ * job in the Job Scheduler Policy. This function does not itself terminate any
+ * resources, at most it just updates information in the policy and context.
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held.
+ */
+void kbasep_js_policy_deregister_job(union kbasep_js_policy *js_policy, struct kbase_context *kctx, struct kbase_jd_atom *katom);
+
+/**
+ * @brief Dequeue a Job for a job slot from the Job Scheduler Policy Run Pool
+ *
+ * The job returned by the policy will match at least one of the bits in the
+ * job slot's core requirements (but it may match more than one, or all @ref
+ * base_jd_core_req bits supported by the job slot).
+ *
+ * In addition, the requirements of the job returned will be a subset of those
+ * requested - the job returned will not have requirements that \a job_slot_idx
+ * cannot satisfy.
+ *
+ * The caller will submit the job to the GPU as soon as the GPU's NEXT register
+ * for the corresponding slot is empty. Of course, the GPU will then only run
+ * this new job when the currently executing job (in the jobslot's HEAD
+ * register) has completed.
+ *
+ * @return true if a job was available, and *kctx_ptr points to
+ * the kctx dequeued.
+ * @return false if no jobs were available among all ctxs in the Run Pool.
+ *
+ * @note base_jd_core_req is currently a u8 - beware of type conversion.
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_device_data::runpool_lock::irq will be held.
+ * - kbasep_js_device_data::runpool_mutex will be held.
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex. will be held
+ */
+bool kbasep_js_policy_dequeue_job(struct kbase_device *kbdev, int job_slot_idx, struct kbase_jd_atom ** const katom_ptr);
+
+/**
+ * @brief Requeue a Job back into the Job Scheduler Policy Run Pool
+ *
+ * This will be used to enqueue a job after its creation and also to requeue
+ * a job into the Run Pool that was previously dequeued (running). It notifies
+ * the policy that the job should be run again at some point later.
+ *
+ * The caller has the following conditions on locking:
+ * - hwaccess_lock (a spinlock) will be held.
+ * - kbasep_js_device_data::runpool_mutex will be held.
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held.
+ */
+void kbasep_js_policy_enqueue_job(union kbasep_js_policy *js_policy, struct kbase_jd_atom *katom);
+
+/**
+ * @brief Log the result of a job: the time spent on a job/context, and whether
+ * the job failed or not.
+ *
+ * Since a struct kbase_jd_atom contains a pointer to the struct kbase_context owning it,
+ * then this can also be used to log time on either/both the job and the
+ * containing context.
+ *
+ * The completion state of the job can be found by examining \a katom->event.event_code
+ *
+ * If the Job failed and the policy is implementing fair-sharing, then the
+ * policy must penalize the failing job/context:
+ * - At the very least, it should penalize the time taken by the amount of
+ * time spent processing the IRQ in SW. This because a job in the NEXT slot
+ * waiting to run will be delayed until the failing job has had the IRQ
+ * cleared.
+ * - \b Optionally, the policy could apply other penalties. For example, based
+ * on a threshold of a number of failing jobs, after which a large penalty is
+ * applied.
+ *
+ * The kbasep_js_device_data::runpool_mutex will be held by the caller.
+ *
+ * @note This API is called from IRQ context.
+ *
+ * The caller has the following conditions on locking:
+ * - hwaccess_lock will be held.
+ *
+ * @param js_policy     job scheduler policy
+ * @param katom         job dispatch atom
+ * @param time_spent_us the time spent by the job, in microseconds (10^-6 seconds).
+ */
+void kbasep_js_policy_log_job_result(union kbasep_js_policy *js_policy, struct kbase_jd_atom *katom, u64 time_spent_us);
+
+	  /** @} *//* end group kbase_js_policy_job */
+
+	  /** @} *//* end group kbase_js_policy */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif				/* _KBASE_JS_POLICY_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c b/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c
new file mode 100644
index 0000000..1ac0569
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.c
@@ -0,0 +1,292 @@
+/*
+ *
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/*
+ * Job Scheduler: Completely Fair Policy Implementation
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_js.h>
+#include <mali_kbase_js_policy_cfs.h>
+#include <linux/version.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 9, 0)
+#include <linux/sched/rt.h>
+#endif
+
+/**
+ * Define for when dumping is enabled.
+ * This should not be based on the instrumentation level as whether dumping is enabled for a particular level is down to the integrator.
+ * However this is being used for now as otherwise the cinstr headers would be needed.
+ */
+#define CINSTR_DUMPING_ENABLED (2 == MALI_INSTRUMENTATION_LEVEL)
+
+/* Fixed point constants used for runtime weight calculations */
+#define WEIGHT_FIXEDPOINT_SHIFT 10
+#define WEIGHT_TABLE_SIZE       40
+#define WEIGHT_0_NICE           (WEIGHT_TABLE_SIZE/2)
+#define WEIGHT_0_VAL            (1 << WEIGHT_FIXEDPOINT_SHIFT)
+
+#define PROCESS_PRIORITY_MIN (-20)
+#define PROCESS_PRIORITY_MAX  (19)
+
+/* Defines for easy asserts 'is scheduled'/'is queued'/'is neither queued norscheduled' */
+#define KBASEP_JS_CHECKFLAG_QUEUED       (1u << 0) /**< Check the queued state */
+#define KBASEP_JS_CHECKFLAG_SCHEDULED    (1u << 1) /**< Check the scheduled state */
+#define KBASEP_JS_CHECKFLAG_IS_QUEUED    (1u << 2) /**< Expect queued state to be set */
+#define KBASEP_JS_CHECKFLAG_IS_SCHEDULED (1u << 3) /**< Expect scheduled state to be set */
+
+enum {
+	KBASEP_JS_CHECK_NOTQUEUED = KBASEP_JS_CHECKFLAG_QUEUED,
+	KBASEP_JS_CHECK_NOTSCHEDULED = KBASEP_JS_CHECKFLAG_SCHEDULED,
+	KBASEP_JS_CHECK_QUEUED = KBASEP_JS_CHECKFLAG_QUEUED | KBASEP_JS_CHECKFLAG_IS_QUEUED,
+	KBASEP_JS_CHECK_SCHEDULED = KBASEP_JS_CHECKFLAG_SCHEDULED | KBASEP_JS_CHECKFLAG_IS_SCHEDULED
+};
+
+typedef u32 kbasep_js_check;
+
+/*
+ * Private Functions
+ */
+
+/* Table autogenerated using util built from: base/tools/gen_cfs_weight_of_prio/ */
+
+/* weight = 1.25 */
+static const int weight_of_priority[] = {
+	/*  -20 */ 11, 14, 18, 23,
+	/*  -16 */ 29, 36, 45, 56,
+	/*  -12 */ 70, 88, 110, 137,
+	/*   -8 */ 171, 214, 268, 335,
+	/*   -4 */ 419, 524, 655, 819,
+	/*    0 */ 1024, 1280, 1600, 2000,
+	/*    4 */ 2500, 3125, 3906, 4883,
+	/*    8 */ 6104, 7630, 9538, 11923,
+	/*   12 */ 14904, 18630, 23288, 29110,
+	/*   16 */ 36388, 45485, 56856, 71070
+};
+
+/*
+ * Note: There is nothing to stop the priority of the ctx containing
+ * ctx_info changing during or immediately after this function is called
+ * (because its jsctx_mutex cannot be held during IRQ). Therefore, this
+ * function should only be seen as a heuristic guide as to the priority weight
+ * of the context.
+ */
+static u64 priority_weight(struct kbasep_js_policy_cfs_ctx *ctx_info, u64 time_us)
+{
+	u64 time_delta_us;
+	int priority;
+
+	priority = ctx_info->process_priority;
+
+	/* Adjust runtime_us using priority weight if required */
+	if (priority != 0 && time_us != 0) {
+		int clamped_priority;
+
+		/* Clamp values to min..max weights */
+		if (priority > PROCESS_PRIORITY_MAX)
+			clamped_priority = PROCESS_PRIORITY_MAX;
+		else if (priority < PROCESS_PRIORITY_MIN)
+			clamped_priority = PROCESS_PRIORITY_MIN;
+		else
+			clamped_priority = priority;
+
+		/* Fixed point multiplication */
+		time_delta_us = (time_us * weight_of_priority[WEIGHT_0_NICE + clamped_priority]);
+		/* Remove fraction */
+		time_delta_us = time_delta_us >> WEIGHT_FIXEDPOINT_SHIFT;
+		/* Make sure the time always increases */
+		if (0 == time_delta_us)
+			time_delta_us++;
+	} else {
+		time_delta_us = time_us;
+	}
+
+	return time_delta_us;
+}
+
+#if KBASE_TRACE_ENABLE
+static int kbasep_js_policy_trace_get_refcnt_nolock(struct kbase_device *kbdev, struct kbase_context *kctx)
+{
+	struct kbasep_js_device_data *js_devdata;
+	int as_nr;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		struct kbasep_js_per_as_data *js_per_as_data;
+
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		refcnt = js_per_as_data->as_busy_refcount;
+	}
+
+	return refcnt;
+}
+
+static inline int kbasep_js_policy_trace_get_refcnt(struct kbase_device *kbdev, struct kbase_context *kctx)
+{
+	unsigned long flags;
+	struct kbasep_js_device_data *js_devdata;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	refcnt = kbasep_js_policy_trace_get_refcnt_nolock(kbdev, kctx);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return refcnt;
+}
+#else				/* KBASE_TRACE_ENABLE  */
+static inline int kbasep_js_policy_trace_get_refcnt(struct kbase_device *kbdev, struct kbase_context *kctx)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(kctx);
+	return 0;
+}
+#endif				/* KBASE_TRACE_ENABLE  */
+
+
+/*
+ * Non-private functions
+ */
+
+int kbasep_js_policy_init(struct kbase_device *kbdev)
+{
+	struct kbasep_js_device_data *js_devdata;
+	struct kbasep_js_policy_cfs *policy_info;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+	policy_info = &js_devdata->policy.cfs;
+
+	atomic64_set(&policy_info->least_runtime_us, KBASEP_JS_RUNTIME_EMPTY);
+	atomic64_set(&policy_info->rt_least_runtime_us, KBASEP_JS_RUNTIME_EMPTY);
+
+	policy_info->head_runtime_us = 0;
+
+	return 0;
+}
+
+void kbasep_js_policy_term(union kbasep_js_policy *js_policy)
+{
+	CSTD_UNUSED(js_policy);
+}
+
+int kbasep_js_policy_init_ctx(struct kbase_device *kbdev, struct kbase_context *kctx)
+{
+	struct kbasep_js_device_data *js_devdata;
+	struct kbasep_js_policy_cfs_ctx *ctx_info;
+	struct kbasep_js_policy_cfs *policy_info;
+	int policy;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	js_devdata = &kbdev->js_data;
+	policy_info = &kbdev->js_data.policy.cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_INIT_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx));
+
+	policy = current->policy;
+	if (policy == SCHED_FIFO || policy == SCHED_RR) {
+		ctx_info->process_rt_policy = true;
+		ctx_info->process_priority = (((MAX_RT_PRIO - 1) - current->rt_priority) / 5) - 20;
+	} else {
+		ctx_info->process_rt_policy = false;
+		ctx_info->process_priority = (current->static_prio - MAX_RT_PRIO) - 20;
+	}
+
+	/* Initial runtime (relative to least-run context runtime)
+	 *
+	 * This uses the Policy Queue's most up-to-date head_runtime_us by using the
+	 * queue mutex to issue memory barriers - also ensure future updates to
+	 * head_runtime_us occur strictly after this context is initialized */
+	mutex_lock(&js_devdata->queue_mutex);
+
+	/* No need to hold the the hwaccess_lock here, because we're initializing
+	 * the value, and the context is definitely not being updated in the
+	 * runpool at this point. The queue_mutex ensures the memory barrier. */
+	ctx_info->runtime_us = policy_info->head_runtime_us + priority_weight(ctx_info, (u64) js_devdata->cfs_ctx_runtime_init_slices * (u64) (js_devdata->ctx_timeslice_ns / 1000u));
+
+	mutex_unlock(&js_devdata->queue_mutex);
+
+	return 0;
+}
+
+void kbasep_js_policy_term_ctx(union kbasep_js_policy *js_policy, struct kbase_context *kctx)
+{
+	struct kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kbdev = container_of(js_policy, struct kbase_device, js_data.policy);
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_TERM_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx));
+
+	/* No work to do */
+}
+
+/*
+ * Job Chain Management
+ */
+
+void kbasep_js_policy_log_job_result(union kbasep_js_policy *js_policy, struct kbase_jd_atom *katom, u64 time_spent_us)
+{
+	struct kbasep_js_policy_cfs_ctx *ctx_info;
+	struct kbase_context *parent_ctx;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	CSTD_UNUSED(js_policy);
+
+	parent_ctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(parent_ctx != NULL);
+
+	ctx_info = &parent_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	ctx_info->runtime_us += priority_weight(ctx_info, time_spent_us);
+
+	katom->time_spent_us += time_spent_us;
+}
+
+bool kbasep_js_policy_ctx_has_priority(union kbasep_js_policy *js_policy, struct kbase_context *current_ctx, struct kbase_context *new_ctx)
+{
+	struct kbasep_js_policy_cfs_ctx *current_ctx_info;
+	struct kbasep_js_policy_cfs_ctx *new_ctx_info;
+
+	KBASE_DEBUG_ASSERT(current_ctx != NULL);
+	KBASE_DEBUG_ASSERT(new_ctx != NULL);
+	CSTD_UNUSED(js_policy);
+
+	current_ctx_info = &current_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+	new_ctx_info = &new_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	if (!current_ctx_info->process_rt_policy && new_ctx_info->process_rt_policy)
+		return true;
+
+	if (current_ctx_info->process_rt_policy ==
+			new_ctx_info->process_rt_policy)
+		return true;
+
+	return false;
+}
diff --git a/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h b/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h
new file mode 100644
index 0000000..0a8454c
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_js_policy_cfs.h
@@ -0,0 +1,81 @@
+/*
+ *
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/*
+ * Completely Fair Job Scheduler Policy structure definitions
+ */
+
+#ifndef _KBASE_JS_POLICY_CFS_H_
+#define _KBASE_JS_POLICY_CFS_H_
+
+#define KBASEP_JS_RUNTIME_EMPTY ((u64)-1)
+
+/**
+ * struct kbasep_js_policy_cfs - Data for the CFS policy
+ * @head_runtime_us:  Number of microseconds the least-run context has been
+ *                    running for. The kbasep_js_device_data.queue_mutex must
+ *                    be held whilst updating this.
+ *                    Reads are possible without this mutex, but an older value
+ *                    might be read if no memory barries are issued beforehand.
+ * @least_runtime_us: Number of microseconds the least-run context in the
+ *                    context queue has been running for.
+ *                    -1 if context queue is empty.
+ * @rt_least_runtime_us: Number of microseconds the least-run context in the
+ *                       realtime (priority) context queue has been running for.
+ *                       -1 if realtime context queue is empty
+ */
+struct kbasep_js_policy_cfs {
+	u64 head_runtime_us;
+	atomic64_t least_runtime_us;
+	atomic64_t rt_least_runtime_us;
+};
+
+/**
+ * struct kbasep_js_policy_cfs_ctx - a single linked list of all contexts
+ * @runtime_us:        microseconds this context has been running for
+ * @process_rt_policy: set if calling process policy scheme is a realtime
+ *                     scheduler and will use the priority queue. Non-mutable
+ *                     after ctx init
+ * @process_priority:  calling process NICE priority, in the range -20..19
+ *
+ * hwaccess_lock must be held when updating @runtime_us. Initializing will occur
+ * on context init and context enqueue (which can only occur in one thread at a
+ * time), but multi-thread access only occurs while the context is in the
+ * runpool.
+ *
+ * Reads are possible without the spinlock, but an older value might be read if
+ * no memory barries are issued beforehand.
+ */
+struct kbasep_js_policy_cfs_ctx {
+	u64 runtime_us;
+	bool process_rt_policy;
+	int process_priority;
+};
+
+/**
+ * struct kbasep_js_policy_cfs_job - per job information for CFS
+ * @ticks: number of ticks that this job has been executing for
+ *
+ * hwaccess_lock must be held when accessing @ticks.
+ */
+struct kbasep_js_policy_cfs_job {
+	u32 ticks;
+};
+
+#endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem.c b/drivers/gpu/arm/midgard/mali_kbase_mem.c
index 4a223e8..4824b31 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -40,71 +40,25 @@
 #include <mali_kbase_hwaccess_time.h>
 #include <mali_kbase_tlstream.h>
 
-/* This function finds out which RB tree the given GPU VA region belongs to
- * based on the region zone */
-static struct rb_root *kbase_reg_flags_to_rbtree(struct kbase_context *kctx,
-						    struct kbase_va_region *reg)
+/**
+ * @brief Check the zone compatibility of two regions.
+ */
+static int kbase_region_tracker_match_zone(struct kbase_va_region *reg1,
+		struct kbase_va_region *reg2)
 {
-	struct rb_root *rbtree = NULL;
-
-	switch (reg->flags & KBASE_REG_ZONE_MASK) {
-	case KBASE_REG_ZONE_CUSTOM_VA:
-		rbtree = &kctx->reg_rbtree_custom;
-		break;
-	case KBASE_REG_ZONE_EXEC:
-		rbtree = &kctx->reg_rbtree_exec;
-		break;
-	case KBASE_REG_ZONE_SAME_VA:
-		rbtree = &kctx->reg_rbtree_same;
-		/* fall through */
-	default:
-		rbtree = &kctx->reg_rbtree_same;
-		break;
-	}
-
-	return rbtree;
+	return ((reg1->flags & KBASE_REG_ZONE_MASK) ==
+		(reg2->flags & KBASE_REG_ZONE_MASK));
 }
 
-/* This function finds out which RB tree the given pfn from the GPU VA belongs
- * to based on the memory zone the pfn refers to */
-static struct rb_root *kbase_gpu_va_to_rbtree(struct kbase_context *kctx,
-								    u64 gpu_pfn)
-{
-	struct rb_root *rbtree = NULL;
-
-#ifdef CONFIG_64BIT
-	if (kbase_ctx_flag(kctx, KCTX_COMPAT)) {
-#endif /* CONFIG_64BIT */
-		if (gpu_pfn >= KBASE_REG_ZONE_CUSTOM_VA_BASE)
-			rbtree = &kctx->reg_rbtree_custom;
-		else if (gpu_pfn >= KBASE_REG_ZONE_EXEC_BASE)
-			rbtree = &kctx->reg_rbtree_exec;
-		else
-			rbtree = &kctx->reg_rbtree_same;
-#ifdef CONFIG_64BIT
-	} else {
-		if (gpu_pfn >= kctx->same_va_end)
-			rbtree = &kctx->reg_rbtree_custom;
-		else
-			rbtree = &kctx->reg_rbtree_same;
-	}
-#endif /* CONFIG_64BIT */
-
-	return rbtree;
-}
+KBASE_EXPORT_TEST_API(kbase_region_tracker_match_zone);
 
 /* This function inserts a region into the tree. */
-static void kbase_region_tracker_insert(struct kbase_context *kctx,
-						struct kbase_va_region *new_reg)
+static void kbase_region_tracker_insert(struct kbase_context *kctx, struct kbase_va_region *new_reg)
 {
 	u64 start_pfn = new_reg->start_pfn;
-	struct rb_node **link = NULL;
+	struct rb_node **link = &(kctx->reg_rbtree.rb_node);
 	struct rb_node *parent = NULL;
-	struct rb_root *rbtree = NULL;
-
-	rbtree = kbase_reg_flags_to_rbtree(kctx, new_reg);
 
-	link = &(rbtree->rb_node);
 	/* Find the right place in the tree using tree search */
 	while (*link) {
 		struct kbase_va_region *old_reg;
@@ -123,24 +77,19 @@ static void kbase_region_tracker_insert(struct kbase_context *kctx,
 
 	/* Put the new node there, and rebalance tree */
 	rb_link_node(&(new_reg->rblink), parent, link);
-
-	rb_insert_color(&(new_reg->rblink), rbtree);
+	rb_insert_color(&(new_reg->rblink), &(kctx->reg_rbtree));
 }
 
 /* Find allocated region enclosing free range. */
 static struct kbase_va_region *kbase_region_tracker_find_region_enclosing_range_free(
 		struct kbase_context *kctx, u64 start_pfn, size_t nr_pages)
 {
-	struct rb_node *rbnode = NULL;
-	struct kbase_va_region *reg = NULL;
-	struct rb_root *rbtree = NULL;
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
 
 	u64 end_pfn = start_pfn + nr_pages;
 
-	rbtree = kbase_gpu_va_to_rbtree(kctx, start_pfn);
-
-	rbnode = rbtree->rb_node;
-
+	rbnode = kctx->reg_rbtree.rb_node;
 	while (rbnode) {
 		u64 tmp_start_pfn, tmp_end_pfn;
 
@@ -167,16 +116,12 @@ struct kbase_va_region *kbase_region_tracker_find_region_enclosing_address(struc
 	struct rb_node *rbnode;
 	struct kbase_va_region *reg;
 	u64 gpu_pfn = gpu_addr >> PAGE_SHIFT;
-	struct rb_root *rbtree = NULL;
 
 	KBASE_DEBUG_ASSERT(NULL != kctx);
 
 	lockdep_assert_held(&kctx->reg_lock);
 
-	rbtree = kbase_gpu_va_to_rbtree(kctx, gpu_pfn);
-
-	rbnode = rbtree->rb_node;
-
+	rbnode = kctx->reg_rbtree.rb_node;
 	while (rbnode) {
 		u64 tmp_start_pfn, tmp_end_pfn;
 
@@ -203,18 +148,14 @@ KBASE_EXPORT_TEST_API(kbase_region_tracker_find_region_enclosing_address);
 struct kbase_va_region *kbase_region_tracker_find_region_base_address(struct kbase_context *kctx, u64 gpu_addr)
 {
 	u64 gpu_pfn = gpu_addr >> PAGE_SHIFT;
-	struct rb_node *rbnode = NULL;
-	struct kbase_va_region *reg = NULL;
-	struct rb_root *rbtree = NULL;
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
 
 	KBASE_DEBUG_ASSERT(NULL != kctx);
 
 	lockdep_assert_held(&kctx->reg_lock);
 
-	rbtree = kbase_gpu_va_to_rbtree(kctx, gpu_pfn);
-
-	rbnode = rbtree->rb_node;
-
+	rbnode = kctx->reg_rbtree.rb_node;
 	while (rbnode) {
 		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
 		if (reg->start_pfn > gpu_pfn)
@@ -234,21 +175,17 @@ KBASE_EXPORT_TEST_API(kbase_region_tracker_find_region_base_address);
 /* Find region meeting given requirements */
 static struct kbase_va_region *kbase_region_tracker_find_region_meeting_reqs(struct kbase_context *kctx, struct kbase_va_region *reg_reqs, size_t nr_pages, size_t align)
 {
-	struct rb_node *rbnode = NULL;
-	struct kbase_va_region *reg = NULL;
-	struct rb_root *rbtree = NULL;
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
 
 	/* Note that this search is a linear search, as we do not have a target
 	   address in mind, so does not benefit from the rbtree search */
-
-	rbtree = kbase_reg_flags_to_rbtree(kctx, reg_reqs);
-
-	rbnode = rb_first(rbtree);
-
+	rbnode = rb_first(&(kctx->reg_rbtree));
 	while (rbnode) {
 		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
 		if ((reg->nr_pages >= nr_pages) &&
-				(reg->flags & KBASE_REG_FREE)) {
+				(reg->flags & KBASE_REG_FREE) &&
+				kbase_region_tracker_match_zone(reg, reg_reqs)) {
 			/* Check alignment */
 			u64 start_pfn = (reg->start_pfn + align - 1) & ~(align - 1);
 
@@ -277,25 +214,19 @@ static int kbase_remove_va_region(struct kbase_context *kctx, struct kbase_va_re
 	struct kbase_va_region *prev = NULL;
 	struct rb_node *rbnext;
 	struct kbase_va_region *next = NULL;
-	struct rb_root *reg_rbtree = NULL;
 
 	int merged_front = 0;
 	int merged_back = 0;
 	int err = 0;
 
-	reg_rbtree = kbase_reg_flags_to_rbtree(kctx, reg);
-
 	/* Try to merge with the previous block first */
 	rbprev = rb_prev(&(reg->rblink));
 	if (rbprev) {
 		prev = rb_entry(rbprev, struct kbase_va_region, rblink);
-		if (prev->flags & KBASE_REG_FREE) {
-			/* We're compatible with the previous VMA,
-			 * merge with it */
-			WARN_ON((prev->flags & KBASE_REG_ZONE_MASK) !=
-					    (reg->flags & KBASE_REG_ZONE_MASK));
+		if ((prev->flags & KBASE_REG_FREE) && kbase_region_tracker_match_zone(prev, reg)) {
+			/* We're compatible with the previous VMA, merge with it */
 			prev->nr_pages += reg->nr_pages;
-			rb_erase(&(reg->rblink), reg_rbtree);
+			rb_erase(&(reg->rblink), &kctx->reg_rbtree);
 			reg = prev;
 			merged_front = 1;
 		}
@@ -307,12 +238,10 @@ static int kbase_remove_va_region(struct kbase_context *kctx, struct kbase_va_re
 	if (rbnext) {
 		/* We're compatible with the next VMA, merge with it */
 		next = rb_entry(rbnext, struct kbase_va_region, rblink);
-		if (next->flags & KBASE_REG_FREE) {
-			WARN_ON((next->flags & KBASE_REG_ZONE_MASK) !=
-					    (reg->flags & KBASE_REG_ZONE_MASK));
+		if ((next->flags & KBASE_REG_FREE) && kbase_region_tracker_match_zone(next, reg)) {
 			next->start_pfn = reg->start_pfn;
 			next->nr_pages += reg->nr_pages;
-			rb_erase(&(reg->rblink), reg_rbtree);
+			rb_erase(&(reg->rblink), &kctx->reg_rbtree);
 			merged_back = 1;
 			if (merged_front) {
 				/* We already merged with prev, free it */
@@ -334,7 +263,8 @@ static int kbase_remove_va_region(struct kbase_context *kctx, struct kbase_va_re
 			err = -ENOMEM;
 			goto out;
 		}
-		rb_replace_node(&(reg->rblink), &(free_reg->rblink), reg_rbtree);
+
+		rb_replace_node(&(reg->rblink), &(free_reg->rblink), &(kctx->reg_rbtree));
 	}
 
  out:
@@ -348,11 +278,8 @@ KBASE_EXPORT_TEST_API(kbase_remove_va_region);
  */
 static int kbase_insert_va_region_nolock(struct kbase_context *kctx, struct kbase_va_region *new_reg, struct kbase_va_region *at_reg, u64 start_pfn, size_t nr_pages)
 {
-	struct rb_root *reg_rbtree = NULL;
 	int err = 0;
 
-	reg_rbtree = kbase_reg_flags_to_rbtree(kctx, at_reg);
-
 	/* Must be a free region */
 	KBASE_DEBUG_ASSERT((at_reg->flags & KBASE_REG_FREE) != 0);
 	/* start_pfn should be contained within at_reg */
@@ -365,8 +292,7 @@ static int kbase_insert_va_region_nolock(struct kbase_context *kctx, struct kbas
 
 	/* Regions are a whole use, so swap and delete old one. */
 	if (at_reg->start_pfn == start_pfn && at_reg->nr_pages == nr_pages) {
-		rb_replace_node(&(at_reg->rblink), &(new_reg->rblink),
-								reg_rbtree);
+		rb_replace_node(&(at_reg->rblink), &(new_reg->rblink), &(kctx->reg_rbtree));
 		kbase_free_alloced_region(at_reg);
 	}
 	/* New region replaces the start of the old one, so insert before. */
@@ -441,7 +367,9 @@ int kbase_add_va_region(struct kbase_context *kctx,
 			err = -ENOMEM;
 			goto exit;
 		}
-		if (!(tmp->flags & KBASE_REG_FREE)) {
+
+		if ((!kbase_region_tracker_match_zone(tmp, reg)) ||
+				(!(tmp->flags & KBASE_REG_FREE))) {
 			dev_warn(dev, "Zone mismatch: %lu != %lu", tmp->flags & KBASE_REG_ZONE_MASK, reg->flags & KBASE_REG_ZONE_MASK);
 			dev_warn(dev, "!(tmp->flags & KBASE_REG_FREE): tmp->start_pfn=0x%llx tmp->flags=0x%lx tmp->nr_pages=0x%zx gpu_pfn=0x%llx nr_pages=0x%zx\n", tmp->start_pfn, tmp->flags, tmp->nr_pages, gpu_pfn, nr_pages);
 			dev_warn(dev, "in function %s (%p, %p, 0x%llx, 0x%zx, 0x%zx)\n", __func__, kctx, reg, addr, nr_pages, align);
@@ -506,43 +434,31 @@ static void kbase_region_tracker_ds_init(struct kbase_context *kctx,
 		struct kbase_va_region *exec_reg,
 		struct kbase_va_region *custom_va_reg)
 {
-	kctx->reg_rbtree_same = RB_ROOT;
+	kctx->reg_rbtree = RB_ROOT;
 	kbase_region_tracker_insert(kctx, same_va_reg);
 
-	/* Although exec and custom_va_reg don't always exist,
-	 * initialize unconditionally because of the mem_view debugfs
-	 * implementation which relies on these being empty */
-	kctx->reg_rbtree_exec = RB_ROOT;
-	kctx->reg_rbtree_custom = RB_ROOT;
-
-	if (exec_reg)
+	/* exec and custom_va_reg doesn't always exist */
+	if (exec_reg && custom_va_reg) {
 		kbase_region_tracker_insert(kctx, exec_reg);
-	if (custom_va_reg)
 		kbase_region_tracker_insert(kctx, custom_va_reg);
+	}
 }
 
-static void kbase_region_tracker_erase_rbtree(struct rb_root *rbtree)
+void kbase_region_tracker_term(struct kbase_context *kctx)
 {
 	struct rb_node *rbnode;
 	struct kbase_va_region *reg;
 
 	do {
-		rbnode = rb_first(rbtree);
+		rbnode = rb_first(&(kctx->reg_rbtree));
 		if (rbnode) {
-			rb_erase(rbnode, rbtree);
+			rb_erase(rbnode, &(kctx->reg_rbtree));
 			reg = rb_entry(rbnode, struct kbase_va_region, rblink);
 			kbase_free_alloced_region(reg);
 		}
 	} while (rbnode);
 }
 
-void kbase_region_tracker_term(struct kbase_context *kctx)
-{
-	kbase_region_tracker_erase_rbtree(&kctx->reg_rbtree_same);
-	kbase_region_tracker_erase_rbtree(&kctx->reg_rbtree_exec);
-	kbase_region_tracker_erase_rbtree(&kctx->reg_rbtree_custom);
-}
-
 /**
  * Initialize the region tracker data structure.
  */
@@ -712,7 +628,6 @@ int kbase_region_tracker_init_jit(struct kbase_context *kctx, u64 jit_va_pages)
 				kctx->same_va_end,
 				jit_va_pages,
 				KBASE_REG_ZONE_CUSTOM_VA);
-
 	if (!custom_va_reg) {
 		/*
 		 * The context will be destroyed if we fail here so no point
@@ -961,9 +876,6 @@ bad_insert:
 
 KBASE_EXPORT_TEST_API(kbase_gpu_mmap);
 
-static void kbase_jd_user_buf_unmap(struct kbase_context *kctx,
-		struct kbase_mem_phy_alloc *alloc, bool writeable);
-
 int kbase_gpu_munmap(struct kbase_context *kctx, struct kbase_va_region *reg)
 {
 	int err;
@@ -984,20 +896,6 @@ int kbase_gpu_munmap(struct kbase_context *kctx, struct kbase_va_region *reg)
 		kbase_mem_phy_alloc_gpu_unmapped(reg->gpu_alloc);
 	}
 
-	if (reg->gpu_alloc && reg->gpu_alloc->type ==
-			KBASE_MEM_TYPE_IMPORTED_USER_BUF) {
-		struct kbase_alloc_import_user_buf *user_buf =
-			&reg->gpu_alloc->imported.user_buf;
-
-		if (user_buf->current_mapping_usage_count & PINNED_ON_IMPORT) {
-			user_buf->current_mapping_usage_count &=
-				~PINNED_ON_IMPORT;
-
-			kbase_jd_user_buf_unmap(kctx, reg->gpu_alloc,
-					(reg->flags & KBASE_REG_GPU_WR));
-		}
-	}
-
 	if (err)
 		return err;
 
@@ -1005,66 +903,54 @@ int kbase_gpu_munmap(struct kbase_context *kctx, struct kbase_va_region *reg)
 	return err;
 }
 
-static struct kbase_cpu_mapping *kbasep_find_enclosing_cpu_mapping(
-		struct kbase_context *kctx,
-		unsigned long uaddr, size_t size, u64 *offset)
+static struct kbase_cpu_mapping *kbasep_find_enclosing_cpu_mapping_of_region(const struct kbase_va_region *reg, unsigned long uaddr, size_t size)
 {
-	struct vm_area_struct *vma;
 	struct kbase_cpu_mapping *map;
-	unsigned long vm_pgoff_in_region;
-	unsigned long vm_off_in_region;
-	unsigned long map_start;
-	size_t map_size;
+	struct list_head *pos;
 
-	lockdep_assert_held(&current->mm->mmap_sem);
+	KBASE_DEBUG_ASSERT(NULL != reg);
+	KBASE_DEBUG_ASSERT(reg->cpu_alloc);
 
 	if ((uintptr_t) uaddr + size < (uintptr_t) uaddr) /* overflow check */
 		return NULL;
 
-	vma = find_vma_intersection(current->mm, uaddr, uaddr+size);
-
-	if (!vma || vma->vm_start > uaddr)
-		return NULL;
-	if (vma->vm_ops != &kbase_vm_ops)
-		/* Not ours! */
-		return NULL;
-
-	map = vma->vm_private_data;
-
-	if (map->kctx != kctx)
-		/* Not from this context! */
-		return NULL;
-
-	vm_pgoff_in_region = vma->vm_pgoff - map->region->start_pfn;
-	vm_off_in_region = vm_pgoff_in_region << PAGE_SHIFT;
-	map_start = vma->vm_start - vm_off_in_region;
-	map_size = map->region->nr_pages << PAGE_SHIFT;
-
-	if ((uaddr + size) > (map_start + map_size))
-		/* Not within the CPU mapping */
-		return NULL;
-
-	*offset = (uaddr - vma->vm_start) + vm_off_in_region;
+	list_for_each(pos, &reg->cpu_alloc->mappings) {
+		map = list_entry(pos, struct kbase_cpu_mapping, mappings_list);
+		if (map->vm_start <= uaddr && map->vm_end >= uaddr + size)
+			return map;
+	}
 
-	return map;
+	return NULL;
 }
 
+KBASE_EXPORT_TEST_API(kbasep_find_enclosing_cpu_mapping_of_region);
+
 int kbasep_find_enclosing_cpu_mapping_offset(
-		struct kbase_context *kctx,
-		unsigned long uaddr, size_t size, u64 *offset)
+	struct kbase_context *kctx, u64 gpu_addr,
+	unsigned long uaddr, size_t size, u64 * offset)
 {
-	struct kbase_cpu_mapping *map;
+	struct kbase_cpu_mapping *map = NULL;
+	const struct kbase_va_region *reg;
+	int err = -EINVAL;
 
-	kbase_os_mem_map_lock(kctx);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
 
-	map = kbasep_find_enclosing_cpu_mapping(kctx, uaddr, size, offset);
+	kbase_gpu_vm_lock(kctx);
 
-	kbase_os_mem_map_unlock(kctx);
+	reg = kbase_region_tracker_find_region_enclosing_address(kctx, gpu_addr);
+	if (reg && !(reg->flags & KBASE_REG_FREE)) {
+		map = kbasep_find_enclosing_cpu_mapping_of_region(reg, uaddr,
+				size);
+		if (map) {
+			*offset = (uaddr - PTR_TO_U64(map->vm_start)) +
+						 (map->page_off << PAGE_SHIFT);
+			err = 0;
+		}
+	}
 
-	if (!map)
-		return -EINVAL;
+	kbase_gpu_vm_unlock(kctx);
 
-	return 0;
+	return err;
 }
 
 KBASE_EXPORT_TEST_API(kbasep_find_enclosing_cpu_mapping_offset);
@@ -1121,9 +1007,10 @@ void kbase_sync_single(struct kbase_context *kctx,
 }
 
 static int kbase_do_syncset(struct kbase_context *kctx,
-		struct basep_syncset *sset, enum kbase_sync_type sync_fn)
+		struct base_syncset *set, enum kbase_sync_type sync_fn)
 {
 	int err = 0;
+	struct basep_syncset *sset = &set->basep_sset;
 	struct kbase_va_region *reg;
 	struct kbase_cpu_mapping *map;
 	unsigned long start;
@@ -1132,7 +1019,7 @@ static int kbase_do_syncset(struct kbase_context *kctx,
 	phys_addr_t *gpu_pa;
 	u64 page_off, page_count;
 	u64 i;
-	u64 offset;
+	off_t offset;
 
 	kbase_os_mem_map_lock(kctx);
 	kbase_gpu_vm_lock(kctx);
@@ -1153,7 +1040,7 @@ static int kbase_do_syncset(struct kbase_context *kctx,
 	start = (uintptr_t)sset->user_addr;
 	size = (size_t)sset->size;
 
-	map = kbasep_find_enclosing_cpu_mapping(kctx, start, size, &offset);
+	map = kbasep_find_enclosing_cpu_mapping_of_region(reg, start, size);
 	if (!map) {
 		dev_warn(kctx->kbdev->dev, "Can't find CPU mapping 0x%016lX for VA 0x%016llX",
 				start, sset->mem_handle.basep.handle);
@@ -1161,19 +1048,12 @@ static int kbase_do_syncset(struct kbase_context *kctx,
 		goto out_unlock;
 	}
 
-	page_off = offset >> PAGE_SHIFT;
-	offset &= ~PAGE_MASK;
+	offset = start & (PAGE_SIZE - 1);
+	page_off = map->page_off + ((start - map->vm_start) >> PAGE_SHIFT);
 	page_count = (size + offset + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 	cpu_pa = kbase_get_cpu_phy_pages(reg);
 	gpu_pa = kbase_get_gpu_phy_pages(reg);
 
-	if (page_off > reg->nr_pages ||
-			page_off + page_count > reg->nr_pages) {
-		/* Sync overflows the region */
-		err = -EINVAL;
-		goto out_unlock;
-	}
-
 	/* Sync first page */
 	if (cpu_pa[page_off]) {
 		size_t sz = MIN(((size_t) PAGE_SIZE - offset), size);
@@ -1207,26 +1087,23 @@ out_unlock:
 	return err;
 }
 
-int kbase_sync_now(struct kbase_context *kctx, struct basep_syncset *sset)
+int kbase_sync_now(struct kbase_context *kctx, struct base_syncset *syncset)
 {
 	int err = -EINVAL;
+	struct basep_syncset *sset;
 
-	KBASE_DEBUG_ASSERT(kctx != NULL);
-	KBASE_DEBUG_ASSERT(sset != NULL);
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != syncset);
 
-	if (sset->mem_handle.basep.handle & ~PAGE_MASK) {
-		dev_warn(kctx->kbdev->dev,
-				"mem_handle: passed parameter is invalid");
-		return -EINVAL;
-	}
+	sset = &syncset->basep_sset;
 
 	switch (sset->type) {
 	case BASE_SYNCSET_OP_MSYNC:
-		err = kbase_do_syncset(kctx, sset, KBASE_SYNC_TO_DEVICE);
+		err = kbase_do_syncset(kctx, syncset, KBASE_SYNC_TO_DEVICE);
 		break;
 
 	case BASE_SYNCSET_OP_CSYNC:
-		err = kbase_do_syncset(kctx, sset, KBASE_SYNC_TO_CPU);
+		err = kbase_do_syncset(kctx, syncset, KBASE_SYNC_TO_CPU);
 		break;
 
 	default:
@@ -1297,11 +1174,6 @@ int kbase_mem_free(struct kbase_context *kctx, u64 gpu_addr)
 
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 
-	if ((gpu_addr & ~PAGE_MASK) && (gpu_addr >= PAGE_SIZE)) {
-		dev_warn(kctx->kbdev->dev, "kbase_mem_free: gpu_addr parameter is invalid");
-		return -EINVAL;
-	}
-
 	if (0 == gpu_addr) {
 		dev_warn(kctx->kbdev->dev, "gpu_addr 0 is reserved for the ringbuffer and it's an error to try to free it using kbase_mem_free\n");
 		return -EINVAL;
@@ -1352,7 +1224,7 @@ int kbase_mem_free(struct kbase_context *kctx, u64 gpu_addr)
 
 KBASE_EXPORT_TEST_API(kbase_mem_free);
 
-int kbase_update_region_flags(struct kbase_context *kctx,
+void kbase_update_region_flags(struct kbase_context *kctx,
 		struct kbase_va_region *reg, unsigned long flags)
 {
 	KBASE_DEBUG_ASSERT(NULL != reg);
@@ -1380,18 +1252,11 @@ int kbase_update_region_flags(struct kbase_context *kctx,
 	if (0 == (flags & BASE_MEM_PROT_GPU_EX))
 		reg->flags |= KBASE_REG_GPU_NX;
 
-	if (!kbase_device_is_cpu_coherent(kctx->kbdev)) {
-		if (flags & BASE_MEM_COHERENT_SYSTEM_REQUIRED)
-			return -EINVAL;
-	} else if (flags & (BASE_MEM_COHERENT_SYSTEM |
-			BASE_MEM_COHERENT_SYSTEM_REQUIRED)) {
+	if (flags & BASE_MEM_COHERENT_SYSTEM ||
+			flags & BASE_MEM_COHERENT_SYSTEM_REQUIRED)
 		reg->flags |= KBASE_REG_SHARE_BOTH;
-	}
-
-	if (!(reg->flags & KBASE_REG_SHARE_BOTH) &&
-			flags & BASE_MEM_COHERENT_LOCAL) {
+	else if (flags & BASE_MEM_COHERENT_LOCAL)
 		reg->flags |= KBASE_REG_SHARE_IN;
-	}
 
 	/* Set up default MEMATTR usage */
 	if (kctx->kbdev->system_coherency == COHERENCY_ACE &&
@@ -1402,9 +1267,8 @@ int kbase_update_region_flags(struct kbase_context *kctx,
 		reg->flags |=
 			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_DEFAULT);
 	}
-
-	return 0;
 }
+KBASE_EXPORT_TEST_API(kbase_update_region_flags);
 
 int kbase_alloc_phy_pages_helper(
 	struct kbase_mem_phy_alloc *alloc,
@@ -1440,7 +1304,7 @@ int kbase_alloc_phy_pages_helper(
 	if (kbase_zone_cache_update(alloc, old_page_count) != 0)
 		kbase_zone_cache_clear(alloc);
 
-	KBASE_TLSTREAM_AUX_PAGESALLOC(
+	kbase_tlstream_aux_pagesalloc(
 			(u32)alloc->imported.kctx->id,
 			(u64)new_page_count);
 
@@ -1505,7 +1369,7 @@ int kbase_free_phy_pages_helper(
 		kbase_atomic_sub_pages(nr_pages_to_free,
 				       &kctx->kbdev->memdev.used_pages);
 
-		KBASE_TLSTREAM_AUX_PAGESALLOC(
+		kbase_tlstream_aux_pagesalloc(
 				(u32)kctx->id,
 				(u64)new_page_count);
 	}
@@ -1799,7 +1663,7 @@ static int kbase_jit_debugfs_count_get(struct kbase_jit_debugfs_data *data)
 	struct kbase_context *kctx = data->kctx;
 	struct list_head *tmp;
 
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
 	list_for_each(tmp, &kctx->jit_active_head) {
 		data->active_value++;
 	}
@@ -1811,7 +1675,7 @@ static int kbase_jit_debugfs_count_get(struct kbase_jit_debugfs_data *data)
 	list_for_each(tmp, &kctx->jit_destroy_head) {
 		data->destroy_value++;
 	}
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->jit_lock);
 
 	return 0;
 }
@@ -1823,7 +1687,7 @@ static int kbase_jit_debugfs_vm_get(struct kbase_jit_debugfs_data *data)
 	struct kbase_context *kctx = data->kctx;
 	struct kbase_va_region *reg;
 
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
 	list_for_each_entry(reg, &kctx->jit_active_head, jit_node) {
 		data->active_value += reg->nr_pages;
 	}
@@ -1835,7 +1699,7 @@ static int kbase_jit_debugfs_vm_get(struct kbase_jit_debugfs_data *data)
 	list_for_each_entry(reg, &kctx->jit_destroy_head, jit_node) {
 		data->destroy_value += reg->nr_pages;
 	}
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->jit_lock);
 
 	return 0;
 }
@@ -1847,7 +1711,7 @@ static int kbase_jit_debugfs_phys_get(struct kbase_jit_debugfs_data *data)
 	struct kbase_context *kctx = data->kctx;
 	struct kbase_va_region *reg;
 
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
 	list_for_each_entry(reg, &kctx->jit_active_head, jit_node) {
 		data->active_value += reg->gpu_alloc->nents;
 	}
@@ -1859,7 +1723,7 @@ static int kbase_jit_debugfs_phys_get(struct kbase_jit_debugfs_data *data)
 	list_for_each_entry(reg, &kctx->jit_destroy_head, jit_node) {
 		data->destroy_value += reg->gpu_alloc->nents;
 	}
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->jit_lock);
 
 	return 0;
 }
@@ -1902,22 +1766,23 @@ static void kbase_jit_destroy_worker(struct work_struct *work)
 
 	kctx = container_of(work, struct kbase_context, jit_work);
 	do {
-		mutex_lock(&kctx->jit_evict_lock);
-		if (list_empty(&kctx->jit_destroy_head)) {
-			mutex_unlock(&kctx->jit_evict_lock);
-			break;
-		}
-
-		reg = list_first_entry(&kctx->jit_destroy_head,
+		mutex_lock(&kctx->jit_lock);
+		if (list_empty(&kctx->jit_destroy_head))
+			reg = NULL;
+		else
+			reg = list_first_entry(&kctx->jit_destroy_head,
 				struct kbase_va_region, jit_node);
 
-		list_del(&reg->jit_node);
-		mutex_unlock(&kctx->jit_evict_lock);
+		if (reg) {
+			list_del(&reg->jit_node);
+			mutex_unlock(&kctx->jit_lock);
 
-		kbase_gpu_vm_lock(kctx);
-		kbase_mem_free_region(kctx, reg);
-		kbase_gpu_vm_unlock(kctx);
-	} while (1);
+			kbase_gpu_vm_lock(kctx);
+			kbase_mem_free_region(kctx, reg);
+			kbase_gpu_vm_unlock(kctx);
+		} else
+			mutex_unlock(&kctx->jit_lock);
+	} while (reg);
 }
 
 int kbase_jit_init(struct kbase_context *kctx)
@@ -1925,11 +1790,9 @@ int kbase_jit_init(struct kbase_context *kctx)
 	INIT_LIST_HEAD(&kctx->jit_active_head);
 	INIT_LIST_HEAD(&kctx->jit_pool_head);
 	INIT_LIST_HEAD(&kctx->jit_destroy_head);
+	mutex_init(&kctx->jit_lock);
 	INIT_WORK(&kctx->jit_work, kbase_jit_destroy_worker);
 
-	INIT_LIST_HEAD(&kctx->jit_pending_alloc);
-	INIT_LIST_HEAD(&kctx->jit_atoms_head);
-
 	return 0;
 }
 
@@ -1943,7 +1806,7 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 
 	int ret;
 
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
 	/*
 	 * Scan the pool for an existing allocation which meets our
 	 * requirements and remove it.
@@ -1980,15 +1843,11 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 		 * Remove the found region from the pool and add it to the
 		 * active list.
 		 */
-		list_move(&reg->jit_node, &kctx->jit_active_head);
+		list_del_init(&reg->jit_node);
+		list_add(&reg->jit_node, &kctx->jit_active_head);
 
-		/*
-		 * Remove the allocation from the eviction list as it's no
-		 * longer eligible for eviction. This must be done before
-		 * dropping the jit_evict_lock
-		 */
-		list_del_init(&reg->gpu_alloc->evict_node);
-		mutex_unlock(&kctx->jit_evict_lock);
+		/* Release the jit lock before modifying the allocation */
+		mutex_unlock(&kctx->jit_lock);
 
 		kbase_gpu_vm_lock(kctx);
 
@@ -2032,17 +1891,18 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 				BASE_MEM_PROT_GPU_WR | BASE_MEM_GROW_ON_GPF |
 				BASE_MEM_COHERENT_LOCAL;
 		u64 gpu_addr;
+		u16 alignment;
 
-		mutex_unlock(&kctx->jit_evict_lock);
+		mutex_unlock(&kctx->jit_lock);
 
 		reg = kbase_mem_alloc(kctx, info->va_pages, info->commit_pages,
-				info->extent, &flags, &gpu_addr);
+				info->extent, &flags, &gpu_addr, &alignment);
 		if (!reg)
 			goto out_unlocked;
 
-		mutex_lock(&kctx->jit_evict_lock);
+		mutex_lock(&kctx->jit_lock);
 		list_add(&reg->jit_node, &kctx->jit_active_head);
-		mutex_unlock(&kctx->jit_evict_lock);
+		mutex_unlock(&kctx->jit_lock);
 	}
 
 	return reg;
@@ -2054,9 +1914,10 @@ update_failed:
 	 * the allocation to the pool and return the function with failure.
 	 */
 	kbase_gpu_vm_unlock(kctx);
-	mutex_lock(&kctx->jit_evict_lock);
-	list_move(&reg->jit_node, &kctx->jit_pool_head);
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
+	list_del_init(&reg->jit_node);
+	list_add(&reg->jit_node, &kctx->jit_pool_head);
+	mutex_unlock(&kctx->jit_lock);
 out_unlocked:
 	return NULL;
 }
@@ -2064,21 +1925,22 @@ out_unlocked:
 void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 {
 	/* The physical backing of memory in the pool is always reclaimable */
+	down_read(&kctx->process_mm->mmap_sem);
 	kbase_gpu_vm_lock(kctx);
 	kbase_mem_evictable_make(reg->gpu_alloc);
 	kbase_gpu_vm_unlock(kctx);
+	up_read(&kctx->process_mm->mmap_sem);
 
-	mutex_lock(&kctx->jit_evict_lock);
-	list_move(&reg->jit_node, &kctx->jit_pool_head);
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
+	list_del_init(&reg->jit_node);
+	list_add(&reg->jit_node, &kctx->jit_pool_head);
+	mutex_unlock(&kctx->jit_lock);
 }
 
 void kbase_jit_backing_lost(struct kbase_va_region *reg)
 {
 	struct kbase_context *kctx = reg->kctx;
 
-	lockdep_assert_held(&kctx->jit_evict_lock);
-
 	/*
 	 * JIT allocations will always be on a list, if the region
 	 * is not on a list then it's not a JIT allocation.
@@ -2091,7 +1953,10 @@ void kbase_jit_backing_lost(struct kbase_va_region *reg)
 	 * to take now, so move the allocation to the free list and kick
 	 * the worker which will do the freeing.
 	 */
-	list_move(&reg->jit_node, &kctx->jit_destroy_head);
+	mutex_lock(&kctx->jit_lock);
+	list_del_init(&reg->jit_node);
+	list_add(&reg->jit_node, &kctx->jit_destroy_head);
+	mutex_unlock(&kctx->jit_lock);
 
 	schedule_work(&kctx->jit_work);
 }
@@ -2103,13 +1968,13 @@ bool kbase_jit_evict(struct kbase_context *kctx)
 	lockdep_assert_held(&kctx->reg_lock);
 
 	/* Free the oldest allocation from the pool */
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->jit_lock);
 	if (!list_empty(&kctx->jit_pool_head)) {
 		reg = list_entry(kctx->jit_pool_head.prev,
 				struct kbase_va_region, jit_node);
 		list_del(&reg->jit_node);
 	}
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->jit_lock);
 
 	if (reg)
 		kbase_mem_free_region(kctx, reg);
@@ -2130,15 +1995,12 @@ void kbase_jit_term(struct kbase_context *kctx)
 	cancel_work_sync(&kctx->jit_work);
 
 	kbase_gpu_vm_lock(kctx);
-	mutex_lock(&kctx->jit_evict_lock);
 	/* Free all allocations from the pool */
 	while (!list_empty(&kctx->jit_pool_head)) {
 		walker = list_first_entry(&kctx->jit_pool_head,
 				struct kbase_va_region, jit_node);
 		list_del(&walker->jit_node);
-		mutex_unlock(&kctx->jit_evict_lock);
 		kbase_mem_free_region(kctx, walker);
-		mutex_lock(&kctx->jit_evict_lock);
 	}
 
 	/* Free all allocations from active list */
@@ -2146,11 +2008,8 @@ void kbase_jit_term(struct kbase_context *kctx)
 		walker = list_first_entry(&kctx->jit_active_head,
 				struct kbase_va_region, jit_node);
 		list_del(&walker->jit_node);
-		mutex_unlock(&kctx->jit_evict_lock);
 		kbase_mem_free_region(kctx, walker);
-		mutex_lock(&kctx->jit_evict_lock);
 	}
-	mutex_unlock(&kctx->jit_evict_lock);
 	kbase_gpu_vm_unlock(kctx);
 }
 
@@ -2184,24 +2043,12 @@ static int kbase_jd_user_buf_map(struct kbase_context *kctx,
 			alloc->imported.user_buf.nr_pages,
 			reg->flags & KBASE_REG_GPU_WR,
 			0, pages, NULL);
-#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+#else
 	pinned_pages = get_user_pages_remote(NULL, mm,
 			address,
 			alloc->imported.user_buf.nr_pages,
 			reg->flags & KBASE_REG_GPU_WR,
 			0, pages, NULL);
-#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
-	pinned_pages = get_user_pages_remote(NULL, mm,
-			address,
-			alloc->imported.user_buf.nr_pages,
-			reg->flags & KBASE_REG_GPU_WR ? FOLL_WRITE : 0,
-			pages, NULL);
-#else
-	pinned_pages = get_user_pages_remote(NULL, mm,
-			address,
-			alloc->imported.user_buf.nr_pages,
-			reg->flags & KBASE_REG_GPU_WR ? FOLL_WRITE : 0,
-			pages, NULL, NULL);
 #endif
 
 	if (pinned_pages <= 0)
@@ -2335,43 +2182,27 @@ static int kbase_jd_umm_map(struct kbase_context *kctx,
 		alloc->imported.umm.dma_buf->size);
 	}
 
-	if (!(reg->flags & KBASE_REG_IMPORT_PAD) &&
-			WARN_ONCE(count < reg->nr_pages,
+	if (WARN_ONCE(count < reg->nr_pages,
 			"sg list from dma_buf_map_attachment < dma_buf->size=%zu\n",
 			alloc->imported.umm.dma_buf->size)) {
 		err = -EINVAL;
-		goto err_unmap_attachment;
+		goto out;
 	}
 
 	/* Update nents as we now have pages to map */
-	alloc->nents = reg->nr_pages;
+	alloc->nents = count;
 
 	err = kbase_mmu_insert_pages(kctx, reg->start_pfn,
 			kbase_get_gpu_phy_pages(reg),
-			count,
+			kbase_reg_current_backed_size(reg),
 			reg->flags | KBASE_REG_GPU_WR | KBASE_REG_GPU_RD);
-	if (err)
-		goto err_unmap_attachment;
-
-	if (reg->flags & KBASE_REG_IMPORT_PAD) {
-		err = kbase_mmu_insert_single_page(kctx,
-				reg->start_pfn + count,
-				page_to_phys(kctx->aliasing_sink_page),
-				reg->nr_pages - count,
-				(reg->flags | KBASE_REG_GPU_RD) &
-				~KBASE_REG_GPU_WR);
-		if (err)
-			goto err_teardown_orig_pages;
-	}
-
-	return 0;
 
-err_teardown_orig_pages:
-	kbase_mmu_teardown_pages(kctx, reg->start_pfn, count);
-err_unmap_attachment:
-	dma_buf_unmap_attachment(alloc->imported.umm.dma_attachment,
-			alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
-	alloc->imported.umm.sgt = NULL;
+out:
+	if (err) {
+		dma_buf_unmap_attachment(alloc->imported.umm.dma_attachment,
+				alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
+		alloc->imported.umm.sgt = NULL;
+	}
 
 	return err;
 }
@@ -2496,15 +2327,11 @@ void kbase_unmap_external_resource(struct kbase_context *kctx,
 		alloc->imported.umm.current_mapping_usage_count--;
 
 		if (0 == alloc->imported.umm.current_mapping_usage_count) {
-			if (reg && reg->gpu_alloc == alloc) {
-				int err;
-
-				err = kbase_mmu_teardown_pages(
+			if (reg && reg->gpu_alloc == alloc)
+				kbase_mmu_teardown_pages(
 						kctx,
 						reg->start_pfn,
-						alloc->nents);
-				WARN_ON(err);
-			}
+						kbase_reg_current_backed_size(reg));
 
 			kbase_jd_umm_unmap(kctx, alloc);
 		}
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem.h b/drivers/gpu/arm/midgard/mali_kbase_mem.h
index e293577..8953c85 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -69,8 +69,10 @@ struct kbase_cpu_mapping {
 	struct   kbase_mem_phy_alloc *alloc;
 	struct   kbase_context *kctx;
 	struct   kbase_va_region *region;
+	pgoff_t  page_off;
 	int      count;
-	int      free_on_close;
+	unsigned long vm_start;
+	unsigned long vm_end;
 };
 
 enum kbase_memory_type {
@@ -151,31 +153,18 @@ struct kbase_mem_phy_alloc {
 		} alias;
 		/* Used by type = (KBASE_MEM_TYPE_NATIVE, KBASE_MEM_TYPE_TB) */
 		struct kbase_context *kctx;
-		struct kbase_alloc_import_user_buf {
+		struct {
 			unsigned long address;
 			unsigned long size;
 			unsigned long nr_pages;
 			struct page **pages;
-			/* top bit (1<<31) of current_mapping_usage_count
-			 * specifies that this import was pinned on import
-			 * See PINNED_ON_IMPORT
-			 */
-			u32 current_mapping_usage_count;
+			unsigned int current_mapping_usage_count;
 			struct mm_struct *mm;
 			dma_addr_t *dma_addrs;
 		} user_buf;
 	} imported;
 };
 
-/* The top bit of kbase_alloc_import_user_buf::current_mapping_usage_count is
- * used to signify that a buffer was pinned when it was imported. Since the
- * reference count is limited by the number of atoms that can be submitted at
- * once there should be no danger of overflowing into this bit.
- * Stealing the top bit also has the benefit that
- * current_mapping_usage_count != 0 if and only if the buffer is mapped.
- */
-#define PINNED_ON_IMPORT	(1<<31)
-
 static inline void kbase_mem_phy_alloc_gpu_mapped(struct kbase_mem_phy_alloc *alloc)
 {
 	KBASE_DEBUG_ASSERT(alloc);
@@ -259,6 +248,9 @@ struct kbase_va_region {
 /* CPU read access */
 #define KBASE_REG_CPU_RD            (1ul<<14)
 
+/* Aligned for GPU EX in SAME_VA */
+#define KBASE_REG_ALIGNED           (1ul<<15)
+
 /* Index of chosen MEMATTR for this region (0..7) */
 #define KBASE_REG_MEMATTR_MASK      (7ul << 16)
 #define KBASE_REG_MEMATTR_INDEX(x)  (((x) & 7) << 16)
@@ -268,9 +260,6 @@ struct kbase_va_region {
 
 #define KBASE_REG_DONT_NEED         (1ul << 20)
 
-/* Imported buffer is padded? */
-#define KBASE_REG_IMPORT_PAD        (1ul << 21)
-
 #define KBASE_REG_ZONE_SAME_VA      KBASE_REG_ZONE(0)
 
 /* only used with 32-bit clients */
@@ -637,20 +626,7 @@ int kbase_add_va_region(struct kbase_context *kctx, struct kbase_va_region *reg,
 
 bool kbase_check_alloc_flags(unsigned long flags);
 bool kbase_check_import_flags(unsigned long flags);
-
-/**
- * kbase_update_region_flags - Convert user space flags to kernel region flags
- *
- * @kctx:  kbase context
- * @reg:   The region to update the flags on
- * @flags: The flags passed from user space
- *
- * The user space flag BASE_MEM_COHERENT_SYSTEM_REQUIRED will be rejected and
- * this function will fail if the system does not support system coherency.
- *
- * Return: 0 if successful, -EINVAL if the flags are not supported
- */
-int kbase_update_region_flags(struct kbase_context *kctx,
+void kbase_update_region_flags(struct kbase_context *kctx,
 		struct kbase_va_region *reg, unsigned long flags);
 
 void kbase_gpu_vm_lock(struct kbase_context *kctx);
@@ -742,16 +718,7 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat);
  */
 void *kbase_mmu_dump(struct kbase_context *kctx, int nr_pages);
 
-/**
- * kbase_sync_now - Perform cache maintenance on a memory region
- *
- * @kctx: The kbase context of the region
- * @sset: A syncset structure describing the region and direction of the
- *        synchronisation required
- *
- * Return: 0 on success or error code
- */
-int kbase_sync_now(struct kbase_context *kctx, struct basep_syncset *sset);
+int kbase_sync_now(struct kbase_context *kctx, struct base_syncset *syncset);
 void kbase_sync_single(struct kbase_context *kctx, phys_addr_t cpu_pa,
 		phys_addr_t gpu_pa, off_t offset, size_t size,
 		enum kbase_sync_type sync_fn);
@@ -807,24 +774,31 @@ static inline void kbase_process_page_usage_dec(struct kbase_context *kctx, int
 }
 
 /**
- * kbasep_find_enclosing_cpu_mapping_offset() - Find the offset of the CPU
- * mapping of a memory allocation containing a given address range
- *
- * Searches for a CPU mapping of any part of any region that fully encloses the
- * CPU virtual address range specified by @uaddr and @size. Returns a failure
- * indication if only part of the address range lies within a CPU mapping.
- *
- * @kctx:      The kernel base context used for the allocation.
- * @uaddr:     Start of the CPU virtual address range.
- * @size:      Size of the CPU virtual address range (in bytes).
- * @offset:    The offset from the start of the allocation to the specified CPU
- *             virtual address.
- *
- * Return: 0 if offset was obtained successfully. Error code otherwise.
+ * @brief Find the offset of the CPU mapping of a memory allocation containing
+ *        a given address range
+ *
+ * Searches for a CPU mapping of any part of the region starting at @p gpu_addr
+ * that fully encloses the CPU virtual address range specified by @p uaddr and
+ * @p size. Returns a failure indication if only part of the address range lies
+ * within a CPU mapping, or the address range lies within a CPU mapping of a
+ * different region.
+ *
+ * @param[in,out] kctx      The kernel base context used for the allocation.
+ * @param[in]     gpu_addr  GPU address of the start of the allocated region
+ *                          within which to search.
+ * @param[in]     uaddr     Start of the CPU virtual address range.
+ * @param[in]     size      Size of the CPU virtual address range (in bytes).
+ * @param[out]    offset    The offset from the start of the allocation to the
+ *                          specified CPU virtual address.
+ *
+ * @return 0 if offset was obtained successfully. Error code
+ *         otherwise.
  */
-int kbasep_find_enclosing_cpu_mapping_offset(
-		struct kbase_context *kctx,
-		unsigned long uaddr, size_t size, u64 *offset);
+int kbasep_find_enclosing_cpu_mapping_offset(struct kbase_context *kctx,
+							u64 gpu_addr,
+							unsigned long uaddr,
+							size_t size,
+							u64 *offset);
 
 enum hrtimer_restart kbasep_as_poke_timer_callback(struct hrtimer *timer);
 void kbase_as_poking_timer_retain_atom(struct kbase_device *kbdev, struct kbase_context *kctx, struct kbase_jd_atom *katom);
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c b/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
index eea429a..b6dac55 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem_linux.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -47,6 +47,7 @@
 #include <mali_kbase_tlstream.h>
 
 static int kbase_tracking_page_setup(struct kbase_context *kctx, struct vm_area_struct *vma);
+static const struct vm_operations_struct kbase_vm_ops;
 
 /**
  * kbase_mem_shrink_cpu_mapping - Shrink the CPU mapping(s) of an allocation
@@ -55,12 +56,14 @@ static int kbase_tracking_page_setup(struct kbase_context *kctx, struct vm_area_
  * @new_pages: The number of pages after the shrink
  * @old_pages: The number of pages before the shrink
  *
+ * Return: 0 on success, -errno on error.
+ *
  * Shrink (or completely remove) all CPU mappings which reference the shrunk
  * part of the allocation.
  *
  * Note: Caller must be holding the processes mmap_sem lock.
  */
-static void kbase_mem_shrink_cpu_mapping(struct kbase_context *kctx,
+static int kbase_mem_shrink_cpu_mapping(struct kbase_context *kctx,
 		struct kbase_va_region *reg,
 		u64 new_pages, u64 old_pages);
 
@@ -81,9 +84,7 @@ static int kbase_mem_shrink_gpu_mapping(struct kbase_context *kctx,
 		struct kbase_va_region *reg,
 		u64 new_pages, u64 old_pages);
 
-struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
-		u64 va_pages, u64 commit_pages, u64 extent, u64 *flags,
-		u64 *gpu_va)
+struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages, u64 commit_pages, u64 extent, u64 *flags, u64 *gpu_va, u16 *va_alignment)
 {
 	int zone;
 	int gpu_pc_bits;
@@ -94,8 +95,10 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 	KBASE_DEBUG_ASSERT(kctx);
 	KBASE_DEBUG_ASSERT(flags);
 	KBASE_DEBUG_ASSERT(gpu_va);
+	KBASE_DEBUG_ASSERT(va_alignment);
 
 	dev = kctx->kbdev->dev;
+	*va_alignment = 0; /* no alignment by default */
 	*gpu_va = 0; /* return 0 on failure */
 
 	gpu_pc_bits = kctx->kbdev->gpu_props.props.core_props.log2_program_counter_size;
@@ -152,8 +155,7 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 		goto no_region;
 	}
 
-	if (kbase_update_region_flags(kctx, reg, *flags) != 0)
-		goto invalid_flags;
+	kbase_update_region_flags(kctx, reg, *flags);
 
 	if (kbase_reg_prepare_native(reg, kctx) != 0) {
 		dev_err(dev, "Failed to prepare region");
@@ -200,6 +202,13 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 		cookie = cookie_nr + PFN_DOWN(BASE_MEM_COOKIE_BASE);
 		cookie <<= PAGE_SHIFT;
 
+		/* See if we must align memory due to GPU PC bits vs CPU VA */
+		if ((*flags & BASE_MEM_PROT_GPU_EX) &&
+		    (cpu_va_bits > gpu_pc_bits)) {
+			*va_alignment = gpu_pc_bits;
+			reg->flags |= KBASE_REG_ALIGNED;
+		}
+
 		/*
 		 * 10.1-10.4 UKU userland relies on the kernel to call mmap.
 		 * For all other versions we can just return the cookie
@@ -209,6 +218,15 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 			*gpu_va = (u64) cookie;
 			return reg;
 		}
+
+		/*
+		 * To achieve alignment and avoid allocating on large alignment
+		 * (to work around a GPU hardware issue) we must allocate 3
+		 * times the required size.
+		 */
+		if (*va_alignment)
+			va_map += 3 * (1UL << *va_alignment);
+
 		if (*flags & BASE_MEM_PROT_CPU_RD)
 			prot |= PROT_READ;
 		if (*flags & BASE_MEM_PROT_CPU_WR)
@@ -225,7 +243,53 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 			goto no_mmap;
 		}
 
-		*gpu_va = (u64) cpu_addr;
+		/*
+		 * If we had to allocate extra VA space to force the
+		 * alignment release it.
+		 */
+		if (*va_alignment) {
+			unsigned long alignment = 1UL << *va_alignment;
+			unsigned long align_mask = alignment - 1;
+			unsigned long addr;
+			unsigned long addr_end;
+			unsigned long aligned_addr;
+			unsigned long aligned_addr_end;
+
+			addr = cpu_addr;
+			addr_end = addr + va_map;
+
+			aligned_addr = (addr + align_mask) &
+					~((u64) align_mask);
+			aligned_addr_end = aligned_addr + va_size;
+
+			if ((aligned_addr_end & BASE_MEM_MASK_4GB) == 0) {
+				/*
+				 * Can't end at 4GB boundary on some GPUs as
+				 * it will halt the shader.
+				 */
+				aligned_addr += 2 * alignment;
+				aligned_addr_end += 2 * alignment;
+			} else if ((aligned_addr & BASE_MEM_MASK_4GB) == 0) {
+				/*
+				 * Can't start at 4GB boundary on some GPUs as
+				 * it will halt the shader.
+				 */
+				aligned_addr += alignment;
+				aligned_addr_end += alignment;
+			}
+
+			/* anything to chop off at the start? */
+			if (addr != aligned_addr)
+				vm_munmap(addr, aligned_addr - addr);
+
+			/* anything at the end? */
+			if (addr_end != aligned_addr_end)
+				vm_munmap(aligned_addr_end,
+						addr_end - aligned_addr_end);
+
+			*gpu_va = (u64) aligned_addr;
+		} else
+			*gpu_va = (u64) cpu_addr;
 	} else /* we control the VA */ {
 		if (kbase_gpu_mmap(kctx, reg, 0, va_pages, 1) != 0) {
 			dev_warn(dev, "Failed to map memory on GPU");
@@ -245,7 +309,6 @@ no_cookie:
 no_mem:
 	kbase_mem_phy_alloc_put(reg->cpu_alloc);
 	kbase_mem_phy_alloc_put(reg->gpu_alloc);
-invalid_flags:
 prepare_failed:
 	kfree(reg);
 no_region:
@@ -264,11 +327,6 @@ int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, int query, u64 * c
 	KBASE_DEBUG_ASSERT(kctx);
 	KBASE_DEBUG_ASSERT(out);
 
-	if (gpu_addr & ~PAGE_MASK) {
-		dev_warn(kctx->kbdev->dev, "mem_query: gpu_addr: passed parameter is invalid");
-		return -EINVAL;
-	}
-
 	kbase_gpu_vm_lock(kctx);
 
 	/* Validate the region */
@@ -343,12 +401,12 @@ unsigned long kbase_mem_evictable_reclaim_count_objects(struct shrinker *s,
 
 	kctx = container_of(s, struct kbase_context, reclaim);
 
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->evict_lock);
 
 	list_for_each_entry(alloc, &kctx->evict_list, evict_node)
 		pages += alloc->nents;
 
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->evict_lock);
 	return pages;
 }
 
@@ -381,7 +439,7 @@ unsigned long kbase_mem_evictable_reclaim_scan_objects(struct shrinker *s,
 	unsigned long freed = 0;
 
 	kctx = container_of(s, struct kbase_context, reclaim);
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->evict_lock);
 
 	list_for_each_entry_safe(alloc, tmp, &kctx->evict_list, evict_node) {
 		int err;
@@ -420,7 +478,7 @@ unsigned long kbase_mem_evictable_reclaim_scan_objects(struct shrinker *s,
 			break;
 	}
 out_unlock:
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->evict_lock);
 
 	return freed;
 }
@@ -439,7 +497,7 @@ static int kbase_mem_evictable_reclaim_shrink(struct shrinker *s,
 int kbase_mem_evictable_init(struct kbase_context *kctx)
 {
 	INIT_LIST_HEAD(&kctx->evict_list);
-	mutex_init(&kctx->jit_evict_lock);
+	mutex_init(&kctx->evict_lock);
 
 	/* Register shrinker */
 #if LINUX_VERSION_CODE < KERNEL_VERSION(3, 12, 0)
@@ -593,7 +651,7 @@ static void kbase_mem_evictable_mark_reclaim(struct kbase_mem_phy_alloc *alloc)
 						&kctx->used_pages);
 	kbase_atomic_sub_pages(alloc->nents, &kctx->kbdev->memdev.used_pages);
 
-	KBASE_TLSTREAM_AUX_PAGESALLOC(
+	kbase_tlstream_aux_pagesalloc(
 			(u32)kctx->id,
 			(u64)new_page_count);
 }
@@ -639,7 +697,7 @@ void kbase_mem_evictable_unmark_reclaim(struct kbase_mem_phy_alloc *alloc)
 		}
 	}
 
-	KBASE_TLSTREAM_AUX_PAGESALLOC(
+	kbase_tlstream_aux_pagesalloc(
 			(u32)kctx->id,
 			(u64)new_page_count);
 }
@@ -647,22 +705,29 @@ void kbase_mem_evictable_unmark_reclaim(struct kbase_mem_phy_alloc *alloc)
 int kbase_mem_evictable_make(struct kbase_mem_phy_alloc *gpu_alloc)
 {
 	struct kbase_context *kctx = gpu_alloc->imported.kctx;
+	int err;
 
 	lockdep_assert_held(&kctx->reg_lock);
 
 	/* This alloction can't already be on a list. */
 	WARN_ON(!list_empty(&gpu_alloc->evict_node));
 
-	kbase_mem_shrink_cpu_mapping(kctx, gpu_alloc->reg,
+	/*
+	 * Try to shrink the CPU mappings as required, if we fail then
+	 * fail the process of making this allocation evictable.
+	 */
+	err = kbase_mem_shrink_cpu_mapping(kctx, gpu_alloc->reg,
 			0, gpu_alloc->nents);
+	if (err)
+		return -EINVAL;
 
 	/*
 	 * Add the allocation to the eviction list, after this point the shrink
 	 * can reclaim it.
 	 */
-	mutex_lock(&kctx->jit_evict_lock);
+	mutex_lock(&kctx->evict_lock);
 	list_add(&gpu_alloc->evict_node, &kctx->evict_list);
-	mutex_unlock(&kctx->jit_evict_lock);
+	mutex_unlock(&kctx->evict_lock);
 	kbase_mem_evictable_mark_reclaim(gpu_alloc);
 
 	gpu_alloc->reg->flags |= KBASE_REG_DONT_NEED;
@@ -680,7 +745,9 @@ bool kbase_mem_evictable_unmake(struct kbase_mem_phy_alloc *gpu_alloc)
 	 * First remove the allocation from the eviction list as it's no
 	 * longer eligible for eviction.
 	 */
+	mutex_lock(&kctx->evict_lock);
 	list_del_init(&gpu_alloc->evict_node);
+	mutex_unlock(&kctx->evict_lock);
 
 	if (gpu_alloc->evicted == 0) {
 		/*
@@ -728,9 +795,6 @@ int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, unsigned in
 	if (!gpu_addr)
 		return -EINVAL;
 
-	if ((gpu_addr & ~PAGE_MASK) && (gpu_addr >= PAGE_SIZE))
-		return -EINVAL;
-
 	/* nuke other bits */
 	flags &= mask;
 
@@ -947,8 +1011,7 @@ bad_flags:
 #endif				/* CONFIG_UMP */
 
 #ifdef CONFIG_DMA_SHARED_BUFFER
-static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx,
-		int fd, u64 *va_pages, u64 *flags, u32 padding)
+static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx, int fd, u64 *va_pages, u64 *flags)
 {
 	struct kbase_va_region *reg;
 	struct dma_buf *dma_buf;
@@ -963,7 +1026,7 @@ static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx,
 	if (!dma_attachment)
 		goto no_attachment;
 
-	*va_pages = (PAGE_ALIGN(dma_buf->size) >> PAGE_SHIFT) + padding;
+	*va_pages = PAGE_ALIGN(dma_buf->size) >> PAGE_SHIFT;
 	if (!*va_pages)
 		goto bad_size;
 
@@ -1006,19 +1069,27 @@ static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx,
 	/* No pages to map yet */
 	reg->gpu_alloc->nents = 0;
 
-	if (kbase_update_region_flags(kctx, reg, *flags) != 0)
-		goto invalid_flags;
-
 	reg->flags &= ~KBASE_REG_FREE;
 	reg->flags |= KBASE_REG_GPU_NX;	/* UMM is always No eXecute */
 	reg->flags &= ~KBASE_REG_GROWABLE;	/* UMM cannot be grown */
 	reg->flags |= KBASE_REG_GPU_CACHED;
 
+	if (*flags & BASE_MEM_PROT_CPU_WR)
+		reg->flags |= KBASE_REG_CPU_WR;
+
+	if (*flags & BASE_MEM_PROT_CPU_RD)
+		reg->flags |= KBASE_REG_CPU_RD;
+
+	if (*flags & BASE_MEM_PROT_GPU_WR)
+		reg->flags |= KBASE_REG_GPU_WR;
+
+	if (*flags & BASE_MEM_PROT_GPU_RD)
+		reg->flags |= KBASE_REG_GPU_RD;
+
 	if (*flags & BASE_MEM_SECURE)
 		reg->flags |= KBASE_REG_SECURE;
 
-	if (padding)
-		reg->flags |= KBASE_REG_IMPORT_PAD;
+	/* no read or write permission given on import, only on run do we give the right permissions */
 
 	reg->gpu_alloc->type = KBASE_MEM_TYPE_IMPORTED_UMM;
 	reg->gpu_alloc->imported.umm.sgt = NULL;
@@ -1029,8 +1100,6 @@ static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx,
 
 	return reg;
 
-invalid_flags:
-	kbase_mem_phy_alloc_put(reg->gpu_alloc);
 no_alloc_obj:
 	kfree(reg);
 no_region:
@@ -1043,45 +1112,15 @@ no_buf:
 }
 #endif  /* CONFIG_DMA_SHARED_BUFFER */
 
-static u32 kbase_get_cache_line_alignment(struct kbase_context *kctx)
-{
-	u32 cpu_cache_line_size = cache_line_size();
-	u32 gpu_cache_line_size =
-		(1UL << kctx->kbdev->gpu_props.props.l2_props.log2_line_size);
-
-	return ((cpu_cache_line_size > gpu_cache_line_size) ?
-				cpu_cache_line_size :
-				gpu_cache_line_size);
-}
 
 static struct kbase_va_region *kbase_mem_from_user_buffer(
 		struct kbase_context *kctx, unsigned long address,
 		unsigned long size, u64 *va_pages, u64 *flags)
 {
-	long i;
 	struct kbase_va_region *reg;
 	long faulted_pages;
 	int zone = KBASE_REG_ZONE_CUSTOM_VA;
 	bool shared_zone = false;
-	u32 cache_line_alignment = kbase_get_cache_line_alignment(kctx);
-	struct kbase_alloc_import_user_buf *user_buf;
-	struct page **pages = NULL;
-
-	if ((address & (cache_line_alignment - 1)) != 0 ||
-			(size & (cache_line_alignment - 1)) != 0) {
-		/* Coherency must be enabled to handle partial cache lines */
-		if (*flags & (BASE_MEM_COHERENT_SYSTEM |
-			BASE_MEM_COHERENT_SYSTEM_REQUIRED)) {
-			/* Force coherent system required flag, import will
-			 * then fail if coherency isn't available
-			 */
-			*flags |= BASE_MEM_COHERENT_SYSTEM_REQUIRED;
-		} else {
-			dev_warn(kctx->kbdev->dev,
-					"User buffer is not cache line aligned and no coherency enabled\n");
-			goto bad_size;
-		}
-	}
 
 	*va_pages = (PAGE_ALIGN(address + size) >> PAGE_SHIFT) -
 		PFN_DOWN(address);
@@ -1125,109 +1164,61 @@ static struct kbase_va_region *kbase_mem_from_user_buffer(
 
 	reg->cpu_alloc = kbase_mem_phy_alloc_get(reg->gpu_alloc);
 
-	if (kbase_update_region_flags(kctx, reg, *flags) != 0)
-		goto invalid_flags;
-
 	reg->flags &= ~KBASE_REG_FREE;
 	reg->flags |= KBASE_REG_GPU_NX; /* User-buffers are always No eXecute */
 	reg->flags &= ~KBASE_REG_GROWABLE; /* Cannot be grown */
-	reg->flags &= ~KBASE_REG_CPU_CACHED;
 
-	user_buf = &reg->gpu_alloc->imported.user_buf;
+	if (*flags & BASE_MEM_PROT_CPU_WR)
+		reg->flags |= KBASE_REG_CPU_WR;
 
-	user_buf->size = size;
-	user_buf->address = address;
-	user_buf->nr_pages = *va_pages;
-	user_buf->mm = current->mm;
-	user_buf->pages = kmalloc_array(*va_pages, sizeof(struct page *),
-			GFP_KERNEL);
+	if (*flags & BASE_MEM_PROT_CPU_RD)
+		reg->flags |= KBASE_REG_CPU_RD;
 
-	if (!user_buf->pages)
-		goto no_page_array;
+	if (*flags & BASE_MEM_PROT_GPU_WR)
+		reg->flags |= KBASE_REG_GPU_WR;
 
-	/* If the region is coherent with the CPU then the memory is imported
-	 * and mapped onto the GPU immediately.
-	 * Otherwise get_user_pages is called as a sanity check, but with
-	 * NULL as the pages argument which will fault the pages, but not
-	 * pin them. The memory will then be pinned only around the jobs that
-	 * specify the region as an external resource.
-	 */
-	if (reg->flags & KBASE_REG_SHARE_BOTH) {
-		pages = user_buf->pages;
-		*flags |= KBASE_MEM_IMPORT_HAVE_PAGES;
-	}
+	if (*flags & BASE_MEM_PROT_GPU_RD)
+		reg->flags |= KBASE_REG_GPU_RD;
 
 	down_read(&current->mm->mmap_sem);
 
+	/* A sanity check that get_user_pages will work on the memory */
+	/* (so the initial import fails on weird memory regions rather than */
+	/* the job failing when we try to handle the external resources). */
+	/* It doesn't take a reference to the pages (because the page list is NULL). */
+	/* We can't really store the page list because that would involve */
+	/* keeping the pages pinned - instead we pin/unpin around the job */
+	/* (as part of the external resources handling code) */
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
 	faulted_pages = get_user_pages(current, current->mm, address, *va_pages,
-			reg->flags & KBASE_REG_GPU_WR, 0, pages, NULL);
-#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
-	faulted_pages = get_user_pages(address, *va_pages,
-			reg->flags & KBASE_REG_GPU_WR, 0, pages, NULL);
+			reg->flags & KBASE_REG_GPU_WR, 0, NULL, NULL);
 #else
 	faulted_pages = get_user_pages(address, *va_pages,
-			reg->flags & KBASE_REG_GPU_WR ? FOLL_WRITE : 0,
-			pages, NULL);
+			reg->flags & KBASE_REG_GPU_WR, 0, NULL, NULL);
 #endif
-
 	up_read(&current->mm->mmap_sem);
 
 	if (faulted_pages != *va_pages)
 		goto fault_mismatch;
 
+	reg->gpu_alloc->imported.user_buf.size = size;
+	reg->gpu_alloc->imported.user_buf.address = address;
+	reg->gpu_alloc->imported.user_buf.nr_pages = faulted_pages;
+	reg->gpu_alloc->imported.user_buf.pages = kmalloc_array(faulted_pages,
+			sizeof(struct page *), GFP_KERNEL);
+	reg->gpu_alloc->imported.user_buf.mm = current->mm;
 	atomic_inc(&current->mm->mm_count);
 
+	if (!reg->gpu_alloc->imported.user_buf.pages)
+		goto no_page_array;
+
 	reg->gpu_alloc->nents = 0;
 	reg->extent = 0;
 
-	if (pages) {
-		struct device *dev = kctx->kbdev->dev;
-		unsigned long local_size = user_buf->size;
-		unsigned long offset = user_buf->address & ~PAGE_MASK;
-		phys_addr_t *pa = kbase_get_gpu_phy_pages(reg);
-
-		/* Top bit signifies that this was pinned on import */
-		user_buf->current_mapping_usage_count |= PINNED_ON_IMPORT;
-
-		for (i = 0; i < faulted_pages; i++) {
-			dma_addr_t dma_addr;
-			unsigned long min;
-
-			min = MIN(PAGE_SIZE - offset, local_size);
-			dma_addr = dma_map_page(dev, pages[i],
-					offset, min,
-					DMA_BIDIRECTIONAL);
-			if (dma_mapping_error(dev, dma_addr))
-				goto unwind_dma_map;
-
-			user_buf->dma_addrs[i] = dma_addr;
-			pa[i] = page_to_phys(pages[i]);
-
-			local_size -= min;
-			offset = 0;
-		}
-
-		reg->gpu_alloc->nents = faulted_pages;
-	}
-
 	return reg;
 
-unwind_dma_map:
-	while (i--) {
-		dma_unmap_page(kctx->kbdev->dev,
-				user_buf->dma_addrs[i],
-				PAGE_SIZE, DMA_BIDIRECTIONAL);
-	}
-fault_mismatch:
-	if (pages) {
-		for (i = 0; i < faulted_pages; i++)
-			put_page(pages[i]);
-	}
-	kfree(user_buf->pages);
 no_page_array:
-invalid_flags:
-	kbase_mem_phy_alloc_put(reg->cpu_alloc);
+fault_mismatch:
 	kbase_mem_phy_alloc_put(reg->gpu_alloc);
 no_alloc_obj:
 	kfree(reg);
@@ -1304,8 +1295,7 @@ u64 kbase_mem_alias(struct kbase_context *kctx, u64 *flags, u64 stride,
 
 	reg->cpu_alloc = kbase_mem_phy_alloc_get(reg->gpu_alloc);
 
-	if (kbase_update_region_flags(kctx, reg, *flags) != 0)
-		goto invalid_flags;
+	kbase_update_region_flags(kctx, reg, *flags);
 
 	reg->gpu_alloc->imported.alias.nents = nents;
 	reg->gpu_alloc->imported.alias.stride = stride;
@@ -1414,7 +1404,6 @@ no_mmap:
 bad_handle:
 	kbase_gpu_vm_unlock(kctx);
 no_aliased_array:
-invalid_flags:
 	kbase_mem_phy_alloc_put(reg->cpu_alloc);
 	kbase_mem_phy_alloc_put(reg->gpu_alloc);
 no_alloc_obj:
@@ -1427,8 +1416,34 @@ bad_flags:
 	return 0;
 }
 
+static u32 kbase_get_cache_line_alignment(struct kbase_context *kctx)
+{
+	u32 cpu_cache_line_size = cache_line_size();
+	u32 gpu_cache_line_size =
+		(1UL << kctx->kbdev->gpu_props.props.l2_props.log2_line_size);
+
+	return ((cpu_cache_line_size > gpu_cache_line_size) ?
+				cpu_cache_line_size :
+				gpu_cache_line_size);
+}
+
+static int kbase_check_buffer_size(struct kbase_context *kctx, u64 size)
+{
+	u32 cache_line_align = kbase_get_cache_line_alignment(kctx);
+
+	return (size & (cache_line_align - 1)) == 0 ? 0 : -EINVAL;
+}
+
+static int kbase_check_buffer_cache_alignment(struct kbase_context *kctx,
+					void __user *ptr)
+{
+	u32 cache_line_align = kbase_get_cache_line_alignment(kctx);
+
+	return ((uintptr_t)ptr & (cache_line_align - 1)) == 0 ? 0 : -EINVAL;
+}
+
 int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
-		void __user *phandle, u32 padding, u64 *gpu_va, u64 *va_pages,
+		void __user *phandle, u64 *gpu_va, u64 *va_pages,
 		u64 *flags)
 {
 	struct kbase_va_region *reg;
@@ -1450,24 +1465,6 @@ int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
 		goto bad_flags;
 	}
 
-	if ((*flags & BASE_MEM_COHERENT_SYSTEM_REQUIRED) != 0 &&
-			!kbase_device_is_cpu_coherent(kctx->kbdev)) {
-		dev_warn(kctx->kbdev->dev,
-				"kbase_mem_import call required coherent mem when unavailable");
-		goto bad_flags;
-	}
-	if ((*flags & BASE_MEM_COHERENT_SYSTEM) != 0 &&
-			!kbase_device_is_cpu_coherent(kctx->kbdev)) {
-		/* Remove COHERENT_SYSTEM flag if coherent mem is unavailable */
-		*flags &= ~BASE_MEM_COHERENT_SYSTEM;
-	}
-
-	if ((padding != 0) && (type != BASE_MEM_IMPORT_TYPE_UMM)) {
-		dev_warn(kctx->kbdev->dev,
-				"padding is only supported for UMM");
-		goto bad_flags;
-	}
-
 	switch (type) {
 #ifdef CONFIG_UMP
 	case BASE_MEM_IMPORT_TYPE_UMP: {
@@ -1487,8 +1484,7 @@ int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
 		if (get_user(fd, (int __user *)phandle))
 			reg = NULL;
 		else
-			reg = kbase_mem_from_umm(kctx, fd, va_pages, flags,
-					padding);
+			reg = kbase_mem_from_umm(kctx, fd, va_pages, flags);
 	}
 	break;
 #endif /* CONFIG_DMA_SHARED_BUFFER */
@@ -1507,6 +1503,20 @@ int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
 #endif
 				uptr = user_buffer.ptr.value;
 
+			if (0 != kbase_check_buffer_cache_alignment(kctx,
+									uptr)) {
+				dev_warn(kctx->kbdev->dev,
+				"User buffer is not cache line aligned!\n");
+				goto no_reg;
+			}
+
+			if (0 != kbase_check_buffer_size(kctx,
+					user_buffer.length)) {
+				dev_warn(kctx->kbdev->dev,
+				"User buffer size is not multiple of cache line size!\n");
+				goto no_reg;
+			}
+
 			reg = kbase_mem_from_user_buffer(kctx,
 					(unsigned long)uptr, user_buffer.length,
 					va_pages, flags);
@@ -1574,6 +1584,48 @@ bad_flags:
 	return -ENOMEM;
 }
 
+
+static int zap_range_nolock(struct mm_struct *mm,
+		const struct vm_operations_struct *vm_ops,
+		unsigned long start, unsigned long end)
+{
+	struct vm_area_struct *vma;
+	int err = -EINVAL; /* in case end < start */
+
+	while (start < end) {
+		unsigned long local_start;
+		unsigned long local_end;
+
+		vma = find_vma_intersection(mm, start, end);
+		if (!vma)
+			break;
+
+		/* is it ours? */
+		if (vma->vm_ops != vm_ops)
+			goto try_next;
+
+		local_start = vma->vm_start;
+
+		if (start > local_start)
+			local_start = start;
+
+		local_end = vma->vm_end;
+
+		if (end < local_end)
+			local_end = end;
+
+		err = zap_vma_ptes(vma, local_start, local_end - local_start);
+		if (unlikely(err))
+			break;
+
+try_next:
+		/* go to next vma, if any */
+		start = vma->vm_end;
+	}
+
+	return err;
+}
+
 int kbase_mem_grow_gpu_mapping(struct kbase_context *kctx,
 		struct kbase_va_region *reg,
 		u64 new_pages, u64 old_pages)
@@ -1592,19 +1644,52 @@ int kbase_mem_grow_gpu_mapping(struct kbase_context *kctx,
 	return ret;
 }
 
-static void kbase_mem_shrink_cpu_mapping(struct kbase_context *kctx,
+static int kbase_mem_shrink_cpu_mapping(struct kbase_context *kctx,
 		struct kbase_va_region *reg,
 		u64 new_pages, u64 old_pages)
 {
-	u64 gpu_va_start = reg->start_pfn;
+	struct kbase_mem_phy_alloc *cpu_alloc = reg->cpu_alloc;
+	struct kbase_cpu_mapping *mapping;
+	int err;
 
-	if (new_pages == old_pages)
-		/* Nothing to do */
-		return;
+	lockdep_assert_held(&kctx->process_mm->mmap_sem);
+
+	list_for_each_entry(mapping, &cpu_alloc->mappings, mappings_list) {
+		unsigned long mapping_size;
+
+		mapping_size = (mapping->vm_end - mapping->vm_start)
+				>> PAGE_SHIFT;
+
+		/* is this mapping affected ?*/
+		if ((mapping->page_off + mapping_size) > new_pages) {
+			unsigned long first_bad = 0;
 
-	unmap_mapping_range(kctx->filp->f_inode->i_mapping,
-			(gpu_va_start + new_pages)<<PAGE_SHIFT,
-			(old_pages - new_pages)<<PAGE_SHIFT, 1);
+			if (new_pages > mapping->page_off)
+				first_bad = new_pages - mapping->page_off;
+
+			err = zap_range_nolock(current->mm,
+					&kbase_vm_ops,
+					mapping->vm_start +
+					(first_bad << PAGE_SHIFT),
+					mapping->vm_end);
+
+			WARN(err,
+			     "Failed to zap VA range (0x%lx - 0x%lx);\n",
+			     mapping->vm_start +
+			     (first_bad << PAGE_SHIFT),
+			     mapping->vm_end
+			     );
+
+			/* The zap failed, give up and exit */
+			if (err)
+				goto failed;
+		}
+	}
+
+	return 0;
+
+failed:
+	return err;
 }
 
 static int kbase_mem_shrink_gpu_mapping(struct kbase_context *kctx,
@@ -1620,7 +1705,7 @@ static int kbase_mem_shrink_gpu_mapping(struct kbase_context *kctx,
 	return ret;
 }
 
-int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
+int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages, enum base_backing_threshold_status *failure_reason)
 {
 	u64 old_pages;
 	u64 delta;
@@ -1629,40 +1714,48 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 	bool read_locked = false;
 
 	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(failure_reason);
 	KBASE_DEBUG_ASSERT(gpu_addr != 0);
 
-	if (gpu_addr & ~PAGE_MASK) {
-		dev_warn(kctx->kbdev->dev, "kbase:mem_commit: gpu_addr: passed parameter is invalid");
-		return -EINVAL;
-	}
-
 	down_write(&current->mm->mmap_sem);
 	kbase_gpu_vm_lock(kctx);
 
 	/* Validate the region */
 	reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
-	if (!reg || (reg->flags & KBASE_REG_FREE))
+	if (!reg || (reg->flags & KBASE_REG_FREE)) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS;
 		goto out_unlock;
+	}
 
 	KBASE_DEBUG_ASSERT(reg->cpu_alloc);
 	KBASE_DEBUG_ASSERT(reg->gpu_alloc);
 
-	if (reg->gpu_alloc->type != KBASE_MEM_TYPE_NATIVE)
+	if (reg->gpu_alloc->type != KBASE_MEM_TYPE_NATIVE) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
 		goto out_unlock;
+	}
 
-	if (0 == (reg->flags & KBASE_REG_GROWABLE))
+	if (0 == (reg->flags & KBASE_REG_GROWABLE)) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
 		goto out_unlock;
+	}
 
-	/* Would overflow the VA region */
-	if (new_pages > reg->nr_pages)
+	if (new_pages > reg->nr_pages) {
+		/* Would overflow the VA region */
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS;
 		goto out_unlock;
+	}
 
 	/* can't be mapped more than once on the GPU */
-	if (atomic_read(&reg->gpu_alloc->gpu_mappings) > 1)
+	if (atomic_read(&reg->gpu_alloc->gpu_mappings) > 1) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
 		goto out_unlock;
+	}
 	/* can't grow regions which are ephemeral */
-	if (reg->flags & KBASE_REG_DONT_NEED)
+	if (reg->flags & KBASE_REG_DONT_NEED) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
 		goto out_unlock;
+	}
 
 	if (new_pages == reg->gpu_alloc->nents) {
 		/* no change */
@@ -1683,13 +1776,13 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 
 		/* Allocate some more pages */
 		if (kbase_alloc_phy_pages_helper(reg->cpu_alloc, delta) != 0) {
-			res = -ENOMEM;
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
 			goto out_unlock;
 		}
 		if (reg->cpu_alloc != reg->gpu_alloc) {
 			if (kbase_alloc_phy_pages_helper(
 					reg->gpu_alloc, delta) != 0) {
-				res = -ENOMEM;
+				*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
 				kbase_free_phy_pages_helper(reg->cpu_alloc,
 						delta);
 				goto out_unlock;
@@ -1708,21 +1801,25 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 			if (reg->cpu_alloc != reg->gpu_alloc)
 				kbase_free_phy_pages_helper(reg->gpu_alloc,
 						delta);
-			res = -ENOMEM;
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
 			goto out_unlock;
 		}
 	} else {
 		delta = old_pages - new_pages;
 
 		/* Update all CPU mapping(s) */
-		kbase_mem_shrink_cpu_mapping(kctx, reg,
+		res = kbase_mem_shrink_cpu_mapping(kctx, reg,
 				new_pages, old_pages);
+		if (res) {
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
+			goto out_unlock;
+		}
 
 		/* Update the GPU mapping */
 		res = kbase_mem_shrink_gpu_mapping(kctx, reg,
 				new_pages, old_pages);
 		if (res) {
-			res = -ENOMEM;
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
 			goto out_unlock;
 		}
 
@@ -1767,7 +1864,7 @@ static void kbase_cpu_vm_close(struct vm_area_struct *vma)
 
 	kbase_gpu_vm_lock(map->kctx);
 
-	if (map->free_on_close) {
+	if (map->region) {
 		KBASE_DEBUG_ASSERT((map->region->flags & KBASE_REG_ZONE_MASK) ==
 				KBASE_REG_ZONE_SAME_VA);
 		/* Avoid freeing memory on the process death which results in
@@ -1793,17 +1890,19 @@ static int kbase_cpu_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	struct kbase_cpu_mapping *map = vma->vm_private_data;
 	pgoff_t rel_pgoff;
 	size_t i;
-	pgoff_t addr;
 
 	KBASE_DEBUG_ASSERT(map);
 	KBASE_DEBUG_ASSERT(map->count > 0);
 	KBASE_DEBUG_ASSERT(map->kctx);
 	KBASE_DEBUG_ASSERT(map->alloc);
 
-	rel_pgoff = vmf->pgoff - map->region->start_pfn;
+	/* we don't use vmf->pgoff as it's affected by our mmap with
+	 * offset being a GPU VA or a cookie */
+	rel_pgoff = ((unsigned long)vmf->virtual_address - map->vm_start)
+			>> PAGE_SHIFT;
 
 	kbase_gpu_vm_lock(map->kctx);
-	if (rel_pgoff >= map->alloc->nents)
+	if (map->page_off + rel_pgoff >= map->alloc->nents)
 		goto locked_bad_fault;
 
 	/* Fault on access to DONT_NEED regions */
@@ -1811,19 +1910,13 @@ static int kbase_cpu_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 		goto locked_bad_fault;
 
 	/* insert all valid pages from the fault location */
-	i = rel_pgoff;
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	addr = (pgoff_t)((uintptr_t)vmf->virtual_address >> PAGE_SHIFT);
-#else
-	addr = (pgoff_t)(vmf->address >> PAGE_SHIFT);
-#endif
-	while (i < map->alloc->nents && (addr < vma->vm_end >> PAGE_SHIFT)) {
-		int ret = vm_insert_pfn(vma, addr << PAGE_SHIFT,
-		    PFN_DOWN(map->alloc->pages[i]));
+	for (i = rel_pgoff;
+	     i < MIN((vma->vm_end - vma->vm_start) >> PAGE_SHIFT,
+	     map->alloc->nents - map->page_off); i++) {
+		int ret = vm_insert_pfn(vma, map->vm_start + (i << PAGE_SHIFT),
+		    PFN_DOWN(map->alloc->pages[map->page_off + i]));
 		if (ret < 0 && ret != -EBUSY)
 			goto locked_bad_fault;
-
-		i++; addr++;
 	}
 
 	kbase_gpu_vm_unlock(map->kctx);
@@ -1835,7 +1928,7 @@ locked_bad_fault:
 	return VM_FAULT_SIGBUS;
 }
 
-const struct vm_operations_struct kbase_vm_ops = {
+static const struct vm_operations_struct kbase_vm_ops = {
 	.open  = kbase_cpu_vm_open,
 	.close = kbase_cpu_vm_close,
 	.fault = kbase_cpu_vm_fault
@@ -1844,6 +1937,7 @@ const struct vm_operations_struct kbase_vm_ops = {
 static int kbase_cpu_mmap(struct kbase_va_region *reg, struct vm_area_struct *vma, void *kaddr, size_t nr_pages, unsigned long aligned_offset, int free_on_close)
 {
 	struct kbase_cpu_mapping *map;
+	u64 start_off = vma->vm_pgoff - reg->start_pfn;
 	phys_addr_t *page_array;
 	int err = 0;
 	int i;
@@ -1893,8 +1987,6 @@ static int kbase_cpu_mmap(struct kbase_va_region *reg, struct vm_area_struct *vm
 
 	if (!kaddr) {
 		unsigned long addr = vma->vm_start + aligned_offset;
-		u64 start_off = vma->vm_pgoff - reg->start_pfn +
-			(aligned_offset>>PAGE_SHIFT);
 
 		vma->vm_flags |= VM_PFNMAP;
 		for (i = 0; i < nr_pages; i++) {
@@ -1920,9 +2012,16 @@ static int kbase_cpu_mmap(struct kbase_va_region *reg, struct vm_area_struct *vm
 		goto out;
 	}
 
-	map->region = reg;
-	map->free_on_close = free_on_close;
+	map->page_off = start_off;
+	map->region = free_on_close ? reg : NULL;
 	map->kctx = reg->kctx;
+	map->vm_start = vma->vm_start + aligned_offset;
+	if (aligned_offset) {
+		KBASE_DEBUG_ASSERT(!start_off);
+		map->vm_end = map->vm_start + (reg->nr_pages << PAGE_SHIFT);
+	} else {
+		map->vm_end = vma->vm_end;
+	}
 	map->alloc = kbase_mem_phy_alloc_get(reg->cpu_alloc);
 	map->count = 1; /* start with one ref */
 
@@ -2092,86 +2191,19 @@ void kbase_os_mem_map_unlock(struct kbase_context *kctx)
 	up_read(&mm->mmap_sem);
 }
 
-static int kbasep_reg_mmap(struct kbase_context *kctx,
-			   struct vm_area_struct *vma,
-			   struct kbase_va_region **regm,
-			   size_t *nr_pages, size_t *aligned_offset)
-
-{
-	int cookie = vma->vm_pgoff - PFN_DOWN(BASE_MEM_COOKIE_BASE);
-	struct kbase_va_region *reg;
-	int err = 0;
-
-	*aligned_offset = 0;
-
-	dev_dbg(kctx->kbdev->dev, "in kbasep_reg_mmap\n");
-
-	/* SAME_VA stuff, fetch the right region */
-	reg = kctx->pending_regions[cookie];
-	if (!reg) {
-		err = -ENOMEM;
-		goto out;
-	}
-
-	if ((reg->flags & KBASE_REG_GPU_NX) && (reg->nr_pages != *nr_pages)) {
-		/* incorrect mmap size */
-		/* leave the cookie for a potential later
-		 * mapping, or to be reclaimed later when the
-		 * context is freed */
-		err = -ENOMEM;
-		goto out;
-	}
-
-	if ((vma->vm_flags & VM_READ && !(reg->flags & KBASE_REG_CPU_RD)) ||
-	    (vma->vm_flags & VM_WRITE && !(reg->flags & KBASE_REG_CPU_WR))) {
-		/* VM flags inconsistent with region flags */
-		err = -EPERM;
-		dev_err(kctx->kbdev->dev, "%s:%d inconsistent VM flags\n",
-							__FILE__, __LINE__);
-		goto out;
-	}
-
-	/* adjust down nr_pages to what we have physically */
-	*nr_pages = kbase_reg_current_backed_size(reg);
-
-	if (kbase_gpu_mmap(kctx, reg, vma->vm_start + *aligned_offset,
-						reg->nr_pages, 1) != 0) {
-		dev_err(kctx->kbdev->dev, "%s:%d\n", __FILE__, __LINE__);
-		/* Unable to map in GPU space. */
-		WARN_ON(1);
-		err = -ENOMEM;
-		goto out;
-	}
-	/* no need for the cookie anymore */
-	kctx->pending_regions[cookie] = NULL;
-	kctx->cookies |= (1UL << cookie);
-
-	/*
-	 * Overwrite the offset with the region start_pfn, so we effectively
-	 * map from offset 0 in the region. However subtract the aligned
-	 * offset so that when user space trims the mapping the beginning of
-	 * the trimmed VMA has the correct vm_pgoff;
-	 */
-	vma->vm_pgoff = reg->start_pfn - ((*aligned_offset)>>PAGE_SHIFT);
-out:
-	*regm = reg;
-	dev_dbg(kctx->kbdev->dev, "kbasep_reg_mmap done\n");
-
-	return err;
-}
-
 int kbase_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	struct kbase_context *kctx = file->private_data;
-	struct kbase_va_region *reg = NULL;
+	struct kbase_va_region *reg;
 	void *kaddr = NULL;
-	size_t nr_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+	size_t nr_pages;
 	int err = 0;
 	int free_on_close = 0;
 	struct device *dev = kctx->kbdev->dev;
 	size_t aligned_offset = 0;
 
 	dev_dbg(dev, "kbase_mmap\n");
+	nr_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
 
 	/* strip away corresponding VM_MAY% flags to the VM_% flags requested */
 	vma->vm_flags &= ~((vma->vm_flags & (VM_READ | VM_WRITE)) << 4);
@@ -2218,7 +2250,7 @@ int kbase_mmap(struct file *file, struct vm_area_struct *vma)
 		dev_dbg(dev, "kbase_trace_buffer_mmap ok\n");
 		/* free the region on munmap */
 		free_on_close = 1;
-		break;
+		goto map;
 	case PFN_DOWN(BASE_MEM_MMU_DUMP_HANDLE):
 		/* MMU dump */
 		err = kbase_mmu_dump_mmap(kctx, vma, &reg, &kaddr);
@@ -2226,28 +2258,111 @@ int kbase_mmap(struct file *file, struct vm_area_struct *vma)
 			goto out_unlock;
 		/* free the region on munmap */
 		free_on_close = 1;
-		break;
+		goto map;
 	case PFN_DOWN(BASE_MEM_COOKIE_BASE) ...
 	     PFN_DOWN(BASE_MEM_FIRST_FREE_ADDRESS) - 1: {
-		err = kbasep_reg_mmap(kctx, vma, &reg, &nr_pages,
-							&aligned_offset);
-		if (0 != err)
+		/* SAME_VA stuff, fetch the right region */
+		int gpu_pc_bits;
+		int cookie = vma->vm_pgoff - PFN_DOWN(BASE_MEM_COOKIE_BASE);
+
+		gpu_pc_bits = kctx->kbdev->gpu_props.props.core_props.log2_program_counter_size;
+		reg = kctx->pending_regions[cookie];
+		if (!reg) {
+			err = -ENOMEM;
+			goto out_unlock;
+		}
+
+		if (reg->flags & KBASE_REG_ALIGNED) {
+			/* nr_pages must be able to hold alignment pages
+			 * plus actual pages */
+			unsigned long align = 1ULL << gpu_pc_bits;
+			unsigned long extra_pages = 3 * PFN_DOWN(align);
+			unsigned long aligned_addr;
+			unsigned long aligned_addr_end;
+			unsigned long nr_bytes = reg->nr_pages << PAGE_SHIFT;
+
+			if (kctx->api_version < KBASE_API_VERSION(8, 5))
+				/* Maintain compatibility with old userspace */
+				extra_pages = PFN_DOWN(align);
+
+			if (nr_pages != reg->nr_pages + extra_pages) {
+				/* incorrect mmap size */
+				/* leave the cookie for a potential
+				 * later mapping, or to be reclaimed
+				 * later when the context is freed */
+				err = -ENOMEM;
+				goto out_unlock;
+			}
+
+			aligned_addr = ALIGN(vma->vm_start, align);
+			aligned_addr_end = aligned_addr + nr_bytes;
+
+			if (kctx->api_version >= KBASE_API_VERSION(8, 5)) {
+				if ((aligned_addr_end & BASE_MEM_MASK_4GB) == 0) {
+					/* Can't end at 4GB boundary */
+					aligned_addr += 2 * align;
+				} else if ((aligned_addr & BASE_MEM_MASK_4GB) == 0) {
+					/* Can't start at 4GB boundary */
+					aligned_addr += align;
+				}
+			}
+
+			aligned_offset = aligned_addr - vma->vm_start;
+		} else if (reg->nr_pages != nr_pages) {
+			/* incorrect mmap size */
+			/* leave the cookie for a potential later
+			 * mapping, or to be reclaimed later when the
+			 * context is freed */
+			err = -ENOMEM;
+			goto out_unlock;
+		}
+
+		if ((vma->vm_flags & VM_READ &&
+					!(reg->flags & KBASE_REG_CPU_RD)) ||
+				(vma->vm_flags & VM_WRITE &&
+				 !(reg->flags & KBASE_REG_CPU_WR))) {
+			/* VM flags inconsistent with region flags */
+			err = -EPERM;
+			dev_err(dev, "%s:%d inconsistent VM flags\n",
+					__FILE__, __LINE__);
+			goto out_unlock;
+		}
+
+		/* adjust down nr_pages to what we have physically */
+		nr_pages = kbase_reg_current_backed_size(reg);
+
+		if (kbase_gpu_mmap(kctx, reg,
+					vma->vm_start + aligned_offset,
+					reg->nr_pages, 1) != 0) {
+			dev_err(dev, "%s:%d\n", __FILE__, __LINE__);
+			/* Unable to map in GPU space. */
+			WARN_ON(1);
+			err = -ENOMEM;
 			goto out_unlock;
+		}
+
+		/* no need for the cookie anymore */
+		kctx->pending_regions[cookie] = NULL;
+		kctx->cookies |= (1UL << cookie);
+
+		/*
+		 * Overwrite the offset with the
+		 * region start_pfn, so we effectively
+		 * map from offset 0 in the region.
+		 */
+		vma->vm_pgoff = reg->start_pfn;
+
 		/* free the region on munmap */
 		free_on_close = 1;
-		break;
+		goto map;
 	}
 	default: {
-		reg = kbase_region_tracker_find_region_enclosing_address(kctx,
-					(u64)vma->vm_pgoff << PAGE_SHIFT);
+		reg = kbase_region_tracker_find_region_enclosing_address(kctx, (u64)vma->vm_pgoff << PAGE_SHIFT);
 
 		if (reg && !(reg->flags & KBASE_REG_FREE)) {
 			/* will this mapping overflow the size of the region? */
-			if (nr_pages > (reg->nr_pages -
-					(vma->vm_pgoff - reg->start_pfn))) {
-				err = -ENOMEM;
-				goto out_unlock;
-			}
+			if (nr_pages > (reg->nr_pages - (vma->vm_pgoff - reg->start_pfn)))
+				goto overflow;
 
 			if ((vma->vm_flags & VM_READ &&
 			     !(reg->flags & KBASE_REG_CPU_RD)) ||
@@ -2261,13 +2376,8 @@ int kbase_mmap(struct file *file, struct vm_area_struct *vma)
 			}
 
 #ifdef CONFIG_DMA_SHARED_BUFFER
-			if (KBASE_MEM_TYPE_IMPORTED_UMM ==
-							reg->cpu_alloc->type) {
-				err = dma_buf_mmap(
-					reg->cpu_alloc->imported.umm.dma_buf,
-					vma, vma->vm_pgoff - reg->start_pfn);
-				goto out_unlock;
-			}
+			if (reg->cpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM)
+				goto dma_map;
 #endif /* CONFIG_DMA_SHARED_BUFFER */
 
 			/* limit what we map to the amount currently backed */
@@ -2277,13 +2387,16 @@ int kbase_mmap(struct file *file, struct vm_area_struct *vma)
 				else
 					nr_pages = reg->cpu_alloc->nents - (vma->vm_pgoff - reg->start_pfn);
 			}
-		} else {
-			err = -ENOMEM;
-			goto out_unlock;
+
+			goto map;
 		}
+
+overflow:
+		err = -ENOMEM;
+		goto out_unlock;
 	} /* default */
 	} /* switch */
-
+map:
 	err = kbase_cpu_mmap(reg, vma, kaddr, nr_pages, aligned_offset, free_on_close);
 
 	if (vma->vm_pgoff == PFN_DOWN(BASE_MEM_MMU_DUMP_HANDLE)) {
@@ -2291,7 +2404,12 @@ int kbase_mmap(struct file *file, struct vm_area_struct *vma)
 		 * the pages, so we can now free the kernel mapping */
 		vfree(kaddr);
 	}
+	goto out_unlock;
 
+#ifdef CONFIG_DMA_SHARED_BUFFER
+dma_map:
+	err = dma_buf_mmap(reg->cpu_alloc->imported.umm.dma_buf, vma, vma->vm_pgoff - reg->start_pfn);
+#endif /* CONFIG_DMA_SHARED_BUFFER */
 out_unlock:
 	kbase_gpu_vm_unlock(kctx);
 out:
@@ -2621,8 +2739,7 @@ void *kbase_va_alloc(struct kbase_context *kctx, u32 size, struct kbase_hwc_dma_
 		goto no_reg;
 
 	reg->flags &= ~KBASE_REG_FREE;
-	if (kbase_update_region_flags(kctx, reg, flags) != 0)
-		goto invalid_flags;
+	kbase_update_region_flags(kctx, reg, flags);
 
 	reg->cpu_alloc = kbase_alloc_create(pages, KBASE_MEM_TYPE_RAW);
 	if (IS_ERR_OR_NULL(reg->cpu_alloc))
@@ -2649,7 +2766,6 @@ no_mmap:
 	kbase_mem_phy_alloc_put(reg->cpu_alloc);
 	kbase_mem_phy_alloc_put(reg->gpu_alloc);
 no_alloc:
-invalid_flags:
 	kfree(reg);
 no_reg:
 #if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0))
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem_linux.h b/drivers/gpu/arm/midgard/mali_kbase_mem_linux.h
index a41d646..6471747 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem_linux.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem_linux.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010, 2012-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010, 2012-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -32,27 +32,14 @@ struct kbase_hwc_dma_mapping {
 	size_t      size;
 };
 
-struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
-		u64 va_pages, u64 commit_pages, u64 extent, u64 *flags,
-		u64 *gpu_va);
+struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages, u64 commit_pages, u64 extent, u64 *flags, u64 *gpu_va, u16 *va_alignment);
 int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, int query, u64 *const pages);
 int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
-		void __user *phandle, u32 padding, u64 *gpu_va, u64 *va_pages,
+		void __user *phandle, u64 *gpu_va, u64 *va_pages,
 		u64 *flags);
 u64 kbase_mem_alias(struct kbase_context *kctx, u64 *flags, u64 stride, u64 nents, struct base_mem_aliasing_info *ai, u64 *num_pages);
 int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, unsigned int flags, unsigned int mask);
-
-/**
- * kbase_mem_commit - Change the physical backing size of a region
- *
- * @kctx: The kernel context
- * @gpu_addr: Handle to the memory region
- * @new_pages: Number of physical pages to back the region with
- *
- * Return: 0 on success or error code
- */
-int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages);
-
+int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages, enum base_backing_threshold_status *failure_reason);
 int kbase_mmap(struct file *file, struct vm_area_struct *vma);
 
 /**
@@ -226,6 +213,4 @@ void *kbase_va_alloc(struct kbase_context *kctx, u32 size, struct kbase_hwc_dma_
  */
 void kbase_va_free(struct kbase_context *kctx, struct kbase_hwc_dma_mapping *handle);
 
-extern const struct vm_operations_struct kbase_vm_ops;
-
 #endif				/* _KBASE_MEM_LINUX_H_ */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem_lowlevel.h b/drivers/gpu/arm/midgard/mali_kbase_mem_lowlevel.h
index 9725fd3..441180b6 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem_lowlevel.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem_lowlevel.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2014, 2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2014 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem_pool.c b/drivers/gpu/arm/midgard/mali_kbase_mem_pool.c
index 25c3128..9a3f9b5 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem_pool.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem_pool.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -24,6 +24,18 @@
 #include <linux/atomic.h>
 #include <linux/version.h>
 
+/* This function is only provided for backwards compatibility with kernels
+ * which use the old carveout allocator.
+ *
+ * The forward declaration is to keep sparse happy.
+ */
+int __init kbase_carveout_mem_reserve(
+		phys_addr_t size);
+int __init kbase_carveout_mem_reserve(phys_addr_t size)
+{
+	return 0;
+}
+
 #define pool_dbg(pool, format, ...) \
 	dev_dbg(pool->kbdev->dev, "%s-pool [%zu/%zu]: " format,	\
 		(pool->next_pool) ? "kctx" : "kbdev",	\
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mem_profile_debugfs.c b/drivers/gpu/arm/midgard/mali_kbase_mem_profile_debugfs.c
index d58fd8d..092da9a 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mem_profile_debugfs.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mem_profile_debugfs.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -84,8 +84,6 @@ int kbasep_mem_profile_debugfs_insert(struct kbase_context *kctx, char *data,
 		kfree(kctx->mem_profile_data);
 		kctx->mem_profile_data = data;
 		kctx->mem_profile_size = size;
-	} else {
-		kfree(data);
 	}
 
 	dev_dbg(kctx->kbdev->dev, "returning: %d, initialised: %d",
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mmu.c b/drivers/gpu/arm/midgard/mali_kbase_mmu.c
index 2614485..5c1b9c3 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mmu.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mmu.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -172,23 +172,18 @@ void page_fault_worker(struct work_struct *data)
 		dev_warn(kbdev->dev, "Access flag unexpectedly set");
 		goto fault_done;
 
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
 	case AS_FAULTSTATUS_EXCEPTION_CODE_ADDRESS_SIZE_FAULT:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			kbase_mmu_report_fault_and_kill(kctx, faulting_as,
+
+		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
 					"Address size fault");
-		else
-			kbase_mmu_report_fault_and_kill(kctx, faulting_as,
-					"Unknown fault code");
 		goto fault_done;
 
 	case AS_FAULTSTATUS_EXCEPTION_CODE_MEMORY_ATTRIBUTES_FAULT:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			kbase_mmu_report_fault_and_kill(kctx, faulting_as,
+		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
 					"Memory attributes fault");
-		else
-			kbase_mmu_report_fault_and_kill(kctx, faulting_as,
-					"Unknown fault code");
 		goto fault_done;
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
 
 	default:
 		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
@@ -209,13 +204,6 @@ void page_fault_worker(struct work_struct *data)
 		goto fault_done;
 	}
 
-	if (region->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
-		kbase_gpu_vm_unlock(kctx);
-		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
-				"DMA-BUF is not mapped on the GPU");
-		goto fault_done;
-	}
-
 	if ((region->flags & GROWABLE_FLAGS_REQUIRED)
 			!= GROWABLE_FLAGS_REQUIRED) {
 		kbase_gpu_vm_unlock(kctx);
@@ -344,7 +332,7 @@ void page_fault_worker(struct work_struct *data)
 #if defined(CONFIG_MALI_GATOR_SUPPORT)
 		kbase_trace_mali_page_fault_insert_pages(as_no, new_pages);
 #endif
-		KBASE_TLSTREAM_AUX_PAGEFAULT(kctx->id, (u64)new_pages);
+		kbase_tlstream_aux_pagefault(kctx->id, (u64)new_pages);
 
 		/* AS transaction begin */
 		mutex_lock(&kbdev->mmu_hw_mutex);
@@ -410,7 +398,7 @@ phys_addr_t kbase_mmu_alloc_pgd(struct kbase_context *kctx)
 	if (!p)
 		goto sub_pages;
 
-	KBASE_TLSTREAM_AUX_PAGESALLOC(
+	kbase_tlstream_aux_pagesalloc(
 			(u32)kctx->id,
 			(u64)new_page_count);
 
@@ -522,7 +510,6 @@ static phys_addr_t mmu_insert_pages_recover_get_next_pgd(struct kbase_context *k
 	KBASE_DEBUG_ASSERT(NULL != kctx);
 
 	lockdep_assert_held(&kctx->mmu_lock);
-	lockdep_assert_held(&kctx->reg_lock);
 
 	/*
 	 * Architecture spec defines level-0 as being the top-most.
@@ -573,7 +560,6 @@ static void mmu_insert_pages_failure_recovery(struct kbase_context *kctx, u64 vp
 	KBASE_DEBUG_ASSERT(vpfn <= (U64_MAX / PAGE_SIZE));
 
 	lockdep_assert_held(&kctx->mmu_lock);
-	lockdep_assert_held(&kctx->reg_lock);
 
 	mmu_mode = kctx->kbdev->mmu_mode;
 
@@ -921,9 +907,7 @@ static void kbase_mmu_flush_invalidate(struct kbase_context *kctx,
 		return;
 
 	kbdev = kctx->kbdev;
-	mutex_lock(&kbdev->js_data.queue_mutex);
 	ctx_is_in_runpool = kbasep_js_runpool_retain_ctx(kbdev, kctx);
-	mutex_unlock(&kbdev->js_data.queue_mutex);
 
 	if (ctx_is_in_runpool) {
 		KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
@@ -1210,8 +1194,6 @@ static void mmu_check_unused(struct kbase_context *kctx, phys_addr_t pgd)
 	u64 *page;
 	int i;
 
-	lockdep_assert_held(&kctx->reg_lock);
-
 	page = kmap_atomic(pfn_to_page(PFN_DOWN(pgd)));
 	/* kmap_atomic should NEVER fail. */
 	KBASE_DEBUG_ASSERT(NULL != page);
@@ -1232,7 +1214,6 @@ static void mmu_teardown_level(struct kbase_context *kctx, phys_addr_t pgd, int
 
 	KBASE_DEBUG_ASSERT(NULL != kctx);
 	lockdep_assert_held(&kctx->mmu_lock);
-	lockdep_assert_held(&kctx->reg_lock);
 
 	pgd_page = kmap_atomic(pfn_to_page(PFN_DOWN(pgd)));
 	/* kmap_atomic should NEVER fail. */
@@ -1314,7 +1295,7 @@ void kbase_mmu_free_pgd(struct kbase_context *kctx)
 	new_page_count = kbase_atomic_sub_pages(1, &kctx->used_pages);
 	kbase_atomic_sub_pages(1, &kctx->kbdev->memdev.used_pages);
 
-	KBASE_TLSTREAM_AUX_PAGESALLOC(
+	kbase_tlstream_aux_pagesalloc(
 			(u32)kctx->id,
 			(u64)new_page_count);
 }
@@ -1633,8 +1614,7 @@ const char *kbase_exception_name(struct kbase_device *kbdev, u32 exception_code)
 		e = "TRANSLATION_FAULT";
 		break;
 	case 0xC8:
-		e = "PERMISSION_FAULT";
-		break;
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
 	case 0xC9:
 	case 0xCA:
 	case 0xCB:
@@ -1642,10 +1622,8 @@ const char *kbase_exception_name(struct kbase_device *kbdev, u32 exception_code)
 	case 0xCD:
 	case 0xCE:
 	case 0xCF:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			e = "PERMISSION_FAULT";
-		else
-			e = "UNKNOWN";
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
+		e = "PERMISSION_FAULT";
 		break;
 	case 0xD0:
 	case 0xD1:
@@ -1658,8 +1636,7 @@ const char *kbase_exception_name(struct kbase_device *kbdev, u32 exception_code)
 		e = "TRANSTAB_BUS_FAULT";
 		break;
 	case 0xD8:
-		e = "ACCESS_FLAG";
-		break;
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
 	case 0xD9:
 	case 0xDA:
 	case 0xDB:
@@ -1667,11 +1644,10 @@ const char *kbase_exception_name(struct kbase_device *kbdev, u32 exception_code)
 	case 0xDD:
 	case 0xDE:
 	case 0xDF:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			e = "ACCESS_FLAG";
-		else
-			e = "UNKNOWN";
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
+		e = "ACCESS_FLAG";
 		break;
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
 	case 0xE0:
 	case 0xE1:
 	case 0xE2:
@@ -1680,10 +1656,7 @@ const char *kbase_exception_name(struct kbase_device *kbdev, u32 exception_code)
 	case 0xE5:
 	case 0xE6:
 	case 0xE7:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			e = "ADDRESS_SIZE_FAULT";
-		else
-			e = "UNKNOWN";
+		e = "ADDRESS_SIZE_FAULT";
 		break;
 	case 0xE8:
 	case 0xE9:
@@ -1693,10 +1666,8 @@ const char *kbase_exception_name(struct kbase_device *kbdev, u32 exception_code)
 	case 0xED:
 	case 0xEE:
 	case 0xEF:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			e = "MEMORY_ATTRIBUTES_FAULT";
-		else
-			e = "UNKNOWN";
+		e = "MEMORY_ATTRIBUTES_FAULT";
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
 		break;
 	default:
 		e = "UNKNOWN";
@@ -1711,10 +1682,11 @@ static const char *access_type_name(struct kbase_device *kbdev,
 {
 	switch (fault_status & AS_FAULTSTATUS_ACCESS_TYPE_MASK) {
 	case AS_FAULTSTATUS_ACCESS_TYPE_ATOMIC:
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			return "ATOMIC";
-		else
-			return "UNKNOWN";
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+		return "ATOMIC";
+#else
+		return "UNKNOWN";
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
 	case AS_FAULTSTATUS_ACCESS_TYPE_READ:
 		return "READ";
 	case AS_FAULTSTATUS_ACCESS_TYPE_WRITE:
@@ -1750,7 +1722,7 @@ static void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx,
 	js_devdata = &kbdev->js_data;
 
 	/* ASSERT that the context won't leave the runpool */
-	KBASE_DEBUG_ASSERT(atomic_read(&kctx->refcount) > 0);
+	KBASE_DEBUG_ASSERT(kbasep_js_debug_check_ctx_refcount(kbdev, kctx) > 0);
 
 	/* decode the fault status */
 	exception_type = as->fault_status & 0xFF;
@@ -2049,14 +2021,15 @@ void kbase_mmu_interrupt_process(struct kbase_device *kbdev, struct kbase_contex
 		 */
 		kbasep_js_clear_submit_allowed(js_devdata, kctx);
 
-		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_AARCH64_MMU))
-			dev_warn(kbdev->dev,
-					"Bus error in AS%d at VA=0x%016llx, IPA=0x%016llx\n",
-					as->number, as->fault_addr,
-					as->fault_extra_addr);
-		else
-			dev_warn(kbdev->dev, "Bus error in AS%d at 0x%016llx\n",
-					as->number, as->fault_addr);
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+		dev_warn(kbdev->dev,
+				"Bus error in AS%d at VA=0x%016llx, IPA=0x%016llx\n",
+				as->number, as->fault_addr,
+				as->fault_extra_addr);
+#else
+		dev_warn(kbdev->dev, "Bus error in AS%d at 0x%016llx\n",
+				as->number, as->fault_addr);
+#endif /* CONFIG_MALI_GPU_MMU_AARCH64 */
 
 		/*
 		 * We need to switch to UNMAPPED mode - but we do this in a
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mmu_mode.h b/drivers/gpu/arm/midgard/mali_kbase_mmu_mode.h
index b487c00..2449c60 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mmu_mode.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_mmu_mode.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2015, 2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2015 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_aarch64.c b/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_aarch64.c
index 60df171..791f3ed 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_aarch64.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_aarch64.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2014, 2016, 2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2014, 2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_lpae.c b/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_lpae.c
index 53fbbc7..683cabb 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_lpae.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_mmu_mode_lpae.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -91,7 +91,11 @@ static void mmu_get_as_setup(struct kbase_context *kctx,
 		AS_TRANSTAB_LPAE_ADRMODE_TABLE |
 		AS_TRANSTAB_LPAE_READ_INNER;
 
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+	setup->transcfg = AS_TRANSCFG_ADRMODE_LEGACY;
+#else
 	setup->transcfg = 0;
+#endif
 }
 
 static void mmu_update(struct kbase_context *kctx)
@@ -113,6 +117,10 @@ static void mmu_disable_as(struct kbase_device *kbdev, int as_nr)
 
 	current_setup->transtab = AS_TRANSTAB_LPAE_ADRMODE_UNMAPPED;
 
+#ifdef CONFIG_MALI_GPU_MMU_AARCH64
+	current_setup->transcfg = AS_TRANSCFG_ADRMODE_LEGACY;
+#endif
+
 	/* Apply the address space setting */
 	kbase_mmu_hw_configure(kbdev, as, NULL);
 }
diff --git a/drivers/gpu/arm/midgard/mali_kbase_softjobs.c b/drivers/gpu/arm/midgard/mali_kbase_softjobs.c
index 72809f7..88b91a2 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_softjobs.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_softjobs.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,14 +23,15 @@
 #include <linux/dma-buf.h>
 #include <asm/cacheflush.h>
 #endif /* defined(CONFIG_DMA_SHARED_BUFFER) */
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-#include <mali_kbase_sync.h>
-#endif
 #include <linux/dma-mapping.h>
+#ifdef CONFIG_SYNC
+#include "sync.h"
+#include <linux/syscalls.h>
+#include "mali_kbase_sync.h"
+#endif
 #include <mali_base_kernel.h>
 #include <mali_kbase_hwaccess_time.h>
 #include <mali_kbase_mem_linux.h>
-#include <mali_kbase_tlstream.h>
 #include <linux/version.h>
 #include <linux/ktime.h>
 #include <linux/pfn.h>
@@ -46,7 +47,7 @@
  * executed within the driver rather than being handed over to the GPU.
  */
 
-static void kbasep_add_waiting_soft_job(struct kbase_jd_atom *katom)
+void kbasep_add_waiting_soft_job(struct kbase_jd_atom *katom)
 {
 	struct kbase_context *kctx = katom->kctx;
 	unsigned long lflags;
@@ -193,11 +194,48 @@ static int kbase_dump_cpu_gpu_time(struct kbase_jd_atom *katom)
 	return 0;
 }
 
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
-/* Called by the explicit fence mechanism when a fence wait has completed */
-void kbase_soft_event_wait_callback(struct kbase_jd_atom *katom)
+#ifdef CONFIG_SYNC
+
+static enum base_jd_event_code kbase_fence_trigger(struct kbase_jd_atom *katom, int result)
 {
-	struct kbase_context *kctx = katom->kctx;
+	struct sync_pt *pt;
+	struct sync_timeline *timeline;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
+	if (!list_is_singular(&katom->fence->pt_list_head)) {
+#else
+	if (katom->fence->num_fences != 1) {
+#endif
+		/* Not exactly one item in the list - so it didn't (directly) come from us */
+		return BASE_JD_EVENT_JOB_CANCELLED;
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
+	pt = list_first_entry(&katom->fence->pt_list_head, struct sync_pt, pt_list);
+#else
+	pt = container_of(katom->fence->cbs[0].sync_pt, struct sync_pt, base);
+#endif
+	timeline = sync_pt_parent(pt);
+
+	if (!kbase_sync_timeline_is_ours(timeline)) {
+		/* Fence has a sync_pt which isn't ours! */
+		return BASE_JD_EVENT_JOB_CANCELLED;
+	}
+
+	kbase_sync_signal_pt(pt, result);
+
+	sync_timeline_signal(timeline);
+
+	return (result < 0) ? BASE_JD_EVENT_JOB_CANCELLED : BASE_JD_EVENT_DONE;
+}
+
+static void kbase_fence_wait_worker(struct work_struct *data)
+{
+	struct kbase_jd_atom *katom;
+	struct kbase_context *kctx;
+
+	katom = container_of(data, struct kbase_jd_atom, work);
+	kctx = katom->kctx;
 
 	mutex_lock(&kctx->jctx.lock);
 	kbasep_remove_waiting_soft_job(katom);
@@ -206,8 +244,106 @@ void kbase_soft_event_wait_callback(struct kbase_jd_atom *katom)
 		kbase_js_sched_all(kctx->kbdev);
 	mutex_unlock(&kctx->jctx.lock);
 }
+
+static void kbase_fence_wait_callback(struct sync_fence *fence, struct sync_fence_waiter *waiter)
+{
+	struct kbase_jd_atom *katom = container_of(waiter, struct kbase_jd_atom, sync_waiter);
+	struct kbase_context *kctx;
+
+	KBASE_DEBUG_ASSERT(NULL != katom);
+
+	kctx = katom->kctx;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	/* Propagate the fence status to the atom.
+	 * If negative then cancel this atom and its dependencies.
+	 */
+	if (kbase_fence_get_status(fence) < 0)
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+
+	/* To prevent a potential deadlock we schedule the work onto the job_done_wq workqueue
+	 *
+	 * The issue is that we may signal the timeline while holding kctx->jctx.lock and
+	 * the callbacks are run synchronously from sync_timeline_signal. So we simply defer the work.
+	 */
+
+	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
+	INIT_WORK(&katom->work, kbase_fence_wait_worker);
+	queue_work(kctx->jctx.job_done_wq, &katom->work);
+}
+
+static int kbase_fence_wait(struct kbase_jd_atom *katom)
+{
+	int ret;
+
+	KBASE_DEBUG_ASSERT(NULL != katom);
+	KBASE_DEBUG_ASSERT(NULL != katom->kctx);
+
+	sync_fence_waiter_init(&katom->sync_waiter, kbase_fence_wait_callback);
+
+	ret = sync_fence_wait_async(katom->fence, &katom->sync_waiter);
+
+	if (ret == 1) {
+		/* Already signalled */
+		return 0;
+	}
+
+	if (ret < 0) {
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+		/* We should cause the dependent jobs in the bag to be failed,
+		 * to do this we schedule the work queue to complete this job */
+		KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
+		INIT_WORK(&katom->work, kbase_fence_wait_worker);
+		queue_work(katom->kctx->jctx.job_done_wq, &katom->work);
+	}
+
+#ifdef CONFIG_MALI_FENCE_DEBUG
+	/* The timeout code will add this job to the list of waiting soft jobs.
+	 */
+	kbasep_add_waiting_with_timeout(katom);
+#else
+	kbasep_add_waiting_soft_job(katom);
 #endif
 
+	return 1;
+}
+
+static void kbase_fence_cancel_wait(struct kbase_jd_atom *katom)
+{
+	if(!katom)
+	{
+		pr_err("katom null.forbiden return\n");
+		return;
+	}
+	if(!katom->fence)
+	{
+		pr_info("katom->fence null.may release out of order.so continue unfinished step\n");
+		/*
+		if return here,may result in  infinite loop?
+		we need to delete dep_item[0] from kctx->waiting_soft_jobs?
+		jd_done_nolock function move the dep_item[0] to complete job list and then delete?
+		*/
+		goto finish_softjob;
+	}
+
+	if (sync_fence_cancel_async(katom->fence, &katom->sync_waiter) != 0) {
+		/* The wait wasn't cancelled - leave the cleanup for kbase_fence_wait_callback */
+		return;
+	}
+
+	/* Wait was cancelled - zap the atoms */
+finish_softjob:
+	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+
+	kbasep_remove_waiting_soft_job(katom);
+	kbase_finish_soft_job(katom);
+
+	if (jd_done_nolock(katom, NULL))
+		kbase_js_sched_all(katom->kctx->kbdev);
+}
+#endif /* CONFIG_SYNC */
+
 static void kbasep_soft_event_complete_job(struct work_struct *work)
 {
 	struct kbase_jd_atom *katom = container_of(work, struct kbase_jd_atom,
@@ -268,6 +404,16 @@ void kbasep_complete_triggered_soft_events(struct kbase_context *kctx, u64 evt)
 }
 
 #ifdef CONFIG_MALI_FENCE_DEBUG
+static char *kbase_fence_debug_status_string(int status)
+{
+	if (status == 0)
+		return "signaled";
+	else if (status > 0)
+		return "active";
+	else
+		return "error";
+}
+
 static void kbase_fence_debug_check_atom(struct kbase_jd_atom *katom)
 {
 	struct kbase_context *kctx = katom->kctx;
@@ -284,17 +430,15 @@ static void kbase_fence_debug_check_atom(struct kbase_jd_atom *katom)
 
 			if ((dep->core_req & BASE_JD_REQ_SOFT_JOB_TYPE)
 					== BASE_JD_REQ_SOFT_FENCE_TRIGGER) {
+				struct sync_fence *fence = dep->fence;
+				int status = kbase_fence_get_status(fence);
+
 				/* Found blocked trigger fence. */
-				struct kbase_sync_fence_info info;
-
-				if (!kbase_sync_fence_in_info_get(dep, &info)) {
-					dev_warn(dev,
-						 "\tVictim trigger atom %d fence [%p] %s: %s\n",
-						 kbase_jd_atom_id(kctx, dep),
-						 info.fence,
-						 info.name,
-						 kbase_sync_status_string(info.status));
-				 }
+				dev_warn(dev,
+					 "\tVictim trigger atom %d fence [%p] %s: %s\n",
+					 kbase_jd_atom_id(kctx, dep),
+					 fence, fence->name,
+					 kbase_fence_debug_status_string(status));
 			}
 
 			kbase_fence_debug_check_atom(dep);
@@ -306,32 +450,31 @@ static void kbase_fence_debug_wait_timeout(struct kbase_jd_atom *katom)
 {
 	struct kbase_context *kctx = katom->kctx;
 	struct device *dev = katom->kctx->kbdev->dev;
+	struct sync_fence *fence = katom->fence;
 	int timeout_ms = atomic_read(&kctx->kbdev->js_data.soft_job_timeout_ms);
+	int status = kbase_fence_get_status(fence);
 	unsigned long lflags;
-	struct kbase_sync_fence_info info;
 
 	spin_lock_irqsave(&kctx->waiting_soft_jobs_lock, lflags);
 
-	if (kbase_sync_fence_in_info_get(katom, &info)) {
-		/* Fence must have signaled just after timeout. */
-		spin_unlock_irqrestore(&kctx->waiting_soft_jobs_lock, lflags);
-		return;
-	}
-
 	dev_warn(dev, "ctx %d_%d: Atom %d still waiting for fence [%p] after %dms\n",
 		 kctx->tgid, kctx->id,
 		 kbase_jd_atom_id(kctx, katom),
-		 info.fence, timeout_ms);
+		 fence, timeout_ms);
 	dev_warn(dev, "\tGuilty fence [%p] %s: %s\n",
-		 info.fence, info.name,
-		 kbase_sync_status_string(info.status));
+		 fence, fence->name,
+		 kbase_fence_debug_status_string(status));
 
 	/* Search for blocked trigger atoms */
 	kbase_fence_debug_check_atom(katom);
 
 	spin_unlock_irqrestore(&kctx->waiting_soft_jobs_lock, lflags);
 
-	kbase_sync_fence_in_dump(katom);
+	/* Dump out the full state of all the Android sync fences.
+	 * The function sync_dump() isn't exported to modules, so force
+	 * sync_fence_wait() to time out to trigger sync_dump().
+	 */
+	sync_fence_wait(fence, 1);
 }
 
 struct kbase_fence_debug_work {
@@ -724,8 +867,6 @@ static void kbase_mem_copy_from_extres_page(struct kbase_context *kctx,
 	void *target_page = kmap(pages[*target_page_nr]);
 	size_t chunk = PAGE_SIZE-offset;
 
-	lockdep_assert_held(&kctx->reg_lock);
-
 	if (!target_page) {
 		*target_page_nr += 1;
 		dev_warn(kctx->kbdev->dev, "kmap failed in debug_copy job.");
@@ -868,7 +1009,6 @@ static int kbase_jit_allocate_prepare(struct kbase_jd_atom *katom)
 {
 	__user void *data = (__user void *)(uintptr_t) katom->jc;
 	struct base_jit_alloc_info *info;
-	struct kbase_context *kctx = katom->kctx;
 	int ret;
 
 	/* Fail the job if there is no info structure */
@@ -909,10 +1049,6 @@ static int kbase_jit_allocate_prepare(struct kbase_jd_atom *katom)
 
 	/* Replace the user pointer with our kernel allocated info structure */
 	katom->jc = (u64)(uintptr_t) info;
-	katom->jit_blocked = false;
-
-	lockdep_assert_held(&kctx->jctx.lock);
-	list_add_tail(&katom->jit_node, &kctx->jit_atoms_head);
 
 	/*
 	 * Note:
@@ -934,82 +1070,33 @@ fail:
 	return ret;
 }
 
-static u8 kbase_jit_free_get_id(struct kbase_jd_atom *katom)
-{
-	if (WARN_ON(katom->core_req != BASE_JD_REQ_SOFT_JIT_FREE))
-		return 0;
-
-	return (u8) katom->jc;
-}
-
-static int kbase_jit_allocate_process(struct kbase_jd_atom *katom)
+static void kbase_jit_allocate_process(struct kbase_jd_atom *katom)
 {
 	struct kbase_context *kctx = katom->kctx;
 	struct base_jit_alloc_info *info;
 	struct kbase_va_region *reg;
 	struct kbase_vmap_struct mapping;
-	u64 *ptr, new_addr;
-
-	if (katom->jit_blocked) {
-		list_del(&katom->queue);
-		katom->jit_blocked = false;
-	}
+	u64 *ptr;
 
 	info = (struct base_jit_alloc_info *) (uintptr_t) katom->jc;
 
 	/* The JIT ID is still in use so fail the allocation */
 	if (kctx->jit_alloc[info->id]) {
 		katom->event_code = BASE_JD_EVENT_MEM_GROWTH_FAILED;
-		return 0;
+		return;
 	}
 
+	/*
+	 * Mark the allocation so we know it's in use even if the
+	 * allocation itself fails.
+	 */
+	kctx->jit_alloc[info->id] = (struct kbase_va_region *) -1;
+
 	/* Create a JIT allocation */
 	reg = kbase_jit_allocate(kctx, info);
 	if (!reg) {
-		struct kbase_jd_atom *jit_atom;
-		bool can_block = false;
-
-		lockdep_assert_held(&kctx->jctx.lock);
-
-		jit_atom = list_first_entry(&kctx->jit_atoms_head,
-				struct kbase_jd_atom, jit_node);
-
-		list_for_each_entry(jit_atom, &kctx->jit_atoms_head, jit_node) {
-			if (jit_atom == katom)
-				break;
-			if (jit_atom->core_req == BASE_JD_REQ_SOFT_JIT_FREE) {
-				u8 free_id = kbase_jit_free_get_id(jit_atom);
-
-				if (free_id && kctx->jit_alloc[free_id]) {
-					/* A JIT free which is active and
-					 * submitted before this atom
-					 */
-					can_block = true;
-					break;
-				}
-			}
-		}
-
-		if (!can_block) {
-			/* Mark the allocation so we know it's in use even if
-			 * the allocation itself fails.
-			 */
-			kctx->jit_alloc[info->id] =
-				(struct kbase_va_region *) -1;
-
-			katom->event_code = BASE_JD_EVENT_MEM_GROWTH_FAILED;
-			return 0;
-		}
-
-		/* There are pending frees for an active allocation
-		 * so we should wait to see whether they free the memory.
-		 * Add to the beginning of the list to ensure that the atom is
-		 * processed only once in kbase_jit_free_finish
-		 */
-		list_add(&katom->queue, &kctx->jit_pending_alloc);
-		katom->jit_blocked = true;
-
-		return 1;
+		katom->event_code = BASE_JD_EVENT_MEM_GROWTH_FAILED;
+		return;
 	}
 
 	/*
@@ -1024,13 +1111,10 @@ static int kbase_jit_allocate_process(struct kbase_jd_atom *katom)
 		 * submitted anyway.
 		 */
 		katom->event_code = BASE_JD_EVENT_JOB_INVALID;
-		return 0;
+		return;
 	}
 
-	new_addr = reg->start_pfn << PAGE_SHIFT;
-	*ptr = new_addr;
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_JIT(
-			katom, info->gpu_alloc_addr, new_addr);
+	*ptr = reg->start_pfn << PAGE_SHIFT;
 	kbase_vunmap(kctx, &mapping);
 
 	katom->event_code = BASE_JD_EVENT_DONE;
@@ -1040,43 +1124,21 @@ static int kbase_jit_allocate_process(struct kbase_jd_atom *katom)
 	 * the JIT free racing this JIT alloc job.
 	 */
 	kctx->jit_alloc[info->id] = reg;
-
-	return 0;
 }
 
 static void kbase_jit_allocate_finish(struct kbase_jd_atom *katom)
 {
 	struct base_jit_alloc_info *info;
 
-	lockdep_assert_held(&katom->kctx->jctx.lock);
-
-	/* Remove atom from jit_atoms_head list */
-	list_del(&katom->jit_node);
-
-	if (katom->jit_blocked) {
-		list_del(&katom->queue);
-		katom->jit_blocked = false;
-	}
-
 	info = (struct base_jit_alloc_info *) (uintptr_t) katom->jc;
 	/* Free the info structure */
 	kfree(info);
 }
 
-static int kbase_jit_free_prepare(struct kbase_jd_atom *katom)
-{
-	struct kbase_context *kctx = katom->kctx;
-
-	lockdep_assert_held(&kctx->jctx.lock);
-	list_add_tail(&katom->jit_node, &kctx->jit_atoms_head);
-
-	return 0;
-}
-
 static void kbase_jit_free_process(struct kbase_jd_atom *katom)
 {
 	struct kbase_context *kctx = katom->kctx;
-	u8 id = kbase_jit_free_get_id(katom);
+	u8 id = (u8) katom->jc;
 
 	/*
 	 * If the ID is zero or it is not in use yet then fail the job.
@@ -1096,43 +1158,6 @@ static void kbase_jit_free_process(struct kbase_jd_atom *katom)
 	kctx->jit_alloc[id] = NULL;
 }
 
-static void kbasep_jit_free_finish_worker(struct work_struct *work)
-{
-	struct kbase_jd_atom *katom = container_of(work, struct kbase_jd_atom,
-			work);
-	struct kbase_context *kctx = katom->kctx;
-	int resched;
-
-	mutex_lock(&kctx->jctx.lock);
-	kbase_finish_soft_job(katom);
-	resched = jd_done_nolock(katom, NULL);
-	mutex_unlock(&kctx->jctx.lock);
-
-	if (resched)
-		kbase_js_sched_all(kctx->kbdev);
-}
-
-static void kbase_jit_free_finish(struct kbase_jd_atom *katom)
-{
-	struct list_head *i, *tmp;
-	struct kbase_context *kctx = katom->kctx;
-
-	lockdep_assert_held(&kctx->jctx.lock);
-	/* Remove this atom from the kctx->jit_atoms_head list */
-	list_del(&katom->jit_node);
-
-	list_for_each_safe(i, tmp, &kctx->jit_pending_alloc) {
-		struct kbase_jd_atom *pending_atom = list_entry(i,
-				struct kbase_jd_atom, queue);
-		if (kbase_jit_allocate_process(pending_atom) == 0) {
-			/* Atom has completed */
-			INIT_WORK(&pending_atom->work,
-					kbasep_jit_free_finish_worker);
-			queue_work(kctx->jctx.job_done_wq, &pending_atom->work);
-		}
-	}
-}
-
 static int kbase_ext_res_prepare(struct kbase_jd_atom *katom)
 {
 	__user struct base_external_resource_list *user_ext_res;
@@ -1267,28 +1292,17 @@ int kbase_process_soft_job(struct kbase_jd_atom *katom)
 	switch (katom->core_req & BASE_JD_REQ_SOFT_JOB_TYPE) {
 	case BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME:
 		return kbase_dump_cpu_gpu_time(katom);
-
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
-		katom->event_code = kbase_sync_fence_out_trigger(katom,
-				katom->event_code == BASE_JD_EVENT_DONE ?
-								0 : -EFAULT);
+		KBASE_DEBUG_ASSERT(katom->fence != NULL);
+		katom->event_code = kbase_fence_trigger(katom, katom->event_code == BASE_JD_EVENT_DONE ? 0 : -EFAULT);
+		/* Release the reference as we don't need it any more */
+		sync_fence_put(katom->fence);
+		katom->fence = NULL;
 		break;
 	case BASE_JD_REQ_SOFT_FENCE_WAIT:
-	{
-		int ret = kbase_sync_fence_in_wait(katom);
-
-		if (ret == 1) {
-#ifdef CONFIG_MALI_FENCE_DEBUG
-			kbasep_add_waiting_with_timeout(katom);
-#else
-			kbasep_add_waiting_soft_job(katom);
-#endif
-		}
-		return ret;
-	}
-#endif
-
+		return kbase_fence_wait(katom);
+#endif				/* CONFIG_SYNC */
 	case BASE_JD_REQ_SOFT_REPLAY:
 		return kbase_replay_process(katom);
 	case BASE_JD_REQ_SOFT_EVENT_WAIT:
@@ -1308,8 +1322,11 @@ int kbase_process_soft_job(struct kbase_jd_atom *katom)
 		break;
 	}
 	case BASE_JD_REQ_SOFT_JIT_ALLOC:
-		return kbase_jit_allocate_process(katom);
+		return -EINVAL; /* Temporarily disabled */
+		kbase_jit_allocate_process(katom);
+		break;
 	case BASE_JD_REQ_SOFT_JIT_FREE:
+		return -EINVAL; /* Temporarily disabled */
 		kbase_jit_free_process(katom);
 		break;
 	case BASE_JD_REQ_SOFT_EXT_RES_MAP:
@@ -1327,9 +1344,9 @@ int kbase_process_soft_job(struct kbase_jd_atom *katom)
 void kbase_cancel_soft_job(struct kbase_jd_atom *katom)
 {
 	switch (katom->core_req & BASE_JD_REQ_SOFT_JOB_TYPE) {
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 	case BASE_JD_REQ_SOFT_FENCE_WAIT:
-		kbase_sync_fence_in_cancel_wait(katom);
+		kbase_fence_cancel_wait(katom);
 		break;
 #endif
 	case BASE_JD_REQ_SOFT_EVENT_WAIT:
@@ -1350,7 +1367,7 @@ int kbase_prepare_soft_job(struct kbase_jd_atom *katom)
 				return -EINVAL;
 		}
 		break;
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
 		{
 			struct base_fence fence;
@@ -1359,16 +1376,21 @@ int kbase_prepare_soft_job(struct kbase_jd_atom *katom)
 			if (0 != copy_from_user(&fence, (__user void *)(uintptr_t) katom->jc, sizeof(fence)))
 				return -EINVAL;
 
-			fd = kbase_sync_fence_out_create(katom,
-							 fence.basep.stream_fd);
+			fd = kbase_stream_create_fence(fence.basep.stream_fd);
 			if (fd < 0)
 				return -EINVAL;
 
+			katom->fence = sync_fence_fdget(fd);
+
+			if (katom->fence == NULL) {
+				/* The only way the fence can be NULL is if userspace closed it for us.
+				 * So we don't need to clear it up */
+				return -EINVAL;
+			}
 			fence.basep.fd = fd;
 			if (0 != copy_to_user((__user void *)(uintptr_t) katom->jc, &fence, sizeof(fence))) {
-				kbase_sync_fence_out_remove(katom);
-				kbase_sync_fence_close_fd(fd);
-				fence.basep.fd = -EINVAL;
+				katom->fence = NULL;
+				sys_close(fd);
 				return -EINVAL;
 			}
 		}
@@ -1376,36 +1398,22 @@ int kbase_prepare_soft_job(struct kbase_jd_atom *katom)
 	case BASE_JD_REQ_SOFT_FENCE_WAIT:
 		{
 			struct base_fence fence;
-			int ret;
 
 			if (0 != copy_from_user(&fence, (__user void *)(uintptr_t) katom->jc, sizeof(fence)))
 				return -EINVAL;
 
 			/* Get a reference to the fence object */
-			ret = kbase_sync_fence_in_from_fd(katom,
-							  fence.basep.fd);
-			if (ret < 0)
-				return ret;
-
-#ifdef CONFIG_MALI_DMA_FENCE
-			/*
-			 * Set KCTX_NO_IMPLICIT_FENCE in the context the first
-			 * time a soft fence wait job is observed. This will
-			 * prevent the implicit dma-buf fence to conflict with
-			 * the Android native sync fences.
-			 */
-			if (!kbase_ctx_flag(katom->kctx, KCTX_NO_IMPLICIT_SYNC))
-				kbase_ctx_flag_set(katom->kctx, KCTX_NO_IMPLICIT_SYNC);
-#endif /* CONFIG_MALI_DMA_FENCE */
+			katom->fence = sync_fence_fdget(fence.basep.fd);
+			if (katom->fence == NULL)
+				return -EINVAL;
 		}
 		break;
-#endif /* CONFIG_SYNC || CONFIG_SYNC_FILE */
+#endif				/* CONFIG_SYNC */
 	case BASE_JD_REQ_SOFT_JIT_ALLOC:
 		return kbase_jit_allocate_prepare(katom);
 	case BASE_JD_REQ_SOFT_REPLAY:
-		break;
 	case BASE_JD_REQ_SOFT_JIT_FREE:
-		return kbase_jit_free_prepare(katom);
+		break;
 	case BASE_JD_REQ_SOFT_EVENT_WAIT:
 	case BASE_JD_REQ_SOFT_EVENT_SET:
 	case BASE_JD_REQ_SOFT_EVENT_RESET:
@@ -1431,17 +1439,25 @@ void kbase_finish_soft_job(struct kbase_jd_atom *katom)
 	case BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME:
 		/* Nothing to do */
 		break;
-#if defined(CONFIG_SYNC) || defined(CONFIG_SYNC_FILE)
+#ifdef CONFIG_SYNC
 	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
-		/* If fence has not yet been signaled, do it now */
-		kbase_sync_fence_out_trigger(katom, katom->event_code ==
-				BASE_JD_EVENT_DONE ? 0 : -EFAULT);
+		/* If fence has not yet been signalled, do it now */
+		if (katom->fence) {
+			kbase_fence_trigger(katom, katom->event_code ==
+					BASE_JD_EVENT_DONE ? 0 : -EFAULT);
+			sync_fence_put(katom->fence);
+			katom->fence = NULL;
+		}
 		break;
 	case BASE_JD_REQ_SOFT_FENCE_WAIT:
-		/* Release katom's reference to fence object */
-		kbase_sync_fence_in_remove(katom);
+		/* Release the reference to the fence object */
+		if(katom->fence) {
+			sync_fence_put(katom->fence);
+			katom->fence = NULL;
+		}
 		break;
-#endif /* CONFIG_SYNC || CONFIG_SYNC_FILE */
+#endif				/* CONFIG_SYNC */
+
 	case BASE_JD_REQ_SOFT_DEBUG_COPY:
 		kbase_debug_copy_finish(katom);
 		break;
@@ -1454,9 +1470,6 @@ void kbase_finish_soft_job(struct kbase_jd_atom *katom)
 	case BASE_JD_REQ_SOFT_EXT_RES_UNMAP:
 		kbase_ext_res_finish(katom);
 		break;
-	case BASE_JD_REQ_SOFT_JIT_FREE:
-		kbase_jit_free_finish(katom);
-		break;
 	}
 }
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_sync.c b/drivers/gpu/arm/midgard/mali_kbase_sync.c
new file mode 100644
index 0000000..c5fb981
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_sync.c
@@ -0,0 +1,182 @@
+/*
+ *
+ * (C) COPYRIGHT 2012-2015 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#ifdef CONFIG_SYNC
+
+#include <linux/seq_file.h>
+#include "sync.h"
+#include <mali_kbase.h>
+#include <mali_kbase_sync.h>
+
+struct mali_sync_timeline {
+	struct sync_timeline timeline;
+	atomic_t counter;
+	atomic_t signalled;
+};
+
+struct mali_sync_pt {
+	struct sync_pt pt;
+	int order;
+	int result;
+};
+
+static struct mali_sync_timeline *to_mali_sync_timeline(struct sync_timeline *timeline)
+{
+	return container_of(timeline, struct mali_sync_timeline, timeline);
+}
+
+static struct mali_sync_pt *to_mali_sync_pt(struct sync_pt *pt)
+{
+	return container_of(pt, struct mali_sync_pt, pt);
+}
+
+static struct sync_pt *timeline_dup(struct sync_pt *pt)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	struct mali_sync_pt *new_mpt;
+	struct sync_pt *new_pt = sync_pt_create(sync_pt_parent(pt), sizeof(struct mali_sync_pt));
+
+	if (!new_pt)
+		return NULL;
+
+	new_mpt = to_mali_sync_pt(new_pt);
+	new_mpt->order = mpt->order;
+	new_mpt->result = mpt->result;
+
+	return new_pt;
+}
+
+static int timeline_has_signaled(struct sync_pt *pt)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(sync_pt_parent(pt));
+	int result = mpt->result;
+
+	int diff = atomic_read(&mtl->signalled) - mpt->order;
+
+	if (diff >= 0)
+		return (result < 0) ? result : 1;
+
+	return 0;
+}
+
+static int timeline_compare(struct sync_pt *a, struct sync_pt *b)
+{
+	struct mali_sync_pt *ma = container_of(a, struct mali_sync_pt, pt);
+	struct mali_sync_pt *mb = container_of(b, struct mali_sync_pt, pt);
+
+	int diff = ma->order - mb->order;
+
+	if (diff == 0)
+		return 0;
+
+	return (diff < 0) ? -1 : 1;
+}
+
+static void timeline_value_str(struct sync_timeline *timeline, char *str,
+			       int size)
+{
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(timeline);
+
+	snprintf(str, size, "%d", atomic_read(&mtl->signalled));
+}
+
+static void pt_value_str(struct sync_pt *pt, char *str, int size)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+
+	snprintf(str, size, "%d(%d)", mpt->order, mpt->result);
+}
+
+static struct sync_timeline_ops mali_timeline_ops = {
+	.driver_name = "Mali",
+	.dup = timeline_dup,
+	.has_signaled = timeline_has_signaled,
+	.compare = timeline_compare,
+	.timeline_value_str = timeline_value_str,
+	.pt_value_str       = pt_value_str,
+};
+
+int kbase_sync_timeline_is_ours(struct sync_timeline *timeline)
+{
+	return timeline->ops == &mali_timeline_ops;
+}
+
+struct sync_timeline *kbase_sync_timeline_alloc(const char *name)
+{
+	struct sync_timeline *tl;
+	struct mali_sync_timeline *mtl;
+
+	tl = sync_timeline_create(&mali_timeline_ops, sizeof(struct mali_sync_timeline), name);
+	if (!tl)
+		return NULL;
+
+	/* Set the counter in our private struct */
+	mtl = to_mali_sync_timeline(tl);
+	atomic_set(&mtl->counter, 0);
+	atomic_set(&mtl->signalled, 0);
+
+	return tl;
+}
+
+struct sync_pt *kbase_sync_pt_alloc(struct sync_timeline *parent)
+{
+	struct sync_pt *pt = sync_pt_create(parent, sizeof(struct mali_sync_pt));
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(parent);
+	struct mali_sync_pt *mpt;
+
+	if (!pt)
+		return NULL;
+
+	mpt = to_mali_sync_pt(pt);
+	mpt->order = atomic_inc_return(&mtl->counter);
+	mpt->result = 0;
+
+	return pt;
+}
+
+void kbase_sync_signal_pt(struct sync_pt *pt, int result)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(sync_pt_parent(pt));
+	int signalled;
+	int diff;
+
+	mpt->result = result;
+
+	do {
+		signalled = atomic_read(&mtl->signalled);
+
+		diff = signalled - mpt->order;
+
+		if (diff > 0) {
+			/* The timeline is already at or ahead of this point.
+			 * This should not happen unless userspace has been
+			 * signalling fences out of order, so warn but don't
+			 * violate the sync_pt API.
+			 * The warning is only in debug builds to prevent
+			 * a malicious user being able to spam dmesg.
+			 */
+#ifdef CONFIG_MALI_DEBUG
+			pr_err("Fences were triggered in a different order to allocation!");
+#endif				/* CONFIG_MALI_DEBUG */
+			return;
+		}
+	} while (atomic_cmpxchg(&mtl->signalled, signalled, mpt->order) != signalled);
+}
+
+#endif				/* CONFIG_SYNC */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_sync.h b/drivers/gpu/arm/midgard/mali_kbase_sync.h
index de72147..820bddc 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_sync.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_sync.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -15,189 +15,86 @@
 
 
 
+
+
 /**
  * @file mali_kbase_sync.h
  *
- * This file contains our internal "API" for explicit fences.
- * It hides the implementation details of the actual explicit fence mechanism
- * used (Android fences or sync file with DMA fences).
  */
 
 #ifndef MALI_KBASE_SYNC_H
 #define MALI_KBASE_SYNC_H
 
-#include <linux/syscalls.h>
-#ifdef CONFIG_SYNC
-#include <sync.h>
-#endif
-#ifdef CONFIG_SYNC_FILE
-#include "mali_kbase_fence_defs.h"
-#include <linux/sync_file.h>
-#endif
+#include "sync.h"
 
-#include "mali_kbase.h"
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
+/* For backwards compatiblility with kernels before 3.17. After 3.17
+ * sync_pt_parent is included in the kernel. */
+static inline struct sync_timeline *sync_pt_parent(struct sync_pt *pt)
+{
+	return pt->parent;
+}
+#endif
 
-/**
- * struct kbase_sync_fence_info - Information about a fence
- * @fence: Pointer to fence (type is void*, as underlaying struct can differ)
- * @name: The name given to this fence when it was created
- * @status: < 0 means error, 0 means active, 1 means signaled
- *
- * Use kbase_sync_fence_in_info_get() or kbase_sync_fence_out_info_get()
- * to get the information.
- */
-struct kbase_sync_fence_info {
-	void *fence;
-	char name[32];
-	int status;
-};
+static inline int kbase_fence_get_status(struct sync_fence *fence)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
+	return fence->status;
+#else
+	return atomic_read(&fence->status);
+#endif
+}
 
-/**
- * kbase_sync_fence_stream_create() - Create a stream object
- * @name: Name of stream (only used to ease debugging/visualization)
- * @out_fd: A file descriptor representing the created stream object
- *
- * Can map down to a timeline implementation in some implementations.
+/*
+ * Create a stream object.
+ * Built on top of timeline object.
  * Exposed as a file descriptor.
  * Life-time controlled via the file descriptor:
  * - dup to add a ref
  * - close to remove a ref
- *
- * return: 0 on success, < 0 on error
- */
-int kbase_sync_fence_stream_create(const char *name, int *const out_fd);
-
-/**
- * kbase_sync_fence_out_create Create an explicit output fence to specified atom
- * @katom: Atom to assign the new explicit fence to
- * @stream_fd: File descriptor for stream object to create fence on
- *
- * return: Valid file descriptor to fence or < 0 on error
  */
-int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int stream_fd);
+int kbase_stream_create(const char *name, int *const out_fd);
 
-/**
- * kbase_sync_fence_in_from_fd() Assigns an existing fence to specified atom
- * @katom: Atom to assign the existing explicit fence to
- * @fd: File descriptor to an existing fence
- *
- * Assigns an explicit input fence to atom.
- * This can later be waited for by calling @kbase_sync_fence_in_wait
- *
- * return: 0 on success, < 0 on error
+/*
+ * Create a fence in a stream object
  */
-int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd);
+int kbase_stream_create_fence(int tl_fd);
 
-/**
- * kbase_sync_fence_validate() - Validate a fd to be a valid fence
- * @fd: File descriptor to check
+/*
+ * Validate a fd to be a valid fence
+ * No reference is taken.
  *
  * This function is only usable to catch unintentional user errors early,
  * it does not stop malicious code changing the fd after this function returns.
- *
- * return 0: if fd is for a valid fence, < 0 if invalid
  */
-int kbase_sync_fence_validate(int fd);
+int kbase_fence_validate(int fd);
 
-/**
- * kbase_sync_fence_out_trigger - Signal explicit output fence attached on katom
- * @katom: Atom with an explicit fence to signal
- * @result: < 0 means signal with error, 0 >= indicates success
- *
- * Signal output fence attached on katom and remove the fence from the atom.
- *
- * return: The "next" event code for atom, typically JOB_CANCELLED or EVENT_DONE
- */
-enum base_jd_event_code
-kbase_sync_fence_out_trigger(struct kbase_jd_atom *katom, int result);
+/* Returns true if the specified timeline is allocated by Mali */
+int kbase_sync_timeline_is_ours(struct sync_timeline *timeline);
 
-/**
- * kbase_sync_fence_in_wait() - Wait for explicit input fence to be signaled
- * @katom: Atom with explicit fence to wait for
- *
- * If the fence is already signaled, then 0 is returned, and the caller must
- * continue processing of the katom.
- *
- * If the fence isn't already signaled, then this kbase_sync framework will
- * take responsibility to continue the processing once the fence is signaled.
+/* Allocates a timeline for Mali
  *
- * return: 0 if already signaled, otherwise 1
+ * One timeline should be allocated per API context.
  */
-int kbase_sync_fence_in_wait(struct kbase_jd_atom *katom);
+struct sync_timeline *kbase_sync_timeline_alloc(const char *name);
 
-/**
- * kbase_sync_fence_in_cancel_wait() - Cancel explicit input fence waits
- * @katom: Atom to cancel wait for
+/* Allocates a sync point within the timeline.
  *
- * This function is fully responsible for continuing processing of this atom
- * (remove_waiting_soft_job + finish_soft_job + jd_done + js_sched_all)
- */
-void kbase_sync_fence_in_cancel_wait(struct kbase_jd_atom *katom);
-
-/**
- * kbase_sync_fence_in_remove() - Remove the input fence from the katom
- * @katom: Atom to remove explicit input fence for
+ * The timeline must be the one allocated by kbase_sync_timeline_alloc
  *
- * This will also release the corresponding reference.
- */
-void kbase_sync_fence_in_remove(struct kbase_jd_atom *katom);
-
-/**
- * kbase_sync_fence_out_remove() - Remove the output fence from the katom
- * @katom: Atom to remove explicit output fence for
- *
- * This will also release the corresponding reference.
- */
-void kbase_sync_fence_out_remove(struct kbase_jd_atom *katom);
-
-/**
- * kbase_sync_fence_close_fd() - Close a file descriptor representing a fence
- * @fd: File descriptor to close
+ * Sync points must be triggered in *exactly* the same order as they are allocated.
  */
-static inline void kbase_sync_fence_close_fd(int fd)
-{
-	sys_close(fd);
-}
+struct sync_pt *kbase_sync_pt_alloc(struct sync_timeline *parent);
 
-/**
- * kbase_sync_fence_in_info_get() - Retrieves information about input fence
- * @katom: Atom to get fence information from
- * @info: Struct to be filled with fence information
+/* Signals a particular sync point
  *
- * return: 0 on success, < 0 on error
- */
-int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom,
-				 struct kbase_sync_fence_info *info);
-
-/**
- * kbase_sync_fence_out_info_get() - Retrieves information about output fence
- * @katom: Atom to get fence information from
- * @info: Struct to be filled with fence information
+ * Sync points must be triggered in *exactly* the same order as they are allocated.
  *
- * return: 0 on success, < 0 on error
- */
-int kbase_sync_fence_out_info_get(struct kbase_jd_atom *katom,
-				  struct kbase_sync_fence_info *info);
-
-/**
- * kbase_sync_status_string() - Get string matching @status
- * @status: Value of fence status.
+ * If they are signalled in the wrong order then a message will be printed in debug
+ * builds and otherwise attempts to signal order sync_pts will be ignored.
  *
- * return: Pointer to string describing @status.
- */
-const char *kbase_sync_status_string(int status);
-
-/*
- * Internal worker used to continue processing of atom.
+ * result can be negative to indicate error, any other value is interpreted as success.
  */
-void kbase_sync_fence_wait_worker(struct work_struct *data);
+void kbase_sync_signal_pt(struct sync_pt *pt, int result);
 
-#ifdef CONFIG_MALI_FENCE_DEBUG
-/**
- * kbase_sync_fence_in_dump() Trigger a debug dump of atoms input fence state
- * @katom: Atom to trigger fence debug dump for
- */
-void kbase_sync_fence_in_dump(struct kbase_jd_atom *katom);
 #endif
-
-#endif /* MALI_KBASE_SYNC_H */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_sync_android.c b/drivers/gpu/arm/midgard/mali_kbase_sync_android.c
deleted file mode 100644
index d7349dc..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_sync_android.c
+++ /dev/null
@@ -1,537 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/*
- * Code for supporting explicit Android fences (CONFIG_SYNC)
- * Known to be good for kernels 4.5 and earlier.
- * Replaced with CONFIG_SYNC_FILE for 4.9 and later kernels
- * (see mali_kbase_sync_file.c)
- */
-
-#include <linux/sched.h>
-#include <linux/fdtable.h>
-#include <linux/file.h>
-#include <linux/fs.h>
-#include <linux/module.h>
-#include <linux/anon_inodes.h>
-#include <linux/version.h>
-#include "sync.h"
-#include <mali_kbase.h>
-#include <mali_kbase_sync.h>
-
-struct mali_sync_timeline {
-	struct sync_timeline timeline;
-	atomic_t counter;
-	atomic_t signaled;
-};
-
-struct mali_sync_pt {
-	struct sync_pt pt;
-	int order;
-	int result;
-};
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
-/* For backwards compatibility with kernels before 3.17. After 3.17
- * sync_pt_parent is included in the kernel. */
-static inline struct sync_timeline *sync_pt_parent(struct sync_pt *pt)
-{
-	return pt->parent;
-}
-#endif
-
-static struct mali_sync_timeline *to_mali_sync_timeline(
-						struct sync_timeline *timeline)
-{
-	return container_of(timeline, struct mali_sync_timeline, timeline);
-}
-
-static struct mali_sync_pt *to_mali_sync_pt(struct sync_pt *pt)
-{
-	return container_of(pt, struct mali_sync_pt, pt);
-}
-
-static struct sync_pt *timeline_dup(struct sync_pt *pt)
-{
-	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
-	struct mali_sync_pt *new_mpt;
-	struct sync_pt *new_pt = sync_pt_create(sync_pt_parent(pt),
-						sizeof(struct mali_sync_pt));
-
-	if (!new_pt)
-		return NULL;
-
-	new_mpt = to_mali_sync_pt(new_pt);
-	new_mpt->order = mpt->order;
-	new_mpt->result = mpt->result;
-
-	return new_pt;
-}
-
-static int timeline_has_signaled(struct sync_pt *pt)
-{
-	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
-	struct mali_sync_timeline *mtl = to_mali_sync_timeline(
-							sync_pt_parent(pt));
-	int result = mpt->result;
-
-	int diff = atomic_read(&mtl->signaled) - mpt->order;
-
-	if (diff >= 0)
-		return (result < 0) ? result : 1;
-
-	return 0;
-}
-
-static int timeline_compare(struct sync_pt *a, struct sync_pt *b)
-{
-	struct mali_sync_pt *ma = container_of(a, struct mali_sync_pt, pt);
-	struct mali_sync_pt *mb = container_of(b, struct mali_sync_pt, pt);
-
-	int diff = ma->order - mb->order;
-
-	if (diff == 0)
-		return 0;
-
-	return (diff < 0) ? -1 : 1;
-}
-
-static void timeline_value_str(struct sync_timeline *timeline, char *str,
-			       int size)
-{
-	struct mali_sync_timeline *mtl = to_mali_sync_timeline(timeline);
-
-	snprintf(str, size, "%d", atomic_read(&mtl->signaled));
-}
-
-static void pt_value_str(struct sync_pt *pt, char *str, int size)
-{
-	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
-
-	snprintf(str, size, "%d(%d)", mpt->order, mpt->result);
-}
-
-static struct sync_timeline_ops mali_timeline_ops = {
-	.driver_name = "Mali",
-	.dup = timeline_dup,
-	.has_signaled = timeline_has_signaled,
-	.compare = timeline_compare,
-	.timeline_value_str = timeline_value_str,
-	.pt_value_str       = pt_value_str,
-};
-
-/* Allocates a timeline for Mali
- *
- * One timeline should be allocated per API context.
- */
-static struct sync_timeline *mali_sync_timeline_alloc(const char *name)
-{
-	struct sync_timeline *tl;
-	struct mali_sync_timeline *mtl;
-
-	tl = sync_timeline_create(&mali_timeline_ops,
-				  sizeof(struct mali_sync_timeline), name);
-	if (!tl)
-		return NULL;
-
-	/* Set the counter in our private struct */
-	mtl = to_mali_sync_timeline(tl);
-	atomic_set(&mtl->counter, 0);
-	atomic_set(&mtl->signaled, 0);
-
-	return tl;
-}
-
-static int kbase_stream_close(struct inode *inode, struct file *file)
-{
-	struct sync_timeline *tl;
-
-	tl = (struct sync_timeline *)file->private_data;
-	sync_timeline_destroy(tl);
-	return 0;
-}
-
-static const struct file_operations stream_fops = {
-	.owner = THIS_MODULE,
-	.release = kbase_stream_close,
-};
-
-int kbase_sync_fence_stream_create(const char *name, int *const out_fd)
-{
-	struct sync_timeline *tl;
-
-	if (!out_fd)
-		return -EINVAL;
-
-	tl = mali_sync_timeline_alloc(name);
-	if (!tl)
-		return -EINVAL;
-
-	*out_fd = anon_inode_getfd(name, &stream_fops, tl, O_RDONLY|O_CLOEXEC);
-
-	if (*out_fd < 0) {
-		sync_timeline_destroy(tl);
-		return -EINVAL;
-	}
-
-	return 0;
-}
-
-/* Allocates a sync point within the timeline.
- *
- * The timeline must be the one allocated by kbase_sync_timeline_alloc
- *
- * Sync points must be triggered in *exactly* the same order as they are
- * allocated.
- */
-static struct sync_pt *kbase_sync_pt_alloc(struct sync_timeline *parent)
-{
-	struct sync_pt *pt = sync_pt_create(parent,
-					    sizeof(struct mali_sync_pt));
-	struct mali_sync_timeline *mtl = to_mali_sync_timeline(parent);
-	struct mali_sync_pt *mpt;
-
-	if (!pt)
-		return NULL;
-
-	mpt = to_mali_sync_pt(pt);
-	mpt->order = atomic_inc_return(&mtl->counter);
-	mpt->result = 0;
-
-	return pt;
-}
-
-int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int tl_fd)
-{
-	struct sync_timeline *tl;
-	struct sync_pt *pt;
-	struct sync_fence *fence;
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0)
-	struct files_struct *files;
-	struct fdtable *fdt;
-#endif
-	int fd;
-	struct file *tl_file;
-
-	tl_file = fget(tl_fd);
-	if (tl_file == NULL)
-		return -EBADF;
-
-	if (tl_file->f_op != &stream_fops) {
-		fd = -EBADF;
-		goto out;
-	}
-
-	tl = tl_file->private_data;
-
-	pt = kbase_sync_pt_alloc(tl);
-	if (!pt) {
-		fd = -EFAULT;
-		goto out;
-	}
-
-	fence = sync_fence_create("mali_fence", pt);
-	if (!fence) {
-		sync_pt_free(pt);
-		fd = -EFAULT;
-		goto out;
-	}
-
-	/* from here the fence owns the sync_pt */
-
-	/* create a fd representing the fence */
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0)
-	fd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);
-	if (fd < 0) {
-		sync_fence_put(fence);
-		goto out;
-	}
-#else
-	fd = get_unused_fd();
-	if (fd < 0) {
-		sync_fence_put(fence);
-		goto out;
-	}
-
-	files = current->files;
-	spin_lock(&files->file_lock);
-	fdt = files_fdtable(files);
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 4, 0)
-	__set_close_on_exec(fd, fdt);
-#else
-	FD_SET(fd, fdt->close_on_exec);
-#endif
-	spin_unlock(&files->file_lock);
-#endif  /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0) */
-
-	/* bind fence to the new fd */
-	sync_fence_install(fence, fd);
-
-	katom->fence = sync_fence_fdget(fd);
-	if (katom->fence == NULL) {
-		/* The only way the fence can be NULL is if userspace closed it
-		 * for us, so we don't need to clear it up */
-		fd = -EINVAL;
-		goto out;
-	}
-
-out:
-	fput(tl_file);
-
-	return fd;
-}
-
-int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd)
-{
-	katom->fence = sync_fence_fdget(fd);
-	return katom->fence ? 0 : -ENOENT;
-}
-
-int kbase_sync_fence_validate(int fd)
-{
-	struct sync_fence *fence;
-
-	fence = sync_fence_fdget(fd);
-	if (!fence)
-		return -EINVAL;
-
-	sync_fence_put(fence);
-	return 0;
-}
-
-/* Returns true if the specified timeline is allocated by Mali */
-static int kbase_sync_timeline_is_ours(struct sync_timeline *timeline)
-{
-	return timeline->ops == &mali_timeline_ops;
-}
-
-/* Signals a particular sync point
- *
- * Sync points must be triggered in *exactly* the same order as they are
- * allocated.
- *
- * If they are signaled in the wrong order then a message will be printed in
- * debug builds and otherwise attempts to signal order sync_pts will be ignored.
- *
- * result can be negative to indicate error, any other value is interpreted as
- * success.
- */
-static void kbase_sync_signal_pt(struct sync_pt *pt, int result)
-{
-	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
-	struct mali_sync_timeline *mtl = to_mali_sync_timeline(
-							sync_pt_parent(pt));
-	int signaled;
-	int diff;
-
-	mpt->result = result;
-
-	do {
-		signaled = atomic_read(&mtl->signaled);
-
-		diff = signaled - mpt->order;
-
-		if (diff > 0) {
-			/* The timeline is already at or ahead of this point.
-			 * This should not happen unless userspace has been
-			 * signaling fences out of order, so warn but don't
-			 * violate the sync_pt API.
-			 * The warning is only in debug builds to prevent
-			 * a malicious user being able to spam dmesg.
-			 */
-#ifdef CONFIG_MALI_DEBUG
-			pr_err("Fences were triggered in a different order to allocation!");
-#endif				/* CONFIG_MALI_DEBUG */
-			return;
-		}
-	} while (atomic_cmpxchg(&mtl->signaled,
-				signaled, mpt->order) != signaled);
-}
-
-enum base_jd_event_code
-kbase_sync_fence_out_trigger(struct kbase_jd_atom *katom, int result)
-{
-	struct sync_pt *pt;
-	struct sync_timeline *timeline;
-
-	if (!katom->fence)
-		return BASE_JD_EVENT_JOB_CANCELLED;
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
-	if (!list_is_singular(&katom->fence->pt_list_head)) {
-#else
-	if (katom->fence->num_fences != 1) {
-#endif
-		/* Not exactly one item in the list - so it didn't (directly)
-		 * come from us */
-		return BASE_JD_EVENT_JOB_CANCELLED;
-	}
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
-	pt = list_first_entry(&katom->fence->pt_list_head,
-			      struct sync_pt, pt_list);
-#else
-	pt = container_of(katom->fence->cbs[0].sync_pt, struct sync_pt, base);
-#endif
-	timeline = sync_pt_parent(pt);
-
-	if (!kbase_sync_timeline_is_ours(timeline)) {
-		/* Fence has a sync_pt which isn't ours! */
-		return BASE_JD_EVENT_JOB_CANCELLED;
-	}
-
-	kbase_sync_signal_pt(pt, result);
-
-	sync_timeline_signal(timeline);
-
-	kbase_sync_fence_out_remove(katom);
-
-	return (result < 0) ? BASE_JD_EVENT_JOB_CANCELLED : BASE_JD_EVENT_DONE;
-}
-
-static inline int kbase_fence_get_status(struct sync_fence *fence)
-{
-	if (!fence)
-		return -ENOENT;
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
-	return fence->status;
-#else
-	return atomic_read(&fence->status);
-#endif
-}
-
-static void kbase_fence_wait_callback(struct sync_fence *fence,
-				      struct sync_fence_waiter *waiter)
-{
-	struct kbase_jd_atom *katom = container_of(waiter,
-					struct kbase_jd_atom, sync_waiter);
-	struct kbase_context *kctx = katom->kctx;
-
-	/* Propagate the fence status to the atom.
-	 * If negative then cancel this atom and its dependencies.
-	 */
-	if (kbase_fence_get_status(fence) < 0)
-		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
-
-	/* To prevent a potential deadlock we schedule the work onto the
-	 * job_done_wq workqueue
-	 *
-	 * The issue is that we may signal the timeline while holding
-	 * kctx->jctx.lock and the callbacks are run synchronously from
-	 * sync_timeline_signal. So we simply defer the work.
-	 */
-
-	INIT_WORK(&katom->work, kbase_sync_fence_wait_worker);
-	queue_work(kctx->jctx.job_done_wq, &katom->work);
-}
-
-int kbase_sync_fence_in_wait(struct kbase_jd_atom *katom)
-{
-	int ret;
-
-	sync_fence_waiter_init(&katom->sync_waiter, kbase_fence_wait_callback);
-
-	ret = sync_fence_wait_async(katom->fence, &katom->sync_waiter);
-
-	if (ret == 1) {
-		/* Already signaled */
-		return 0;
-	}
-
-	if (ret < 0) {
-		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
-		/* We should cause the dependent jobs in the bag to be failed,
-		 * to do this we schedule the work queue to complete this job */
-		INIT_WORK(&katom->work, kbase_sync_fence_wait_worker);
-		queue_work(katom->kctx->jctx.job_done_wq, &katom->work);
-	}
-
-	return 1;
-}
-
-void kbase_sync_fence_in_cancel_wait(struct kbase_jd_atom *katom)
-{
-	if (sync_fence_cancel_async(katom->fence, &katom->sync_waiter) != 0) {
-		/* The wait wasn't cancelled - leave the cleanup for
-		 * kbase_fence_wait_callback */
-		return;
-	}
-
-	/* Wait was cancelled - zap the atoms */
-	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
-
-	kbasep_remove_waiting_soft_job(katom);
-	kbase_finish_soft_job(katom);
-
-	if (jd_done_nolock(katom, NULL))
-		kbase_js_sched_all(katom->kctx->kbdev);
-}
-
-void kbase_sync_fence_out_remove(struct kbase_jd_atom *katom)
-{
-	if (katom->fence) {
-		sync_fence_put(katom->fence);
-		katom->fence = NULL;
-	}
-}
-
-void kbase_sync_fence_in_remove(struct kbase_jd_atom *katom)
-{
-	if (katom->fence) {
-		sync_fence_put(katom->fence);
-		katom->fence = NULL;
-	}
-}
-
-int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom,
-				 struct kbase_sync_fence_info *info)
-{
-	if (!katom->fence)
-		return -ENOENT;
-
-	info->fence = katom->fence;
-	info->status = kbase_fence_get_status(katom->fence);
-	strlcpy(info->name, katom->fence->name, sizeof(info->name));
-
-	return 0;
-}
-
-int kbase_sync_fence_out_info_get(struct kbase_jd_atom *katom,
-				 struct kbase_sync_fence_info *info)
-{
-	if (!katom->fence)
-		return -ENOENT;
-
-	info->fence = katom->fence;
-	info->status = kbase_fence_get_status(katom->fence);
-	strlcpy(info->name, katom->fence->name, sizeof(info->name));
-
-	return 0;
-}
-
-#ifdef CONFIG_MALI_FENCE_DEBUG
-void kbase_sync_fence_in_dump(struct kbase_jd_atom *katom)
-{
-	/* Dump out the full state of all the Android sync fences.
-	 * The function sync_dump() isn't exported to modules, so force
-	 * sync_fence_wait() to time out to trigger sync_dump().
-	 */
-	if (katom->fence)
-		sync_fence_wait(katom->fence, 1);
-}
-#endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_sync_common.c b/drivers/gpu/arm/midgard/mali_kbase_sync_common.c
deleted file mode 100644
index 457def2..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_sync_common.c
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/*
- * @file mali_kbase_sync_common.c
- *
- * Common code for our explicit fence functionality
- */
-
-#include <linux/workqueue.h>
-#include "mali_kbase.h"
-
-void kbase_sync_fence_wait_worker(struct work_struct *data)
-{
-	struct kbase_jd_atom *katom;
-
-	katom = container_of(data, struct kbase_jd_atom, work);
-	kbase_soft_event_wait_callback(katom);
-}
-
-const char *kbase_sync_status_string(int status)
-{
-	if (status == 0)
-		return "signaled";
-	else if (status > 0)
-		return "active";
-	else
-		return "error";
-}
diff --git a/drivers/gpu/arm/midgard/mali_kbase_sync_file.c b/drivers/gpu/arm/midgard/mali_kbase_sync_file.c
deleted file mode 100644
index 4e1621c..0000000
--- a/drivers/gpu/arm/midgard/mali_kbase_sync_file.c
+++ /dev/null
@@ -1,339 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/*
- * Code for supporting explicit Linux fences (CONFIG_SYNC_FILE)
- * Introduced in kernel 4.9.
- * Android explicit fences (CONFIG_SYNC) can be used for older kernels
- * (see mali_kbase_sync_android.c)
- */
-
-#include <linux/sched.h>
-#include <linux/fdtable.h>
-#include <linux/file.h>
-#include <linux/fs.h>
-#include <linux/module.h>
-#include <linux/anon_inodes.h>
-#include <linux/version.h>
-#include <linux/uaccess.h>
-#include <linux/sync_file.h>
-#include <linux/slab.h>
-#include "mali_kbase_fence_defs.h"
-#include "mali_kbase_sync.h"
-#include "mali_kbase_fence.h"
-#include "mali_kbase.h"
-
-static const struct file_operations stream_fops = {
-	.owner = THIS_MODULE
-};
-
-int kbase_sync_fence_stream_create(const char *name, int *const out_fd)
-{
-	if (!out_fd)
-		return -EINVAL;
-
-	*out_fd = anon_inode_getfd(name, &stream_fops, NULL,
-				   O_RDONLY | O_CLOEXEC);
-	if (*out_fd < 0)
-		return -EINVAL;
-
-	return 0;
-}
-
-int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int stream_fd)
-{
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence;
-#else
-	struct dma_fence *fence;
-#endif
-	struct sync_file *sync_file;
-	int fd;
-
-	fence = kbase_fence_out_new(katom);
-	if (!fence)
-		return -ENOMEM;
-
-	/* Take an extra reference to the fence on behalf of the katom.
-	 * This is needed because sync_file_create() will take ownership of
-	 * one of these refs */
-	dma_fence_get(fence);
-
-	/* create a sync_file fd representing the fence */
-	sync_file = sync_file_create(fence);
-	if (!sync_file) {
-		dma_fence_put(fence);
-		kbase_fence_out_remove(katom);
-		return -ENOMEM;
-	}
-
-	fd = get_unused_fd_flags(O_CLOEXEC);
-	if (fd < 0) {
-		fput(sync_file->file);
-		kbase_fence_out_remove(katom);
-		return fd;
-	}
-
-	fd_install(fd, sync_file->file);
-
-	return fd;
-}
-
-int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd)
-{
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence = sync_file_get_fence(fd);
-#else
-	struct dma_fence *fence = sync_file_get_fence(fd);
-#endif
-
-	if (!fence)
-		return -ENOENT;
-
-	kbase_fence_fence_in_set(katom, fence);
-
-	return 0;
-}
-
-int kbase_sync_fence_validate(int fd)
-{
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence = sync_file_get_fence(fd);
-#else
-	struct dma_fence *fence = sync_file_get_fence(fd);
-#endif
-
-	if (!fence)
-		return -EINVAL;
-
-	dma_fence_put(fence);
-
-	return 0; /* valid */
-}
-
-enum base_jd_event_code
-kbase_sync_fence_out_trigger(struct kbase_jd_atom *katom, int result)
-{
-	int res;
-
-	if (!kbase_fence_out_is_ours(katom)) {
-		/* Not our fence */
-		return BASE_JD_EVENT_JOB_CANCELLED;
-	}
-
-	res = kbase_fence_out_signal(katom, result);
-	if (unlikely(res < 0)) {
-		dev_warn(katom->kctx->kbdev->dev,
-				"fence_signal() failed with %d\n", res);
-	}
-
-	kbase_sync_fence_out_remove(katom);
-
-	return (result != 0) ? BASE_JD_EVENT_JOB_CANCELLED : BASE_JD_EVENT_DONE;
-}
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-static void kbase_fence_wait_callback(struct fence *fence,
-				      struct fence_cb *cb)
-#else
-static void kbase_fence_wait_callback(struct dma_fence *fence,
-				      struct dma_fence_cb *cb)
-#endif
-{
-	struct kbase_fence_cb *kcb = container_of(cb,
-				struct kbase_fence_cb,
-				fence_cb);
-	struct kbase_jd_atom *katom = kcb->katom;
-	struct kbase_context *kctx = katom->kctx;
-
-	/* Cancel atom if fence is erroneous */
-	if (dma_fence_is_signaled(kcb->fence) && kcb->fence->status < 0)
-		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
-
-	if (kbase_fence_dep_count_dec_and_test(katom)) {
-		/* We take responsibility of handling this */
-		kbase_fence_dep_count_set(katom, -1);
-
-		/* To prevent a potential deadlock we schedule the work onto the
-		 * job_done_wq workqueue
-		 *
-		 * The issue is that we may signal the timeline while holding
-		 * kctx->jctx.lock and the callbacks are run synchronously from
-		 * sync_timeline_signal. So we simply defer the work.
-		 */
-		INIT_WORK(&katom->work, kbase_sync_fence_wait_worker);
-		queue_work(kctx->jctx.job_done_wq, &katom->work);
-	}
-}
-
-int kbase_sync_fence_in_wait(struct kbase_jd_atom *katom)
-{
-	int err;
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence;
-#else
-	struct dma_fence *fence;
-#endif
-
-	fence = kbase_fence_in_get(katom);
-	if (!fence)
-		return 0; /* no input fence to wait for, good to go! */
-
-	kbase_fence_dep_count_set(katom, 1);
-
-	err = kbase_fence_add_callback(katom, fence, kbase_fence_wait_callback);
-
-	kbase_fence_put(fence);
-
-	if (likely(!err)) {
-		/* Test if the callbacks are already triggered */
-		if (kbase_fence_dep_count_dec_and_test(katom)) {
-			kbase_fence_free_callbacks(katom);
-			kbase_fence_dep_count_set(katom, -1);
-			return 0; /* Already signaled, good to go right now */
-		}
-
-		/* Callback installed, so we just need to wait for it... */
-	} else {
-		/* Failure */
-		kbase_fence_free_callbacks(katom);
-		kbase_fence_dep_count_set(katom, -1);
-
-		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
-
-		/* We should cause the dependent jobs in the bag to be failed,
-		 * to do this we schedule the work queue to complete this job */
-
-		INIT_WORK(&katom->work, kbase_sync_fence_wait_worker);
-		queue_work(katom->kctx->jctx.job_done_wq, &katom->work);
-	}
-
-	return 1; /* completion to be done later by callback/worker */
-}
-
-void kbase_sync_fence_in_cancel_wait(struct kbase_jd_atom *katom)
-{
-	if (!kbase_fence_free_callbacks(katom)) {
-		/* The wait wasn't cancelled -
-		 * leave the cleanup for kbase_fence_wait_callback */
-		return;
-	}
-
-	/* Take responsibility of completion */
-	kbase_fence_dep_count_set(katom, -1);
-
-	/* Wait was cancelled - zap the atoms */
-	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
-
-	kbasep_remove_waiting_soft_job(katom);
-	kbase_finish_soft_job(katom);
-
-	if (jd_done_nolock(katom, NULL))
-		kbase_js_sched_all(katom->kctx->kbdev);
-}
-
-void kbase_sync_fence_out_remove(struct kbase_jd_atom *katom)
-{
-	kbase_fence_out_remove(katom);
-}
-
-void kbase_sync_fence_in_remove(struct kbase_jd_atom *katom)
-{
-	kbase_fence_free_callbacks(katom);
-	kbase_fence_in_remove(katom);
-}
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-static void kbase_sync_fence_info_get(struct fence *fence,
-				      struct kbase_sync_fence_info *info)
-#else
-static void kbase_sync_fence_info_get(struct dma_fence *fence,
-				      struct kbase_sync_fence_info *info)
-#endif
-{
-	info->fence = fence;
-
-	/* translate into CONFIG_SYNC status:
-	 * < 0 : error
-	 * 0 : active
-	 * 1 : signaled
-	 */
-	if (dma_fence_is_signaled(fence)) {
-		if (fence->status < 0)
-			info->status = fence->status; /* signaled with error */
-		else
-			info->status = 1; /* signaled with success */
-	} else  {
-		info->status = 0; /* still active (unsignaled) */
-	}
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 8, 0))
-	scnprintf(info->name, sizeof(info->name), "%u#%u",
-		  fence->context, fence->seqno);
-#else
-	scnprintf(info->name, sizeof(info->name), "%llu#%u",
-		  fence->context, fence->seqno);
-#endif
-}
-
-int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom,
-				 struct kbase_sync_fence_info *info)
-{
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence;
-#else
-	struct dma_fence *fence;
-#endif
-
-	fence = kbase_fence_in_get(katom);
-	if (!fence)
-		return -ENOENT;
-
-	kbase_sync_fence_info_get(fence, info);
-
-	kbase_fence_put(fence);
-
-	return 0;
-}
-
-int kbase_sync_fence_out_info_get(struct kbase_jd_atom *katom,
-				  struct kbase_sync_fence_info *info)
-{
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-	struct fence *fence;
-#else
-	struct dma_fence *fence;
-#endif
-
-	fence = kbase_fence_out_get(katom);
-	if (!fence)
-		return -ENOENT;
-
-	kbase_sync_fence_info_get(fence, info);
-
-	kbase_fence_put(fence);
-
-	return 0;
-}
-
-
-#ifdef CONFIG_MALI_FENCE_DEBUG
-void kbase_sync_fence_in_dump(struct kbase_jd_atom *katom)
-{
-	/* Not implemented */
-}
-#endif
diff --git a/drivers/gpu/arm/midgard/mali_kbase_sync_user.c b/drivers/gpu/arm/midgard/mali_kbase_sync_user.c
new file mode 100644
index 0000000..b9baa91
--- /dev/null
+++ b/drivers/gpu/arm/midgard/mali_kbase_sync_user.c
@@ -0,0 +1,156 @@
+/*
+ *
+ * (C) COPYRIGHT 2012-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_sync_user.c
+ *
+ */
+
+#ifdef CONFIG_SYNC
+
+#include <linux/sched.h>
+#include <linux/fdtable.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/module.h>
+#include <linux/anon_inodes.h>
+#include <linux/version.h>
+#include <linux/uaccess.h>
+#include <mali_kbase_sync.h>
+
+static int kbase_stream_close(struct inode *inode, struct file *file)
+{
+	struct sync_timeline *tl;
+
+	tl = (struct sync_timeline *)file->private_data;
+	BUG_ON(!tl);
+	sync_timeline_destroy(tl);
+	return 0;
+}
+
+static const struct file_operations stream_fops = {
+	.owner = THIS_MODULE,
+	.release = kbase_stream_close,
+};
+
+int kbase_stream_create(const char *name, int *const out_fd)
+{
+	struct sync_timeline *tl;
+
+	BUG_ON(!out_fd);
+
+	tl = kbase_sync_timeline_alloc(name);
+	if (!tl)
+		return -EINVAL;
+
+	*out_fd = anon_inode_getfd(name, &stream_fops, tl, O_RDONLY | O_CLOEXEC);
+
+	if (*out_fd < 0) {
+		sync_timeline_destroy(tl);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int kbase_stream_create_fence(int tl_fd)
+{
+	struct sync_timeline *tl;
+	struct sync_pt *pt;
+	struct sync_fence *fence;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 7, 0)
+	struct files_struct *files;
+	struct fdtable *fdt;
+#endif
+	int fd;
+	struct file *tl_file;
+
+	tl_file = fget(tl_fd);
+	if (tl_file == NULL)
+		return -EBADF;
+
+	if (tl_file->f_op != &stream_fops) {
+		fd = -EBADF;
+		goto out;
+	}
+
+	tl = tl_file->private_data;
+
+	pt = kbase_sync_pt_alloc(tl);
+	if (!pt) {
+		fd = -EFAULT;
+		goto out;
+	}
+
+	fence = sync_fence_create("mali_fence", pt);
+	if (!fence) {
+		sync_pt_free(pt);
+		fd = -EFAULT;
+		goto out;
+	}
+
+	/* from here the fence owns the sync_pt */
+
+	/* create a fd representing the fence */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0)
+	fd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);
+	if (fd < 0) {
+		sync_fence_put(fence);
+		goto out;
+	}
+#else
+	fd = get_unused_fd();
+	if (fd < 0) {
+		sync_fence_put(fence);
+		goto out;
+	}
+
+	files = current->files;
+	spin_lock(&files->file_lock);
+	fdt = files_fdtable(files);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 4, 0)
+	__set_close_on_exec(fd, fdt);
+#else
+	FD_SET(fd, fdt->close_on_exec);
+#endif
+	spin_unlock(&files->file_lock);
+#endif  /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 7, 0) */
+
+	/* bind fence to the new fd */
+	sync_fence_install(fence, fd);
+
+ out:
+	fput(tl_file);
+
+	return fd;
+}
+
+int kbase_fence_validate(int fd)
+{
+	struct sync_fence *fence;
+
+	fence = sync_fence_fdget(fd);
+	if (!fence)
+		return -EINVAL;
+
+	sync_fence_put(fence);
+	return 0;
+}
+
+#endif				/* CONFIG_SYNC */
diff --git a/drivers/gpu/arm/midgard/mali_kbase_tlstream.c b/drivers/gpu/arm/midgard/mali_kbase_tlstream.c
index c952993..4c1535f 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_tlstream.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_tlstream.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -145,7 +145,6 @@ enum tl_msg_id_obj {
 	KBASE_TL_ATTRIB_ATOM_PRIORITY,
 	KBASE_TL_ATTRIB_ATOM_STATE,
 	KBASE_TL_ATTRIB_ATOM_PRIORITY_CHANGE,
-	KBASE_TL_ATTRIB_ATOM_JIT,
 	KBASE_TL_ATTRIB_AS_CONFIG,
 	KBASE_TL_EVENT_LPU_SOFTSTOP,
 	KBASE_TL_EVENT_ATOM_SOFTSTOP_EX,
@@ -160,11 +159,7 @@ enum tl_msg_id_aux {
 	KBASE_AUX_PM_STATE,
 	KBASE_AUX_PAGEFAULT,
 	KBASE_AUX_PAGESALLOC,
-	KBASE_AUX_DEVFREQ_TARGET,
-	KBASE_AUX_PROTECTED_ENTER_START,
-	KBASE_AUX_PROTECTED_ENTER_END,
-	KBASE_AUX_PROTECTED_LEAVE_START,
-	KBASE_AUX_PROTECTED_LEAVE_END
+	KBASE_AUX_DEVFREQ_TARGET
 };
 
 /*****************************************************************************/
@@ -460,13 +455,6 @@ static const struct tp_desc tp_desc_obj[] = {
 		"atom"
 	},
 	{
-		KBASE_TL_ATTRIB_ATOM_JIT,
-		__stringify(KBASE_TL_ATTRIB_ATOM_JIT),
-		"jit done for atom",
-		"@pLL",
-		"atom,edit_addr,new_addr"
-	},
-	{
 		KBASE_TL_ATTRIB_AS_CONFIG,
 		__stringify(KBASE_TL_ATTRIB_AS_CONFIG),
 		"address space attributes",
@@ -532,34 +520,6 @@ static const struct tp_desc tp_desc_aux[] = {
 		"New device frequency target",
 		"@L",
 		"target_freq"
-	},
-	{
-		KBASE_AUX_PROTECTED_ENTER_START,
-		__stringify(KBASE_AUX_PROTECTED_ENTER_START),
-		"enter protected mode start",
-		"@p",
-		"gpu"
-	},
-	{
-		KBASE_AUX_PROTECTED_ENTER_END,
-		__stringify(KBASE_AUX_PROTECTED_ENTER_END),
-		"enter protected mode end",
-		"@p",
-		"gpu"
-	},
-	{
-		KBASE_AUX_PROTECTED_LEAVE_START,
-		__stringify(KBASE_AUX_PROTECTED_LEAVE_START),
-		"leave protected mode start",
-		"@p",
-		"gpu"
-	},
-	{
-		KBASE_AUX_PROTECTED_LEAVE_END,
-		__stringify(KBASE_AUX_PROTECTED_LEAVE_END),
-		"leave protected mode end",
-		"@p",
-		"gpu"
 	}
 };
 
@@ -1388,77 +1348,21 @@ void kbase_tlstream_term(void)
 	}
 }
 
-static void kbase_create_timeline_objects(struct kbase_context *kctx)
+int kbase_tlstream_acquire(struct kbase_context *kctx, int *fd, u32 flags)
 {
-	struct kbase_device             *kbdev = kctx->kbdev;
-	unsigned int                    lpu_id;
-	unsigned int                    as_nr;
-	struct kbasep_kctx_list_element *element;
-
-	/* Create LPU objects. */
-	for (lpu_id = 0; lpu_id < kbdev->gpu_props.num_job_slots; lpu_id++) {
-		u32 *lpu =
-			&kbdev->gpu_props.props.raw_props.js_features[lpu_id];
-		KBASE_TLSTREAM_TL_SUMMARY_NEW_LPU(lpu, lpu_id, *lpu);
-	}
-
-	/* Create Address Space objects. */
-	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
-		KBASE_TLSTREAM_TL_SUMMARY_NEW_AS(&kbdev->as[as_nr], as_nr);
-
-	/* Create GPU object and make it retain all LPUs and address spaces. */
-	KBASE_TLSTREAM_TL_SUMMARY_NEW_GPU(
-			kbdev,
-			kbdev->gpu_props.props.raw_props.gpu_id,
-			kbdev->gpu_props.num_cores);
-
-	for (lpu_id = 0; lpu_id < kbdev->gpu_props.num_job_slots; lpu_id++) {
-		void *lpu =
-			&kbdev->gpu_props.props.raw_props.js_features[lpu_id];
-		KBASE_TLSTREAM_TL_SUMMARY_LIFELINK_LPU_GPU(lpu, kbdev);
-	}
-	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
-		KBASE_TLSTREAM_TL_SUMMARY_LIFELINK_AS_GPU(
-				&kbdev->as[as_nr],
-				kbdev);
-
-	/* Create object for each known context. */
-	mutex_lock(&kbdev->kctx_list_lock);
-	list_for_each_entry(element, &kbdev->kctx_list, link) {
-		KBASE_TLSTREAM_TL_SUMMARY_NEW_CTX(
-				element->kctx,
-				(u32)(element->kctx->id),
-				(u32)(element->kctx->tgid));
-	}
-	/* Before releasing the lock, reset body stream buffers.
-	 * This will prevent context creation message to be directed to both
-	 * summary and body stream.
-	 */
-	kbase_tlstream_reset_body_streams();
-	mutex_unlock(&kbdev->kctx_list_lock);
-	/* Static object are placed into summary packet that needs to be
-	 * transmitted first. Flush all streams to make it available to
-	 * user space.
-	 */
-	kbase_tlstream_flush_streams();
-}
-
-int kbase_tlstream_acquire(struct kbase_context *kctx, u32 flags)
-{
-	int ret;
 	u32 tlstream_enabled = TLSTREAM_ENABLED | flags;
 
 	if (0 == atomic_cmpxchg(&kbase_tlstream_enabled, 0, tlstream_enabled)) {
 		int rcode;
 
-		ret = anon_inode_getfd(
+		*fd = anon_inode_getfd(
 				"[mali_tlstream]",
 				&kbasep_tlstream_fops,
 				kctx,
 				O_RDONLY | O_CLOEXEC);
-		if (ret < 0) {
+		if (0 > *fd) {
 			atomic_set(&kbase_tlstream_enabled, 0);
-			return ret;
+			return *fd;
 		}
 
 		/* Reset and initialize header streams. */
@@ -1484,27 +1388,11 @@ int kbase_tlstream_acquire(struct kbase_context *kctx, u32 flags)
 				jiffies + msecs_to_jiffies(AUTOFLUSH_INTERVAL));
 		CSTD_UNUSED(rcode);
 
-		/* If job dumping is enabled, readjust the software event's
-		 * timeout as the default value of 3 seconds is often
-		 * insufficient. */
-		if (flags & BASE_TLSTREAM_JOB_DUMPING_ENABLED) {
-			dev_info(kctx->kbdev->dev,
-					"Job dumping is enabled, readjusting the software event's timeout\n");
-			atomic_set(&kctx->kbdev->js_data.soft_job_timeout_ms,
-					1800000);
-		}
-
-		/* Summary stream was cleared during acquire.
-		 * Create static timeline objects that will be
-		 * read by client.
-		 */
-		kbase_create_timeline_objects(kctx);
-
 	} else {
-		ret = -EBUSY;
+		*fd = -EBUSY;
 	}
 
-	return ret;
+	return 0;
 }
 
 void kbase_tlstream_flush_streams(void)
@@ -2231,35 +2119,6 @@ void __kbase_tlstream_tl_attrib_atom_priority_change(void *atom)
 	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_OBJ, flags);
 }
 
-void __kbase_tlstream_tl_attrib_atom_jit(
-		void *atom, u64 edit_addr, u64 new_addr)
-{
-	const u32     msg_id = KBASE_TL_ATTRIB_ATOM_JIT;
-	const size_t  msg_size =
-		sizeof(msg_id) + sizeof(u64) + sizeof(atom)
-		+ sizeof(edit_addr) + sizeof(new_addr);
-	unsigned long flags;
-	char          *buffer;
-	size_t        pos = 0;
-
-	buffer = kbasep_tlstream_msgbuf_acquire(
-			TL_STREAM_TYPE_OBJ,
-			msg_size, &flags);
-	KBASE_DEBUG_ASSERT(buffer);
-
-	pos = kbasep_tlstream_write_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_tlstream_write_timestamp(buffer, pos);
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &atom, sizeof(atom));
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &edit_addr, sizeof(edit_addr));
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &new_addr, sizeof(new_addr));
-	KBASE_DEBUG_ASSERT(msg_size == pos);
-
-	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_OBJ, flags);
-}
-
 void __kbase_tlstream_tl_attrib_as_config(
 		void *as, u64 transtab, u64 memattr, u64 transcfg)
 {
@@ -2481,92 +2340,3 @@ void __kbase_tlstream_aux_devfreq_target(u64 target_freq)
 	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_AUX, flags);
 }
 
-void __kbase_tlstream_aux_protected_enter_start(void *gpu)
-{
-	const u32     msg_id = KBASE_AUX_PROTECTED_ENTER_START;
-	const size_t  msg_size =
-		sizeof(msg_id) + sizeof(u64) + sizeof(gpu);
-	unsigned long flags;
-	char          *buffer;
-	size_t        pos = 0;
-
-	buffer = kbasep_tlstream_msgbuf_acquire(
-			TL_STREAM_TYPE_AUX,
-			msg_size, &flags);
-	KBASE_DEBUG_ASSERT(buffer);
-
-	pos = kbasep_tlstream_write_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_tlstream_write_timestamp(buffer, pos);
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &gpu, sizeof(gpu));
-	KBASE_DEBUG_ASSERT(msg_size == pos);
-
-	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_AUX, flags);
-}
-void __kbase_tlstream_aux_protected_enter_end(void *gpu)
-{
-	const u32     msg_id = KBASE_AUX_PROTECTED_ENTER_END;
-	const size_t  msg_size =
-		sizeof(msg_id) + sizeof(u64) + sizeof(gpu);
-	unsigned long flags;
-	char          *buffer;
-	size_t        pos = 0;
-
-	buffer = kbasep_tlstream_msgbuf_acquire(
-			TL_STREAM_TYPE_AUX,
-			msg_size, &flags);
-	KBASE_DEBUG_ASSERT(buffer);
-
-	pos = kbasep_tlstream_write_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_tlstream_write_timestamp(buffer, pos);
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &gpu, sizeof(gpu));
-	KBASE_DEBUG_ASSERT(msg_size == pos);
-
-	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_AUX, flags);
-}
-
-void __kbase_tlstream_aux_protected_leave_start(void *gpu)
-{
-	const u32     msg_id = KBASE_AUX_PROTECTED_LEAVE_START;
-	const size_t  msg_size =
-		sizeof(msg_id) + sizeof(u64) + sizeof(gpu);
-	unsigned long flags;
-	char          *buffer;
-	size_t        pos = 0;
-
-	buffer = kbasep_tlstream_msgbuf_acquire(
-			TL_STREAM_TYPE_AUX,
-			msg_size, &flags);
-	KBASE_DEBUG_ASSERT(buffer);
-
-	pos = kbasep_tlstream_write_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_tlstream_write_timestamp(buffer, pos);
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &gpu, sizeof(gpu));
-	KBASE_DEBUG_ASSERT(msg_size == pos);
-
-	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_AUX, flags);
-}
-void __kbase_tlstream_aux_protected_leave_end(void *gpu)
-{
-	const u32     msg_id = KBASE_AUX_PROTECTED_LEAVE_END;
-	const size_t  msg_size =
-		sizeof(msg_id) + sizeof(u64) + sizeof(gpu);
-	unsigned long flags;
-	char          *buffer;
-	size_t        pos = 0;
-
-	buffer = kbasep_tlstream_msgbuf_acquire(
-			TL_STREAM_TYPE_AUX,
-			msg_size, &flags);
-	KBASE_DEBUG_ASSERT(buffer);
-
-	pos = kbasep_tlstream_write_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_tlstream_write_timestamp(buffer, pos);
-	pos = kbasep_tlstream_write_bytes(
-			buffer, pos, &gpu, sizeof(gpu));
-	KBASE_DEBUG_ASSERT(msg_size == pos);
-
-	kbasep_tlstream_msgbuf_release(TL_STREAM_TYPE_AUX, flags);
-}
diff --git a/drivers/gpu/arm/midgard/mali_kbase_tlstream.h b/drivers/gpu/arm/midgard/mali_kbase_tlstream.h
index c0a1117..e29be71 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_tlstream.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_tlstream.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -38,6 +38,7 @@ void kbase_tlstream_term(void);
 /**
  * kbase_tlstream_acquire - acquire timeline stream file descriptor
  * @kctx:  kernel common context
+ * @fd:    timeline stream file descriptor
  * @flags: timeline stream flags
  *
  * This descriptor is meant to be used by userspace timeline to gain access to
@@ -45,11 +46,12 @@ void kbase_tlstream_term(void);
  * timeline client.
  * Only one entity can own the descriptor at any given time. Descriptor shall be
  * closed if unused. If descriptor cannot be obtained (i.e. when it is already
- * being used) return will be a negative value.
+ * being used) argument fd will contain negative value.
  *
- * Return: file descriptor on success, negative number on error
+ * Return: zero on success (this does not necessarily mean that stream
+ *         descriptor could be returned), negative number on error
  */
-int kbase_tlstream_acquire(struct kbase_context *kctx, u32 flags);
+int kbase_tlstream_acquire(struct kbase_context *kctx, int *fd, u32 flags);
 
 /**
  * kbase_tlstream_flush_streams - flush timeline streams.
@@ -135,8 +137,6 @@ void __kbase_tlstream_tl_attrib_atom_config(
 void __kbase_tlstream_tl_attrib_atom_priority(void *atom, u32 prio);
 void __kbase_tlstream_tl_attrib_atom_state(void *atom, u32 state);
 void __kbase_tlstream_tl_attrib_atom_priority_change(void *atom);
-void __kbase_tlstream_tl_attrib_atom_jit(
-		void *atom, u64 edit_addr, u64 new_addr);
 void __kbase_tlstream_tl_attrib_as_config(
 		void *as, u64 transtab, u64 memattr, u64 transcfg);
 void __kbase_tlstream_tl_event_atom_softstop_ex(void *atom);
@@ -147,10 +147,6 @@ void __kbase_tlstream_aux_pm_state(u32 core_type, u64 state);
 void __kbase_tlstream_aux_pagefault(u32 ctx_nr, u64 page_count_change);
 void __kbase_tlstream_aux_pagesalloc(u32 ctx_nr, u64 page_count);
 void __kbase_tlstream_aux_devfreq_target(u64 target_freq);
-void __kbase_tlstream_aux_protected_enter_start(void *gpu);
-void __kbase_tlstream_aux_protected_enter_end(void *gpu);
-void __kbase_tlstream_aux_protected_leave_start(void *gpu);
-void __kbase_tlstream_aux_protected_leave_end(void *gpu);
 
 #define TLSTREAM_ENABLED (1 << 31)
 
@@ -170,17 +166,10 @@ extern atomic_t kbase_tlstream_enabled;
 			__kbase_tlstream_##trace_name(__VA_ARGS__);     \
 	} while (0)
 
-#define __TRACE_IF_ENABLED_JD(trace_name, ...)                      \
-	do {                                                        \
-		int enabled = atomic_read(&kbase_tlstream_enabled); \
-		if (enabled & BASE_TLSTREAM_JOB_DUMPING_ENABLED)    \
-			__kbase_tlstream_##trace_name(__VA_ARGS__); \
-	} while (0)
-
 /*****************************************************************************/
 
 /**
- * KBASE_TLSTREAM_TL_SUMMARY_NEW_CTX - create context object in timeline
+ * kbase_tlstream_tl_summary_new_ctx - create context object in timeline
  *                                     summary
  * @context: name of the context object
  * @nr:      context number
@@ -191,11 +180,11 @@ extern atomic_t kbase_tlstream_enabled;
  * kbase context with userspace context.
  * This message is directed to timeline summary stream.
  */
-#define KBASE_TLSTREAM_TL_SUMMARY_NEW_CTX(context, nr, tgid) \
+#define kbase_tlstream_tl_summary_new_ctx(context, nr, tgid) \
 	__TRACE_IF_ENABLED(tl_summary_new_ctx, context, nr, tgid)
 
 /**
- * KBASE_TLSTREAM_TL_SUMMARY_NEW_GPU - create GPU object in timeline summary
+ * kbase_tlstream_tl_summary_new_gpu - create GPU object in timeline summary
  * @gpu:        name of the GPU object
  * @id:         id value of this GPU
  * @core_count: number of cores this GPU hosts
@@ -204,11 +193,11 @@ extern atomic_t kbase_tlstream_enabled;
  * created with two attributes: id and core count.
  * This message is directed to timeline summary stream.
  */
-#define KBASE_TLSTREAM_TL_SUMMARY_NEW_GPU(gpu, id, core_count) \
+#define kbase_tlstream_tl_summary_new_gpu(gpu, id, core_count) \
 	__TRACE_IF_ENABLED(tl_summary_new_gpu, gpu, id, core_count)
 
 /**
- * KBASE_TLSTREAM_TL_SUMMARY_NEW_LPU - create LPU object in timeline summary
+ * kbase_tlstream_tl_summary_new_lpu - create LPU object in timeline summary
  * @lpu: name of the Logical Processing Unit object
  * @nr:  sequential number assigned to this LPU
  * @fn:  property describing this LPU's functional abilities
@@ -218,11 +207,11 @@ extern atomic_t kbase_tlstream_enabled;
  * and function bearing information about this LPU abilities.
  * This message is directed to timeline summary stream.
  */
-#define KBASE_TLSTREAM_TL_SUMMARY_NEW_LPU(lpu, nr, fn) \
+#define kbase_tlstream_tl_summary_new_lpu(lpu, nr, fn) \
 	__TRACE_IF_ENABLED(tl_summary_new_lpu, lpu, nr, fn)
 
 /**
- * KBASE_TLSTREAM_TL_SUMMARY_LIFELINK_LPU_GPU - lifelink LPU object to GPU
+ * kbase_tlstream_tl_summary_lifelink_lpu_gpu - lifelink LPU object to GPU
  * @lpu: name of the Logical Processing Unit object
  * @gpu: name of the GPU object
  *
@@ -230,11 +219,11 @@ extern atomic_t kbase_tlstream_enabled;
  * along with GPU object.
  * This message is directed to timeline summary stream.
  */
-#define KBASE_TLSTREAM_TL_SUMMARY_LIFELINK_LPU_GPU(lpu, gpu) \
+#define kbase_tlstream_tl_summary_lifelink_lpu_gpu(lpu, gpu) \
 	__TRACE_IF_ENABLED(tl_summary_lifelink_lpu_gpu, lpu, gpu)
 
 /**
- * KBASE_TLSTREAM_TL_SUMMARY_NEW_AS - create address space object in timeline summary
+ * kbase_tlstream_tl_summary_new_as - create address space object in timeline summary
  * @as: name of the address space object
  * @nr: sequential number assigned to this address space
  *
@@ -243,11 +232,11 @@ extern atomic_t kbase_tlstream_enabled;
  * address space.
  * This message is directed to timeline summary stream.
  */
-#define KBASE_TLSTREAM_TL_SUMMARY_NEW_AS(as, nr) \
+#define kbase_tlstream_tl_summary_new_as(as, nr) \
 	__TRACE_IF_ENABLED(tl_summary_new_as, as, nr)
 
 /**
- * KBASE_TLSTREAM_TL_SUMMARY_LIFELINK_AS_GPU - lifelink address space object to GPU
+ * kbase_tlstream_tl_summary_lifelink_as_gpu - lifelink address space object to GPU
  * @as:  name of the address space object
  * @gpu: name of the GPU object
  *
@@ -255,11 +244,11 @@ extern atomic_t kbase_tlstream_enabled;
  * shall be deleted along with GPU object.
  * This message is directed to timeline summary stream.
  */
-#define KBASE_TLSTREAM_TL_SUMMARY_LIFELINK_AS_GPU(as, gpu) \
+#define kbase_tlstream_tl_summary_lifelink_as_gpu(as, gpu) \
 	__TRACE_IF_ENABLED(tl_summary_lifelink_as_gpu, as, gpu)
 
 /**
- * KBASE_TLSTREAM_TL_NEW_CTX - create context object in timeline
+ * kbase_tlstream_tl_new_ctx - create context object in timeline
  * @context: name of the context object
  * @nr:      context number
  * @tgid:    thread Group Id
@@ -268,11 +257,11 @@ extern atomic_t kbase_tlstream_enabled;
  * is created with context number (its attribute), that can be used to link
  * kbase context with userspace context.
  */
-#define KBASE_TLSTREAM_TL_NEW_CTX(context, nr, tgid) \
+#define kbase_tlstream_tl_new_ctx(context, nr, tgid) \
 	__TRACE_IF_ENABLED(tl_new_ctx, context, nr, tgid)
 
 /**
- * KBASE_TLSTREAM_TL_NEW_ATOM - create atom object in timeline
+ * kbase_tlstream_tl_new_atom - create atom object in timeline
  * @atom: name of the atom object
  * @nr:   sequential number assigned to this atom
  *
@@ -280,53 +269,53 @@ extern atomic_t kbase_tlstream_enabled;
  * created with atom number (its attribute) that links it with actual work
  * bucket id understood by hardware.
  */
-#define KBASE_TLSTREAM_TL_NEW_ATOM(atom, nr) \
+#define kbase_tlstream_tl_new_atom(atom, nr) \
 	__TRACE_IF_ENABLED(tl_new_atom, atom, nr)
 
 /**
- * KBASE_TLSTREAM_TL_DEL_CTX - destroy context object in timeline
+ * kbase_tlstream_tl_del_ctx - destroy context object in timeline
  * @context: name of the context object
  *
  * Function emits a timeline message informing that context object ceased to
  * exist.
  */
-#define KBASE_TLSTREAM_TL_DEL_CTX(context) \
+#define kbase_tlstream_tl_del_ctx(context) \
 	__TRACE_IF_ENABLED(tl_del_ctx, context)
 
 /**
- * KBASE_TLSTREAM_TL_DEL_ATOM - destroy atom object in timeline
+ * kbase_tlstream_tl_del_atom - destroy atom object in timeline
  * @atom: name of the atom object
  *
  * Function emits a timeline message informing that atom object ceased to
  * exist.
  */
-#define KBASE_TLSTREAM_TL_DEL_ATOM(atom) \
+#define kbase_tlstream_tl_del_atom(atom) \
 	__TRACE_IF_ENABLED(tl_del_atom, atom)
 
 /**
- * KBASE_TLSTREAM_TL_RET_CTX_LPU - retain context by LPU
+ * kbase_tlstream_tl_ret_ctx_lpu - retain context by LPU
  * @context: name of the context object
  * @lpu:     name of the Logical Processing Unit object
  *
  * Function emits a timeline message informing that context is being held
  * by LPU and must not be deleted unless it is released.
  */
-#define KBASE_TLSTREAM_TL_RET_CTX_LPU(context, lpu) \
+#define kbase_tlstream_tl_ret_ctx_lpu(context, lpu) \
 	__TRACE_IF_ENABLED(tl_ret_ctx_lpu, context, lpu)
 
 /**
- * KBASE_TLSTREAM_TL_RET_ATOM_CTX - retain atom by context
+ * kbase_tlstream_tl_ret_atom_ctx - retain atom by context
  * @atom:    name of the atom object
  * @context: name of the context object
  *
  * Function emits a timeline message informing that atom object is being held
  * by context and must not be deleted unless it is released.
  */
-#define KBASE_TLSTREAM_TL_RET_ATOM_CTX(atom, context) \
+#define kbase_tlstream_tl_ret_atom_ctx(atom, context) \
 	__TRACE_IF_ENABLED(tl_ret_atom_ctx, atom, context)
 
 /**
- * KBASE_TLSTREAM_TL_RET_ATOM_LPU - retain atom by LPU
+ * kbase_tlstream_tl_ret_atom_lpu - retain atom by LPU
  * @atom:              name of the atom object
  * @lpu:               name of the Logical Processing Unit object
  * @attrib_match_list: list containing match operator attributes
@@ -334,121 +323,121 @@ extern atomic_t kbase_tlstream_enabled;
  * Function emits a timeline message informing that atom object is being held
  * by LPU and must not be deleted unless it is released.
  */
-#define KBASE_TLSTREAM_TL_RET_ATOM_LPU(atom, lpu, attrib_match_list) \
+#define kbase_tlstream_tl_ret_atom_lpu(atom, lpu, attrib_match_list) \
 	__TRACE_IF_ENABLED(tl_ret_atom_lpu, atom, lpu, attrib_match_list)
 
 /**
- * KBASE_TLSTREAM_TL_NRET_CTX_LPU - release context by LPU
+ * kbase_tlstream_tl_nret_ctx_lpu - release context by LPU
  * @context: name of the context object
  * @lpu:     name of the Logical Processing Unit object
  *
  * Function emits a timeline message informing that context is being released
  * by LPU object.
  */
-#define KBASE_TLSTREAM_TL_NRET_CTX_LPU(context, lpu) \
+#define kbase_tlstream_tl_nret_ctx_lpu(context, lpu) \
 	__TRACE_IF_ENABLED(tl_nret_ctx_lpu, context, lpu)
 
 /**
- * KBASE_TLSTREAM_TL_NRET_ATOM_CTX - release atom by context
+ * kbase_tlstream_tl_nret_atom_ctx - release atom by context
  * @atom:    name of the atom object
  * @context: name of the context object
  *
  * Function emits a timeline message informing that atom object is being
  * released by context.
  */
-#define KBASE_TLSTREAM_TL_NRET_ATOM_CTX(atom, context) \
+#define kbase_tlstream_tl_nret_atom_ctx(atom, context) \
 	__TRACE_IF_ENABLED(tl_nret_atom_ctx, atom, context)
 
 /**
- * KBASE_TLSTREAM_TL_NRET_ATOM_LPU - release atom by LPU
+ * kbase_tlstream_tl_nret_atom_lpu - release atom by LPU
  * @atom: name of the atom object
  * @lpu:  name of the Logical Processing Unit object
  *
  * Function emits a timeline message informing that atom object is being
  * released by LPU.
  */
-#define KBASE_TLSTREAM_TL_NRET_ATOM_LPU(atom, lpu) \
+#define kbase_tlstream_tl_nret_atom_lpu(atom, lpu) \
 	__TRACE_IF_ENABLED(tl_nret_atom_lpu, atom, lpu)
 
 /**
- * KBASE_TLSTREAM_TL_RET_AS_CTX - lifelink address space object to context
+ * kbase_tlstream_tl_ret_as_ctx - lifelink address space object to context
  * @as:  name of the address space object
  * @ctx: name of the context object
  *
  * Function emits a timeline message informing that address space object
  * is being held by the context object.
  */
-#define KBASE_TLSTREAM_TL_RET_AS_CTX(as, ctx) \
+#define kbase_tlstream_tl_ret_as_ctx(as, ctx) \
 	__TRACE_IF_ENABLED(tl_ret_as_ctx, as, ctx)
 
 /**
- * KBASE_TLSTREAM_TL_NRET_AS_CTX - release address space by context
+ * kbase_tlstream_tl_nret_as_ctx - release address space by context
  * @as:  name of the address space object
  * @ctx: name of the context object
  *
  * Function emits a timeline message informing that address space object
  * is being released by atom.
  */
-#define KBASE_TLSTREAM_TL_NRET_AS_CTX(as, ctx) \
+#define kbase_tlstream_tl_nret_as_ctx(as, ctx) \
 	__TRACE_IF_ENABLED(tl_nret_as_ctx, as, ctx)
 
 /**
- * KBASE_TLSTREAM_TL_RET_ATOM_AS - retain atom by address space
+ * kbase_tlstream_tl_ret_atom_as - retain atom by address space
  * @atom: name of the atom object
  * @as:   name of the address space object
  *
  * Function emits a timeline message informing that atom object is being held
  * by address space and must not be deleted unless it is released.
  */
-#define KBASE_TLSTREAM_TL_RET_ATOM_AS(atom, as) \
+#define kbase_tlstream_tl_ret_atom_as(atom, as) \
 	__TRACE_IF_ENABLED(tl_ret_atom_as, atom, as)
 
 /**
- * KBASE_TLSTREAM_TL_NRET_ATOM_AS - release atom by address space
+ * kbase_tlstream_tl_nret_atom_as - release atom by address space
  * @atom: name of the atom object
  * @as:   name of the address space object
  *
  * Function emits a timeline message informing that atom object is being
  * released by address space.
  */
-#define KBASE_TLSTREAM_TL_NRET_ATOM_AS(atom, as) \
+#define kbase_tlstream_tl_nret_atom_as(atom, as) \
 	__TRACE_IF_ENABLED(tl_nret_atom_as, atom, as)
 
 /**
- * KBASE_TLSTREAM_TL_DEP_ATOM_ATOM - parent atom depends on child atom
+ * kbase_tlstream_tl_dep_atom_atom - parent atom depends on child atom
  * @atom1: name of the child atom object
  * @atom2: name of the parent atom object that depends on child atom
  *
  * Function emits a timeline message informing that parent atom waits for
  * child atom object to be completed before start its execution.
  */
-#define KBASE_TLSTREAM_TL_DEP_ATOM_ATOM(atom1, atom2) \
+#define kbase_tlstream_tl_dep_atom_atom(atom1, atom2) \
 	__TRACE_IF_ENABLED(tl_dep_atom_atom, atom1, atom2)
 
 /**
- * KBASE_TLSTREAM_TL_NDEP_ATOM_ATOM - dependency between atoms resolved
+ * kbase_tlstream_tl_ndep_atom_atom - dependency between atoms resolved
  * @atom1: name of the child atom object
  * @atom2: name of the parent atom object that depended on child atom
  *
  * Function emits a timeline message informing that parent atom execution
  * dependency on child atom has been resolved.
  */
-#define KBASE_TLSTREAM_TL_NDEP_ATOM_ATOM(atom1, atom2) \
+#define kbase_tlstream_tl_ndep_atom_atom(atom1, atom2) \
 	__TRACE_IF_ENABLED(tl_ndep_atom_atom, atom1, atom2)
 
 /**
- * KBASE_TLSTREAM_TL_RDEP_ATOM_ATOM - information about already resolved dependency between atoms
+ * kbase_tlstream_tl_rdep_atom_atom - information about already resolved dependency between atoms
  * @atom1: name of the child atom object
  * @atom2: name of the parent atom object that depended on child atom
  *
  * Function emits a timeline message informing that parent atom execution
  * dependency on child atom has been resolved.
  */
-#define KBASE_TLSTREAM_TL_RDEP_ATOM_ATOM(atom1, atom2) \
+#define kbase_tlstream_tl_rdep_atom_atom(atom1, atom2) \
 	__TRACE_IF_ENABLED(tl_rdep_atom_atom, atom1, atom2)
 
 /**
- * KBASE_TLSTREAM_TL_ATTRIB_ATOM_CONFIG - atom job slot attributes
+ * kbase_tlstream_tl_attrib_atom_config - atom job slot attributes
  * @atom:     name of the atom object
  * @jd:       job descriptor address
  * @affinity: job affinity
@@ -456,49 +445,40 @@ extern atomic_t kbase_tlstream_enabled;
  *
  * Function emits a timeline message containing atom attributes.
  */
-#define KBASE_TLSTREAM_TL_ATTRIB_ATOM_CONFIG(atom, jd, affinity, config) \
+#define kbase_tlstream_tl_attrib_atom_config(atom, jd, affinity, config) \
 	__TRACE_IF_ENABLED(tl_attrib_atom_config, atom, jd, affinity, config)
 
 /**
- * KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY - atom priority
+ * kbase_tlstream_tl_attrib_atom_priority - atom priority
  * @atom: name of the atom object
  * @prio: atom priority
  *
  * Function emits a timeline message containing atom priority.
  */
-#define KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY(atom, prio) \
+#define kbase_tlstream_tl_attrib_atom_priority(atom, prio) \
 	__TRACE_IF_ENABLED_LATENCY(tl_attrib_atom_priority, atom, prio)
 
 /**
- * KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE - atom state
+ * kbase_tlstream_tl_attrib_atom_state - atom state
  * @atom:  name of the atom object
  * @state: atom state
  *
  * Function emits a timeline message containing atom state.
  */
-#define KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(atom, state) \
+#define kbase_tlstream_tl_attrib_atom_state(atom, state) \
 	__TRACE_IF_ENABLED_LATENCY(tl_attrib_atom_state, atom, state)
 
 /**
- * KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY_CHANGE - atom caused priority change
+ * kbase_tlstream_tl_attrib_atom_priority_change - atom caused priority change
  * @atom:  name of the atom object
  *
  * Function emits a timeline message signalling priority change
  */
-#define KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY_CHANGE(atom) \
+#define kbase_tlstream_tl_attrib_atom_priority_change(atom) \
 	__TRACE_IF_ENABLED_LATENCY(tl_attrib_atom_priority_change, atom)
 
 /**
- * KBASE_TLSTREAM_TL_ATTRIB_ATOM_JIT - jit happened on atom
- * @atom:       atom identifier
- * @edit_addr:  address edited by jit
- * @new_addr:   address placed into the edited location
- */
-#define KBASE_TLSTREAM_TL_ATTRIB_ATOM_JIT(atom, edit_addr, new_addr) \
-	__TRACE_IF_ENABLED_JD(tl_attrib_atom_jit, atom, edit_addr, new_addr)
-
-/**
- * KBASE_TLSTREAM_TL_ATTRIB_AS_CONFIG - address space attributes
+ * kbase_tlstream_tl_attrib_as_config - address space attributes
  * @as:       assigned address space
  * @transtab: configuration of the TRANSTAB register
  * @memattr:  configuration of the MEMATTR register
@@ -506,118 +486,73 @@ extern atomic_t kbase_tlstream_enabled;
  *
  * Function emits a timeline message containing address space attributes.
  */
-#define KBASE_TLSTREAM_TL_ATTRIB_AS_CONFIG(as, transtab, memattr, transcfg) \
+#define kbase_tlstream_tl_attrib_as_config(as, transtab, memattr, transcfg) \
 	__TRACE_IF_ENABLED(tl_attrib_as_config, as, transtab, memattr, transcfg)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_ex
+ * kbase_tlstream_tl_event_atom_softstop_ex
  * @atom:       atom identifier
  */
-#define KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_EX(atom) \
+#define kbase_tlstream_tl_event_atom_softstop_ex(atom) \
 	__TRACE_IF_ENABLED(tl_event_atom_softstop_ex, atom)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_LPU_softstop
+ * kbase_tlstream_tl_event_lpu_softstop
  * @lpu:        name of the LPU object
  */
-#define KBASE_TLSTREAM_TL_EVENT_LPU_SOFTSTOP(lpu) \
+#define kbase_tlstream_tl_event_lpu_softstop(lpu) \
 	__TRACE_IF_ENABLED(tl_event_lpu_softstop, lpu)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_issue
+ * kbase_tlstream_tl_event_atom_softstop_issue
  * @atom:       atom identifier
  */
-#define KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_ISSUE(atom) \
+#define kbase_tlstream_tl_event_atom_softstop_issue(atom) \
 	__TRACE_IF_ENABLED(tl_event_atom_softstop_issue, atom)
 
 /**
- * KBASE_TLSTREAM_JD_GPU_SOFT_RESET - The GPU is being soft reset
+ * kbase_tlstream_jd_gpu_soft_reset - The GPU is being soft reset
  * @gpu:        name of the GPU object
  *
  * This imperative tracepoint is specific to job dumping.
  * Function emits a timeline message indicating GPU soft reset.
  */
-#define KBASE_TLSTREAM_JD_GPU_SOFT_RESET(gpu) \
+#define kbase_tlstream_jd_gpu_soft_reset(gpu) \
 	__TRACE_IF_ENABLED(jd_gpu_soft_reset, gpu)
 
-
 /**
- * KBASE_TLSTREAM_AUX_PM_STATE - timeline message: power management state
+ * kbase_tlstream_aux_pm_state - timeline message: power management state
  * @core_type: core type (shader, tiler, l2 cache, l3 cache)
  * @state:     64bits bitmask reporting power state of the cores (1-ON, 0-OFF)
  */
-#define KBASE_TLSTREAM_AUX_PM_STATE(core_type, state) \
+#define kbase_tlstream_aux_pm_state(core_type, state) \
 	__TRACE_IF_ENABLED(aux_pm_state, core_type, state)
 
 /**
- * KBASE_TLSTREAM_AUX_PAGEFAULT - timeline message: MMU page fault event
+ * kbase_tlstream_aux_pagefault - timeline message: MMU page fault event
  *                                resulting in new pages being mapped
  * @ctx_nr:            kernel context number
  * @page_count_change: number of pages to be added
  */
-#define KBASE_TLSTREAM_AUX_PAGEFAULT(ctx_nr, page_count_change) \
+#define kbase_tlstream_aux_pagefault(ctx_nr, page_count_change) \
 	__TRACE_IF_ENABLED(aux_pagefault, ctx_nr, page_count_change)
 
 /**
- * KBASE_TLSTREAM_AUX_PAGESALLOC - timeline message: total number of allocated
+ * kbase_tlstream_aux_pagesalloc - timeline message: total number of allocated
  *                                 pages is changed
  * @ctx_nr:     kernel context number
  * @page_count: number of pages used by the context
  */
-#define KBASE_TLSTREAM_AUX_PAGESALLOC(ctx_nr, page_count) \
+#define kbase_tlstream_aux_pagesalloc(ctx_nr, page_count) \
 	__TRACE_IF_ENABLED(aux_pagesalloc, ctx_nr, page_count)
 
 /**
- * KBASE_TLSTREAM_AUX_DEVFREQ_TARGET - timeline message: new target DVFS
+ * kbase_tlstream_aux_devfreq_target - timeline message: new target DVFS
  *                                     frequency
  * @target_freq: new target frequency
  */
-#define KBASE_TLSTREAM_AUX_DEVFREQ_TARGET(target_freq) \
+#define kbase_tlstream_aux_devfreq_target(target_freq) \
 	__TRACE_IF_ENABLED(aux_devfreq_target, target_freq)
 
-/**
- * KBASE_TLSTREAM_AUX_PROTECTED_ENTER_START - The GPU has started transitioning
- *                                            to protected mode
- * @gpu: name of the GPU object
- *
- * Function emits a timeline message indicating the GPU is starting to
- * transition to protected mode.
- */
-#define KBASE_TLSTREAM_AUX_PROTECTED_ENTER_START(gpu) \
-	__TRACE_IF_ENABLED_LATENCY(aux_protected_enter_start, gpu)
-
-/**
- * KBASE_TLSTREAM_AUX_PROTECTED_ENTER_END - The GPU has finished transitioning
- *                                          to protected mode
- * @gpu: name of the GPU object
- *
- * Function emits a timeline message indicating the GPU has finished
- * transitioning to protected mode.
- */
-#define KBASE_TLSTREAM_AUX_PROTECTED_ENTER_END(gpu) \
-	__TRACE_IF_ENABLED_LATENCY(aux_protected_enter_end, gpu)
-
-/**
- * KBASE_TLSTREAM_AUX_PROTECTED_LEAVE_START - The GPU has started transitioning
- *                                            to non-protected mode
- * @gpu: name of the GPU object
- *
- * Function emits a timeline message indicating the GPU is starting to
- * transition to non-protected mode.
- */
-#define KBASE_TLSTREAM_AUX_PROTECTED_LEAVE_START(gpu) \
-	__TRACE_IF_ENABLED_LATENCY(aux_protected_leave_start, gpu)
-
-/**
- * KBASE_TLSTREAM_AUX_PROTECTED_LEAVE_END - The GPU has finished transitioning
- *                                          to non-protected mode
- * @gpu: name of the GPU object
- *
- * Function emits a timeline message indicating the GPU has finished
- * transitioning to non-protected mode.
- */
-#define KBASE_TLSTREAM_AUX_PROTECTED_LEAVE_END(gpu) \
-	__TRACE_IF_ENABLED_LATENCY(aux_protected_leave_end, gpu)
-
 #endif /* _KBASE_TLSTREAM_H */
 
diff --git a/drivers/gpu/arm/midgard/mali_kbase_uku.h b/drivers/gpu/arm/midgard/mali_kbase_uku.h
index c22a593..711b091 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_uku.h
+++ b/drivers/gpu/arm/midgard/mali_kbase_uku.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2008-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2008-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -66,8 +66,6 @@
 #define BASE_UK_VERSION_MAJOR 10
 #define BASE_UK_VERSION_MINOR 6
 
-#define LINUX_UK_BASE_MAGIC 0x80
-
 struct kbase_uk_mem_alloc {
 	union uk_header header;
 	/* IN */
@@ -337,6 +335,8 @@ struct kbase_uk_context_id {
 	int id;
 };
 
+#if (defined(MALI_MIPE_ENABLED) && MALI_MIPE_ENABLED) || \
+	!defined(MALI_MIPE_ENABLED)
 /**
  * struct kbase_uk_tlstream_acquire - User/Kernel space data exchange structure
  * @header: UK structure header
@@ -422,6 +422,7 @@ struct kbase_uk_tlstream_stats {
 	u32 bytes_generated;
 };
 #endif /* MALI_UNIT_TEST */
+#endif /* MALI_MIPE_ENABLED */
 
 /**
  * struct struct kbase_uk_prfcnt_value for the KBASE_FUNC_SET_PRFCNT_VALUES ioctl
@@ -519,12 +520,15 @@ enum kbase_uk_function_id {
 
 	KBASE_FUNC_GET_CONTEXT_ID = (UK_FUNC_ID + 31),
 
+#if (defined(MALI_MIPE_ENABLED) && MALI_MIPE_ENABLED) || \
+	!defined(MALI_MIPE_ENABLED)
 	KBASE_FUNC_TLSTREAM_ACQUIRE_V10_4 = (UK_FUNC_ID + 32),
 #if MALI_UNIT_TEST
 	KBASE_FUNC_TLSTREAM_TEST = (UK_FUNC_ID + 33),
 	KBASE_FUNC_TLSTREAM_STATS = (UK_FUNC_ID + 34),
 #endif /* MALI_UNIT_TEST */
 	KBASE_FUNC_TLSTREAM_FLUSH = (UK_FUNC_ID + 35),
+#endif /* MALI_MIPE_ENABLED */
 
 	KBASE_FUNC_HWCNT_READER_SETUP = (UK_FUNC_ID + 36),
 
@@ -536,7 +540,10 @@ enum kbase_uk_function_id {
 
 	KBASE_FUNC_MEM_JIT_INIT = (UK_FUNC_ID + 39),
 
+#if (defined(MALI_MIPE_ENABLED) && MALI_MIPE_ENABLED) || \
+	!defined(MALI_MIPE_ENABLED)
 	KBASE_FUNC_TLSTREAM_ACQUIRE = (UK_FUNC_ID + 40),
+#endif /* MALI_MIPE_ENABLED */
 
 	KBASE_FUNC_MAX
 };
diff --git a/drivers/gpu/arm/midgard/mali_kbase_vinstr.c b/drivers/gpu/arm/midgard/mali_kbase_vinstr.c
index b2d46ed7..3adb06d 100644
--- a/drivers/gpu/arm/midgard/mali_kbase_vinstr.c
+++ b/drivers/gpu/arm/midgard/mali_kbase_vinstr.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -328,13 +328,14 @@ static int kbasep_vinstr_map_kernel_dump_buffer(
 	struct kbase_va_region *reg;
 	struct kbase_context *kctx = vinstr_ctx->kctx;
 	u64 flags, nr_pages;
+	u16 va_align = 0;
 
 	flags = BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_GPU_WR;
 	vinstr_ctx->dump_size = kbasep_vinstr_dump_size_ctx(vinstr_ctx);
 	nr_pages = PFN_UP(vinstr_ctx->dump_size);
 
 	reg = kbase_mem_alloc(kctx, nr_pages, nr_pages, 0, &flags,
-			&vinstr_ctx->gpu_va);
+			&vinstr_ctx->gpu_va, &va_align);
 	if (!reg)
 		return -ENOMEM;
 
@@ -396,7 +397,7 @@ static int kbasep_vinstr_create_kctx(struct kbase_vinstr_context *vinstr_ctx)
 		/* Inform timeline client about new context.
 		 * Do this while holding the lock to avoid tracepoint
 		 * being created in both body and summary stream. */
-		KBASE_TLSTREAM_TL_NEW_CTX(
+		kbase_tlstream_tl_new_ctx(
 				vinstr_ctx->kctx,
 				(u32)(vinstr_ctx->kctx->id),
 				(u32)(vinstr_ctx->kctx->tgid));
@@ -427,7 +428,7 @@ static int kbasep_vinstr_create_kctx(struct kbase_vinstr_context *vinstr_ctx)
 			kfree(element);
 			mutex_unlock(&kbdev->kctx_list_lock);
 		}
-		KBASE_TLSTREAM_TL_DEL_CTX(vinstr_ctx->kctx);
+		kbase_tlstream_tl_del_ctx(vinstr_ctx->kctx);
 		vinstr_ctx->kctx = NULL;
 		return err;
 	}
@@ -446,7 +447,7 @@ static int kbasep_vinstr_create_kctx(struct kbase_vinstr_context *vinstr_ctx)
 			kfree(element);
 			mutex_unlock(&kbdev->kctx_list_lock);
 		}
-		KBASE_TLSTREAM_TL_DEL_CTX(vinstr_ctx->kctx);
+		kbase_tlstream_tl_del_ctx(vinstr_ctx->kctx);
 		vinstr_ctx->kctx = NULL;
 		return -EFAULT;
 	}
@@ -486,7 +487,7 @@ static void kbasep_vinstr_destroy_kctx(struct kbase_vinstr_context *vinstr_ctx)
 		dev_warn(kbdev->dev, "kctx not in kctx_list\n");
 
 	/* Inform timeline client about context destruction. */
-	KBASE_TLSTREAM_TL_DEL_CTX(vinstr_ctx->kctx);
+	kbase_tlstream_tl_del_ctx(vinstr_ctx->kctx);
 
 	vinstr_ctx->kctx = NULL;
 }
@@ -1160,28 +1161,6 @@ static enum hrtimer_restart kbasep_vinstr_wake_up_callback(
 	return HRTIMER_NORESTART;
 }
 
-#ifdef CONFIG_DEBUG_OBJECT_TIMERS
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 7, 0))
-/**
- * kbase_destroy_hrtimer_on_stack - kernel's destroy_hrtimer_on_stack(),
- *                                  rewritten
- *
- * @timer: high resolution timer
- *
- * destroy_hrtimer_on_stack() was exported only for 4.7.0 kernel so for
- * earlier kernel versions it is not possible to call it explicitly.
- * Since this function must accompany hrtimer_init_on_stack(), which
- * has to be used for hrtimer initialization if CONFIG_DEBUG_OBJECT_TIMERS
- * is defined in order to avoid the warning about object on stack not being
- * annotated, we rewrite it here to be used for earlier kernel versions.
- */
-static void kbase_destroy_hrtimer_on_stack(struct hrtimer *timer)
-{
-	debug_object_free(timer, &hrtimer_debug_descr);
-}
-#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 7, 0) */
-#endif /* CONFIG_DEBUG_OBJECT_TIMERS */
-
 /**
  * kbasep_vinstr_service_task - HWC dumping service thread
  *
@@ -1196,8 +1175,7 @@ static int kbasep_vinstr_service_task(void *data)
 
 	KBASE_DEBUG_ASSERT(vinstr_ctx);
 
-	hrtimer_init_on_stack(&timer.hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-
+	hrtimer_init(&timer.hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	timer.hrtimer.function = kbasep_vinstr_wake_up_callback;
 	timer.vinstr_ctx       = vinstr_ctx;
 
@@ -1300,14 +1278,6 @@ static int kbasep_vinstr_service_task(void *data)
 		mutex_unlock(&vinstr_ctx->lock);
 	}
 
-#ifdef CONFIG_DEBUG_OBJECTS_TIMERS
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 7, 0))
-	kbase_destroy_hrtimer_on_stack(&timer.hrtimer);
-#else
-	destroy_hrtimer_on_stack(&timer.hrtimer);
-#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(4, 7, 0)) */
-#endif /* CONFIG_DEBUG_OBJECTS_TIMERS */
-
 	return 0;
 }
 
diff --git a/drivers/gpu/arm/midgard/mali_midg_regmap.h b/drivers/gpu/arm/midgard/mali_midg_regmap.h
index 7d7b7bc..de3053b 100644
--- a/drivers/gpu/arm/midgard/mali_midg_regmap.h
+++ b/drivers/gpu/arm/midgard/mali_midg_regmap.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -123,9 +123,6 @@
 #define L2_PRESENT_LO           0x120	/* (RO) Level 2 cache present bitmap, low word */
 #define L2_PRESENT_HI           0x124	/* (RO) Level 2 cache present bitmap, high word */
 
-#define STACK_PRESENT_LO        0xE00   /* (RO) Core stack present bitmap, low word */
-#define STACK_PRESENT_HI        0xE04   /* (RO) Core stack present bitmap, high word */
-
 
 #define SHADER_READY_LO         0x140	/* (RO) Shader core ready bitmap, low word */
 #define SHADER_READY_HI         0x144	/* (RO) Shader core ready bitmap, high word */
@@ -136,9 +133,6 @@
 #define L2_READY_LO             0x160	/* (RO) Level 2 cache ready bitmap, low word */
 #define L2_READY_HI             0x164	/* (RO) Level 2 cache ready bitmap, high word */
 
-#define STACK_READY_LO          0xE10   /* (RO) Core stack ready bitmap, low word */
-#define STACK_READY_HI          0xE14   /* (RO) Core stack ready bitmap, high word */
-
 
 #define SHADER_PWRON_LO         0x180	/* (WO) Shader core power on bitmap, low word */
 #define SHADER_PWRON_HI         0x184	/* (WO) Shader core power on bitmap, high word */
@@ -149,10 +143,6 @@
 #define L2_PWRON_LO             0x1A0	/* (WO) Level 2 cache power on bitmap, low word */
 #define L2_PWRON_HI             0x1A4	/* (WO) Level 2 cache power on bitmap, high word */
 
-#define STACK_PWRON_LO          0xE20   /* (RO) Core stack power on bitmap, low word */
-#define STACK_PWRON_HI          0xE24   /* (RO) Core stack power on bitmap, high word */
-
-
 #define SHADER_PWROFF_LO        0x1C0	/* (WO) Shader core power off bitmap, low word */
 #define SHADER_PWROFF_HI        0x1C4	/* (WO) Shader core power off bitmap, high word */
 
@@ -162,10 +152,6 @@
 #define L2_PWROFF_LO            0x1E0	/* (WO) Level 2 cache power off bitmap, low word */
 #define L2_PWROFF_HI            0x1E4	/* (WO) Level 2 cache power off bitmap, high word */
 
-#define STACK_PWROFF_LO         0xE30   /* (RO) Core stack power off bitmap, low word */
-#define STACK_PRWOFF_HI         0xE34   /* (RO) Core stack power off bitmap, high word */
-
-
 #define SHADER_PWRTRANS_LO      0x200	/* (RO) Shader core power transition bitmap, low word */
 #define SHADER_PWRTRANS_HI      0x204	/* (RO) Shader core power transition bitmap, high word */
 
@@ -175,10 +161,6 @@
 #define L2_PWRTRANS_LO          0x220	/* (RO) Level 2 cache power transition bitmap, low word */
 #define L2_PWRTRANS_HI          0x224	/* (RO) Level 2 cache power transition bitmap, high word */
 
-#define STACK_PWRTRANS_LO       0xE40   /* (RO) Core stack power transition bitmap, low word */
-#define STACK_PRWTRANS_HI       0xE44   /* (RO) Core stack power transition bitmap, high word */
-
-
 #define SHADER_PWRACTIVE_LO     0x240	/* (RO) Shader core active bitmap, low word */
 #define SHADER_PWRACTIVE_HI     0x244	/* (RO) Shader core active bitmap, high word */
 
@@ -595,17 +577,4 @@
 
 /* End TILER_CONFIG register */
 
-/* JM_CONFIG register */
-
-#define JM_TIMESTAMP_OVERRIDE  (1ul << 0)
-#define JM_CLOCK_GATE_OVERRIDE (1ul << 1)
-#define JM_JOB_THROTTLE_ENABLE (1ul << 2)
-#define JM_JOB_THROTTLE_LIMIT_SHIFT (3)
-#define JM_MAX_JOB_THROTTLE_LIMIT (0x3F)
-#define JM_FORCE_COHERENCY_FEATURES_SHIFT (2)
-#define JM_IDVS_GROUP_SIZE_SHIFT (16)
-#define JM_MAX_IDVS_GROUP_SIZE (0x3F)
-/* End JM_CONFIG register */
-
-
 #endif /* _MIDGARD_REGMAP_H_ */
diff --git a/drivers/gpu/arm/midgard/platform/devicetree/Kbuild b/drivers/gpu/arm/midgard/platform/devicetree/Kbuild
index e888a42..b5a49f3 100644
--- a/drivers/gpu/arm/midgard/platform/devicetree/Kbuild
+++ b/drivers/gpu/arm/midgard/platform/devicetree/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2012-2016 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -13,6 +13,10 @@
 #
 
 
-mali_kbase-y += \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_config_devicetree.o \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_runtime_pm.o
+ifeq ($(CONFIG_MALI_MIDGARD),y)
+obj-y += mali_kbase_runtime_pm.c
+obj-y += mali_kbase_config_devicetree.c
+else ifeq ($(CONFIG_MALI_MIDGARD),m)
+SRC += platform/devicetree/mali_kbase_runtime_pm.c
+SRC += platform/devicetree/mali_kbase_config_devicetree.c
+endif
diff --git a/drivers/gpu/arm/midgard/platform/devicetree/mali_kbase_config_platform.h b/drivers/gpu/arm/midgard/platform/devicetree/mali_kbase_config_platform.h
index 49e107f..34f6d573 100644
--- a/drivers/gpu/arm/midgard/platform/devicetree/mali_kbase_config_platform.h
+++ b/drivers/gpu/arm/midgard/platform/devicetree/mali_kbase_config_platform.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -71,3 +71,10 @@
 #define PLATFORM_FUNCS (NULL)
 
 extern struct kbase_pm_callback_conf pm_callbacks;
+
+/**
+ * Protected mode switch
+ *
+ * Attached value: pointer to @ref kbase_protected_ops
+ */
+#define PROTECTED_CALLBACKS (NULL)
diff --git a/drivers/gpu/arm/midgard/platform/juno_soc/Kbuild b/drivers/gpu/arm/midgard/platform/juno_soc/Kbuild
new file mode 100644
index 0000000..5b6ef37
--- /dev/null
+++ b/drivers/gpu/arm/midgard/platform/juno_soc/Kbuild
@@ -0,0 +1,19 @@
+#
+# (C) COPYRIGHT 2013-2014 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+obj-y += mali_kbase_config_juno_soc.o
+
+
+obj-m += juno_mali_opp.o
diff --git a/drivers/gpu/arm/midgard/platform/juno_soc/juno_mali_opp.c b/drivers/gpu/arm/midgard/platform/juno_soc/juno_mali_opp.c
new file mode 100644
index 0000000..ccfd8cc
--- /dev/null
+++ b/drivers/gpu/arm/midgard/platform/juno_soc/juno_mali_opp.c
@@ -0,0 +1,84 @@
+/*
+ *
+ * (C) COPYRIGHT 2014 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/scpi_protocol.h>
+#include <linux/version.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0)
+#include <linux/pm_opp.h>
+#else /* Linux >= 3.13 */
+/* In 3.13 the OPP include header file, types, and functions were all
+ * renamed. Use the old filename for the include, and define the new names to
+ * the old, when an old kernel is detected.
+ */
+#include <linux/opp.h>
+#define dev_pm_opp_add opp_add
+#endif /* Linux >= 3.13 */
+
+
+static int init_juno_opps_from_scpi(struct device *dev)
+{
+	struct scpi_opp *sopp;
+	int i;
+
+	/* Hard coded for Juno. 2 is GPU domain */
+	sopp = scpi_dvfs_get_opps(2);
+	if (IS_ERR_OR_NULL(sopp))
+		return PTR_ERR(sopp);
+
+	for (i = 0; i < sopp->count; i++) {
+		struct scpi_opp_entry *e = &sopp->opp[i];
+
+		dev_info(dev, "Mali OPP from SCPI: %u Hz @ %u mV\n",
+				e->freq_hz, e->volt_mv);
+
+		dev_pm_opp_add(dev, e->freq_hz, e->volt_mv * 1000);
+	}
+
+	return 0;
+}
+
+static int juno_setup_opps(void)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	int err;
+
+	np = of_find_node_by_name(NULL, "gpu");
+	if (!np) {
+		pr_err("Failed to find DT entry for Mali\n");
+		return -EFAULT;
+	}
+
+	pdev = of_find_device_by_node(np);
+	if (!pdev) {
+		pr_err("Failed to find device for Mali\n");
+		of_node_put(np);
+		return -EFAULT;
+	}
+
+	err = init_juno_opps_from_scpi(&pdev->dev);
+
+	of_node_put(np);
+
+	return err;
+}
+
+module_init(juno_setup_opps);
+MODULE_LICENSE("GPL");
diff --git a/drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_juno_soc.c b/drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_juno_soc.c
new file mode 100644
index 0000000..c654818
--- /dev/null
+++ b/drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_juno_soc.c
@@ -0,0 +1,156 @@
+/*
+ *
+ * (C) COPYRIGHT 2011-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <linux/ioport.h>
+#include <linux/thermal.h>
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_config.h>
+#include <mali_kbase_smc.h>
+
+/* Versatile Express (VE) Juno Development Platform */
+
+#define HARD_RESET_AT_POWER_OFF 0
+
+#ifndef CONFIG_OF
+static struct kbase_io_resources io_resources = {
+	.job_irq_number = 65,
+	.mmu_irq_number = 66,
+	.gpu_irq_number = 64,
+	.io_memory_region = {
+			     .start = 0x2D000000,
+			     .end = 0x2D000000 + (4096 * 4) - 1}
+};
+#endif
+
+static int pm_callback_power_on(struct kbase_device *kbdev)
+{
+	/* Nothing is needed on VExpress, but we may have destroyed GPU state (if the below HARD_RESET code is active) */
+	return 1;
+}
+
+static void pm_callback_power_off(struct kbase_device *kbdev)
+{
+#if HARD_RESET_AT_POWER_OFF
+	/* Cause a GPU hard reset to test whether we have actually idled the GPU
+	 * and that we properly reconfigure the GPU on power up.
+	 * Usually this would be dangerous, but if the GPU is working correctly it should
+	 * be completely safe as the GPU should not be active at this point.
+	 * However this is disabled normally because it will most likely interfere with
+	 * bus logging etc.
+	 */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_HARD_RESET, NULL, NULL, 0u, 0);
+	kbase_os_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_HARD_RESET);
+#endif
+}
+
+struct kbase_pm_callback_conf pm_callbacks = {
+	.power_on_callback = pm_callback_power_on,
+	.power_off_callback = pm_callback_power_off,
+	.power_suspend_callback  = NULL,
+	.power_resume_callback = NULL
+};
+
+/*
+ * Juno Protected Mode integration
+ */
+
+/* SMC Function Numbers */
+#define JUNO_SMC_PROTECTED_ENTER_FUNC  0xff06
+#define JUNO_SMC_PROTECTED_RESET_FUNC 0xff07
+
+static int juno_protected_mode_enter(struct kbase_device *kbdev)
+{
+	/* T62X in SoC detected */
+	u64 ret = kbase_invoke_smc(SMC_OEN_SIP,
+		JUNO_SMC_PROTECTED_ENTER_FUNC, false,
+		0, 0, 0);
+	return ret;
+}
+
+/* TODO: Remove these externs, reset should should be done by the firmware */
+extern void kbase_reg_write(struct kbase_device *kbdev, u16 offset, u32 value,
+						struct kbase_context *kctx);
+
+extern u32 kbase_reg_read(struct kbase_device *kbdev, u16 offset,
+						struct kbase_context *kctx);
+
+static int juno_protected_mode_reset(struct kbase_device *kbdev)
+{
+
+	/* T62X in SoC detected */
+	u64 ret = kbase_invoke_smc(SMC_OEN_SIP,
+		JUNO_SMC_PROTECTED_RESET_FUNC, false,
+		0, 0, 0);
+
+	/* TODO: Remove this reset, it should be done by the firmware */
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+						GPU_COMMAND_HARD_RESET, NULL);
+
+	while ((kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT), NULL)
+			& RESET_COMPLETED) != RESET_COMPLETED)
+		;
+
+	return ret;
+}
+
+static bool juno_protected_mode_supported(struct kbase_device *kbdev)
+{
+	u32 gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
+
+	/*
+	 * Protected mode is only supported for the built in GPU
+	 * _and_ only if the right firmware is running.
+	 *
+	 * Given that at init time the GPU is not powered up the
+	 * juno_protected_mode_reset function can't be used as
+	 * is needs to access GPU registers.
+	 * However, although we don't want the GPU to boot into
+	 * protected mode we know a GPU reset will be done after
+	 * this function is called so although we set the GPU to
+	 * protected mode it will exit protected mode before the
+	 * driver is ready to run work.
+	 */
+	if (gpu_id == GPU_ID_MAKE(GPU_ID_PI_T62X, 0, 1, 0) &&
+			(kbdev->reg_start == 0x2d000000))
+		return juno_protected_mode_enter(kbdev) == 0;
+
+	return false;
+}
+
+struct kbase_protected_ops juno_protected_ops = {
+	.protected_mode_enter = juno_protected_mode_enter,
+	.protected_mode_reset = juno_protected_mode_reset,
+	.protected_mode_supported = juno_protected_mode_supported,
+};
+
+static struct kbase_platform_config versatile_platform_config = {
+#ifndef CONFIG_OF
+	.io_resources = &io_resources
+#endif
+};
+
+struct kbase_platform_config *kbase_get_platform_config(void)
+{
+	return &versatile_platform_config;
+}
+
+int kbase_platform_early_init(void)
+{
+	/* Nothing needed at this stage */
+	return 0;
+}
diff --git a/drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_platform.h b/drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_platform.h
new file mode 100644
index 0000000..ab29e9d
--- /dev/null
+++ b/drivers/gpu/arm/midgard/platform/juno_soc/mali_kbase_config_platform.h
@@ -0,0 +1,84 @@
+/*
+ *
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * Maximum frequency GPU will be clocked at. Given in kHz.
+ * This must be specified as there is no default value.
+ *
+ * Attached value: number in kHz
+ * Default value: NA
+ */
+#define GPU_FREQ_KHZ_MAX 600000
+/**
+ * Minimum frequency GPU will be clocked at. Given in kHz.
+ * This must be specified as there is no default value.
+ *
+ * Attached value: number in kHz
+ * Default value: NA
+ */
+#define GPU_FREQ_KHZ_MIN 600000
+
+/**
+ * CPU_SPEED_FUNC - A pointer to a function that calculates the CPU clock
+ *
+ * CPU clock speed of the platform is in MHz - see kbase_cpu_clk_speed_func
+ * for the function prototype.
+ *
+ * Attached value: A kbase_cpu_clk_speed_func.
+ * Default Value:  NA
+ */
+#define CPU_SPEED_FUNC (&kbase_cpuprops_get_default_clock_speed)
+
+/**
+ * GPU_SPEED_FUNC - A pointer to a function that calculates the GPU clock
+ *
+ * GPU clock speed of the platform in MHz - see kbase_gpu_clk_speed_func
+ * for the function prototype.
+ *
+ * Attached value: A kbase_gpu_clk_speed_func.
+ * Default Value:  NA
+ */
+#define GPU_SPEED_FUNC (NULL)
+
+/**
+ * Power management configuration
+ *
+ * Attached value: pointer to @ref kbase_pm_callback_conf
+ * Default value: See @ref kbase_pm_callback_conf
+ */
+#define POWER_MANAGEMENT_CALLBACKS (&pm_callbacks)
+
+/**
+ * Platform specific configuration functions
+ *
+ * Attached value: pointer to @ref kbase_platform_funcs_conf
+ * Default value: See @ref kbase_platform_funcs_conf
+ */
+#define PLATFORM_FUNCS (NULL)
+
+/**
+ * Protected mode switch
+ *
+ * Attached value: pointer to @ref kbase_protected_ops
+ */
+#define PROTECTED_CALLBACKS (&juno_protected_ops)
+
+extern struct kbase_pm_callback_conf pm_callbacks;
+#ifdef CONFIG_DEVFREQ_THERMAL
+extern struct devfreq_cooling_ops juno_model_ops;
+#endif
+extern struct kbase_protected_ops juno_protected_ops;
diff --git a/drivers/gpu/arm/midgard/platform/rk/Kbuild b/drivers/gpu/arm/midgard/platform/rk/Kbuild
index db99348..95ca23b 100755
--- a/drivers/gpu/arm/midgard/platform/rk/Kbuild
+++ b/drivers/gpu/arm/midgard/platform/rk/Kbuild
@@ -12,6 +12,6 @@
 #
 #
 
-midgard_kbase-y += \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_config_rk.o
+SRC += \
+	$(MALI_PLATFORM_DIR)/mali_kbase_config_rk.o \
 
diff --git a/drivers/gpu/arm/midgard/platform/vexpress/Kbuild b/drivers/gpu/arm/midgard/platform/vexpress/Kbuild
index 1caa293..084a156 100644
--- a/drivers/gpu/arm/midgard/platform/vexpress/Kbuild
+++ b/drivers/gpu/arm/midgard/platform/vexpress/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2012-2013, 2016 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -13,6 +13,6 @@
 #
 
 
-mali_kbase-y += \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_config_vexpress.o \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_cpu_vexpress.o
+
+obj-y += mali_kbase_config_vexpress.o
+obj-y += mali_kbase_cpu_vexpress.o
diff --git a/drivers/gpu/arm/midgard/platform/vexpress/mali_kbase_config_platform.h b/drivers/gpu/arm/midgard/platform/vexpress/mali_kbase_config_platform.h
index 02835f1..dc4471b 100644
--- a/drivers/gpu/arm/midgard/platform/vexpress/mali_kbase_config_platform.h
+++ b/drivers/gpu/arm/midgard/platform/vexpress/mali_kbase_config_platform.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -72,4 +72,11 @@
  */
 #define PLATFORM_FUNCS (NULL)
 
+/**
+ * Protected mode switch
+ *
+ * Attached value: pointer to @ref kbase_protected_ops
+ */
+#define PROTECTED_CALLBACKS (NULL)
+
 extern struct kbase_pm_callback_conf pm_callbacks;
diff --git a/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/Kbuild b/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/Kbuild
index 7efe8fa..d9bfabc 100644
--- a/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/Kbuild
+++ b/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2013-2014, 2016 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2013 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -13,4 +13,4 @@
 #
 
 
-mali_kbase-y += $(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_config_vexpress.o
+obj-y += mali_kbase_config_vexpress.o
diff --git a/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/mali_kbase_config_platform.h b/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/mali_kbase_config_platform.h
index 0efbf39..b0490ca 100644
--- a/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/mali_kbase_config_platform.h
+++ b/drivers/gpu/arm/midgard/platform/vexpress_1xv7_a57/mali_kbase_config_platform.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -70,4 +70,11 @@
  */
 #define PLATFORM_FUNCS (NULL)
 
+/**
+ * Protected mode switch
+ *
+ * Attached value: pointer to @ref kbase_protected_ops
+ */
+#define PROTECTED_CALLBACKS (NULL)
+
 extern struct kbase_pm_callback_conf pm_callbacks;
diff --git a/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/Kbuild b/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/Kbuild
index 1caa293..0cb41ce 100644
--- a/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/Kbuild
+++ b/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2012-2013, 2016 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -13,6 +13,6 @@
 #
 
 
-mali_kbase-y += \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_config_vexpress.o \
-	$(MALI_PLATFORM_THIRDPARTY_DIR)/mali_kbase_cpu_vexpress.o
+
+obj-y += mali_kbase_config_vexpress.o
+obj-y += mali_kbase_cpu_vexpress.o
diff --git a/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_platform.h b/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_platform.h
index dbdf21e..22ffccb 100644
--- a/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_platform.h
+++ b/drivers/gpu/arm/midgard/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_platform.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -72,4 +72,11 @@
  */
 #define PLATFORM_FUNCS (NULL)
 
+/**
+ * Protected mode switch
+ *
+ * Attached value: pointer to @ref kbase_protected_ops
+ */
+#define PROTECTED_CALLBACKS (NULL)
+
 extern struct kbase_pm_callback_conf pm_callbacks;
diff --git a/drivers/gpu/arm/midgard/protected_mode_switcher.h b/drivers/gpu/arm/midgard/protected_mode_switcher.h
deleted file mode 100644
index 5dc2f3b..0000000
--- a/drivers/gpu/arm/midgard/protected_mode_switcher.h
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _PROTECTED_MODE_SWITCH_H_
-#define _PROTECTED_MODE_SWITCH_H_
-
-struct protected_mode_device;
-
-/**
- * struct protected_mode_ops - Callbacks for protected mode switch operations
- *
- * @protected_mode_enable:  Callback to enable protected mode for device
- * @protected_mode_disable: Callback to disable protected mode for device
- */
-struct protected_mode_ops {
-	/**
-	 * protected_mode_enable() - Enable protected mode on device
-	 * @dev:	The struct device
-	 *
-	 * Return: 0 on success, non-zero on error
-	 */
-	int (*protected_mode_enable)(
-			struct protected_mode_device *protected_dev);
-
-	/**
-	 * protected_mode_disable() - Disable protected mode on device, and
-	 *                            reset device
-	 * @dev:	The struct device
-	 *
-	 * Return: 0 on success, non-zero on error
-	 */
-	int (*protected_mode_disable)(
-			struct protected_mode_device *protected_dev);
-};
-
-/**
- * struct protected_mode_device - Device structure for protected mode devices
- *
- * @ops  - Callbacks associated with this device
- * @data - Pointer to device private data
- *
- * This structure should be registered with the platform device using
- * platform_set_drvdata().
- */
-struct protected_mode_device {
-	struct protected_mode_ops ops;
-	void *data;
-};
-
-#endif /* _PROTECTED_MODE_SWITCH_H_ */
diff --git a/drivers/gpu/arm/midgard/sconscript b/drivers/gpu/arm/midgard/sconscript
index ff23d7a..7b7ec77 100644
--- a/drivers/gpu/arm/midgard/sconscript
+++ b/drivers/gpu/arm/midgard/sconscript
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2010-2016 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -16,13 +16,14 @@
 import sys
 Import('env')
 
-SConscript( 'tests/sconscript' )
+if Glob('tests/sconscript'):
+	SConscript( 'tests/sconscript' )
 
 mock_test = 0
 
 # Fake platform is a transient solution for GPL drivers running in kernel that does not provide configuration via platform data.
 # For such kernels fake_platform_device should be set to 1. For kernels providing platform data fake_platform_device should be set to 0.
-if env['platform_config']=='devicetree' or env['platform_config']=='juno_soc':
+if env['platform_config']=='devicetree':
 	fake_platform_device = 0
 else:
 	fake_platform_device = 1
@@ -32,14 +33,9 @@ kbase_src = [
 	Glob('*.c'),
 	Glob('backend/*/*.c'),
 	Glob('internal/*/*.c'),
-	Glob('ipa/*.c')
+	Glob('platform/%s/*.c' % env['platform_config']),
 ]
 
-if env['platform_config']=='juno_soc':
-	kbase_src += [Glob('platform/devicetree/*.c')]
-else:
-	kbase_src += [Glob('platform/%s/*.c' % env['platform_config'])]
-
 if Glob('#kernel/drivers/gpu/arm/midgard/tests/internal/src/mock') and env['unit'] == '1':
 	kbase_src += [Glob('#kernel/drivers/gpu/arm/midgard/tests/internal/src/mock/*.c')]
 	mock_test = 1
@@ -60,14 +56,14 @@ make_args = env.kernel_get_config_defines(ret_list = True,
                                           fake = fake_platform_device) + [
 	'PLATFORM=%s' % env['platform'],
 	'MALI_ERROR_INJECT_ON=%s' % env['error_inject'],
-	'MALI_KERNEL_TEST_API=%s' % env['debug'],
+	'MALI_KERNEL_TEST_API=%s' % env['unit'],
 	'MALI_UNIT_TEST=%s' % env['unit'],
 	'MALI_RELEASE_NAME=%s' % env['mali_release_name'],
 	'MALI_MOCK_TEST=%s' % mock_test,
 	'MALI_CUSTOMER_RELEASE=%s' % env['release'],
 	'MALI_INSTRUMENTATION_LEVEL=%s' % env['instr'],
 	'MALI_COVERAGE=%s' % env['coverage'],
-	'MALI_BUS_LOG=%s' % env['buslog']
+	'MALI_BUS_LOG=%s' % env['buslog'],
 ]
 
 kbase = env.BuildKernelModule('$STATIC_LIB_PATH/mali_kbase.ko', kbase_src,
@@ -84,9 +80,6 @@ if env['os'] != 'android':
 if int(env['ump']) == 1:
 	env.Depends(kbase, '$STATIC_LIB_PATH/ump.ko')
 
-if 'smc_protected_mode_switcher' in env:
-	env.Depends('$STATIC_LIB_PATH/mali_kbase.ko', '$STATIC_LIB_PATH/smc_protected_mode_switcher.ko')
-
 env.KernelObjTarget('kbase', kbase)
 
 env.AppendUnique(BASE=['cutils_linked_list'])
diff --git a/drivers/gpu/arm/midgard/tests/Kbuild b/drivers/gpu/arm/midgard/tests/Kbuild
deleted file mode 100644
index b4bed04..0000000
--- a/drivers/gpu/arm/midgard/tests/Kbuild
+++ /dev/null
@@ -1,17 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-obj-$(CONFIG_MALI_KUTF) += kutf/
-obj-$(CONFIG_MALI_IRQ_LATENCY) += mali_kutf_irq_test/
diff --git a/drivers/gpu/arm/midgard/tests/Kconfig b/drivers/gpu/arm/midgard/tests/Kconfig
deleted file mode 100644
index da0515c..0000000
--- a/drivers/gpu/arm/midgard/tests/Kconfig
+++ /dev/null
@@ -1,17 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-source "drivers/gpu/arm/midgard/tests/kutf/Kconfig"
-source "drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kconfig"
diff --git a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_mem.h b/drivers/gpu/arm/midgard/tests/include/kutf/kutf_mem.h
deleted file mode 100644
index 0d145e4..0000000
--- a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_mem.h
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KERNEL_UTF_MEM_H_
-#define _KERNEL_UTF_MEM_H_
-
-/* kutf_mem.h
- * Functions for management of memory pools in the kernel.
- *
- * This module implements a memory pool allocator, allowing a test
- * implementation to allocate linked allocations which can then be freed by a
- * single free which releases all of the resources held by the entire pool.
- *
- * Note that it is not possible to free single resources within the pool once
- * allocated.
- */
-
-#include <linux/list.h>
-
-/**
- * struct kutf_mempool - the memory pool context management structure
- * @head:	list head on which the allocations in this context are added to
- *
- */
-struct kutf_mempool {
-	struct list_head head;
-};
-
-/**
- * kutf_mempool_init() - Initialize a memory pool.
- * @pool:	Memory pool structure to initialize, provided by the user
- *
- * Return:	zero on success
- */
-int kutf_mempool_init(struct kutf_mempool *pool);
-
-/**
- * kutf_mempool_alloc() - Allocate memory from a pool
- * @pool:	Memory pool to allocate from
- * @size:	Size of memory wanted in number of bytes
- *
- * Return:	Pointer to memory on success, NULL on failure.
- */
-void *kutf_mempool_alloc(struct kutf_mempool *pool, size_t size);
-
-/**
- * kutf_mempool_destroy() - Destroy a memory pool, freeing all memory within it.
- * @pool:	The memory pool to free
- */
-void kutf_mempool_destroy(struct kutf_mempool *pool);
-#endif	/* _KERNEL_UTF_MEM_H_ */
diff --git a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_resultset.h b/drivers/gpu/arm/midgard/tests/include/kutf/kutf_resultset.h
deleted file mode 100644
index 1cc85f1..0000000
--- a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_resultset.h
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KERNEL_UTF_RESULTSET_H_
-#define _KERNEL_UTF_RESULTSET_H_
-
-/* kutf_resultset.h
- * Functions and structures for handling test results and result sets.
- *
- * This section of the kernel UTF contains structures and functions used for the
- * management of Results and Result Sets.
- */
-
-/**
- * enum kutf_result_status - Status values for a single Test error.
- * @KUTF_RESULT_BENCHMARK:	Result is a meta-result containing benchmark
- *                              results.
- * @KUTF_RESULT_SKIP:		The test was skipped.
- * @KUTF_RESULT_UNKNOWN:	The test has an unknown result.
- * @KUTF_RESULT_PASS:		The test result passed.
- * @KUTF_RESULT_DEBUG:		The test result passed, but raised a debug
- *                              message.
- * @KUTF_RESULT_INFO:		The test result passed, but raised
- *                              an informative message.
- * @KUTF_RESULT_WARN:		The test result passed, but raised a warning
- *                              message.
- * @KUTF_RESULT_FAIL:		The test result failed with a non-fatal error.
- * @KUTF_RESULT_FATAL:		The test result failed with a fatal error.
- * @KUTF_RESULT_ABORT:		The test result failed due to a non-UTF
- *                              assertion failure.
- * @KUTF_RESULT_COUNT:		The current number of possible status messages.
- */
-enum kutf_result_status {
-	KUTF_RESULT_BENCHMARK = -3,
-	KUTF_RESULT_SKIP    = -2,
-	KUTF_RESULT_UNKNOWN = -1,
-
-	KUTF_RESULT_PASS    = 0,
-	KUTF_RESULT_DEBUG   = 1,
-	KUTF_RESULT_INFO    = 2,
-	KUTF_RESULT_WARN    = 3,
-	KUTF_RESULT_FAIL    = 4,
-	KUTF_RESULT_FATAL   = 5,
-	KUTF_RESULT_ABORT   = 6,
-
-	KUTF_RESULT_COUNT
-};
-
-/* The maximum size of a kutf_result_status result when
- * converted to a string
- */
-#define KUTF_ERROR_MAX_NAME_SIZE 21
-
-#ifdef __KERNEL__
-
-#include <kutf/kutf_mem.h>
-
-/**
- * struct kutf_result - Represents a single test result.
- * @node:	Next result in the list of results.
- * @status:	The status summary (pass / warn / fail / etc).
- * @message:	A more verbose status message.
- */
-struct kutf_result {
-	struct list_head            node;
-	enum kutf_result_status     status;
-	const char                  *message;
-};
-
-/**
- * kutf_create_result_set() - Create a new result set
- *                            to which results can be added.
- *
- * Return: The created resultset.
- */
-struct kutf_result_set *kutf_create_result_set(void);
-
-/**
- * kutf_add_result() - Add a result to the end of an existing resultset.
- *
- * @mempool:	The memory pool to allocate the result storage from.
- * @set:	The resultset to add the result to.
- * @status:	The result status to add.
- * @message:	The result message to add.
- */
-void kutf_add_result(struct kutf_mempool *mempool, struct kutf_result_set *set,
-		enum kutf_result_status status, const char *message);
-
-/**
- * kutf_remove_result() - Remove a result from the head of a resultset.
- * @set:	The resultset.
- *
- * Return: result or NULL if there are no further results in the resultset.
- */
-struct kutf_result *kutf_remove_result(
-		struct kutf_result_set *set);
-
-/**
- * kutf_destroy_result_set() - Free a previously created resultset.
- *
- * @results:	The result set whose resources to free.
- */
-void kutf_destroy_result_set(struct kutf_result_set *results);
-
-#endif	/* __KERNEL__ */
-
-#endif	/* _KERNEL_UTF_RESULTSET_H_ */
diff --git a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_suite.h b/drivers/gpu/arm/midgard/tests/include/kutf/kutf_suite.h
deleted file mode 100644
index 754c3ad..0000000
--- a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_suite.h
+++ /dev/null
@@ -1,508 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KERNEL_UTF_SUITE_H_
-#define _KERNEL_UTF_SUITE_H_
-
-/* kutf_suite.h
- * Functions for management of test suites.
- *
- * This collection of data structures, macros, and functions are used to
- * create Test Suites, Tests within those Test Suites, and Fixture variants
- * of each test.
- */
-
-#include <kutf/kutf_mem.h>
-#include <kutf/kutf_resultset.h>
-
-/**
- * Pseudo-flag indicating an absence of any specified test class. Note that
- * tests should not be annotated with this constant as it is simply a zero
- * value; tests without a more specific class must be marked with the flag
- * KUTF_F_TEST_GENERIC.
- */
-#define KUTF_F_TEST_NONE                ((unsigned int)(0))
-
-/**
- * Class indicating this test is a smoke test.
- * A given set of smoke tests should be quick to run, enabling rapid turn-around
- * of "regress-on-commit" test runs.
- */
-#define KUTF_F_TEST_SMOKETEST           ((unsigned int)(1 << 1))
-
-/**
- * Class indicating this test is a performance test.
- * These tests typically produce a performance metric, such as "time to run" or
- * "frames per second",
- */
-#define KUTF_F_TEST_PERFORMANCE         ((unsigned int)(1 << 2))
-
-/**
- * Class indicating that this test is a deprecated test.
- * These tests have typically been replaced by an alternative test which is
- * more efficient, or has better coverage.
- */
-#define KUTF_F_TEST_DEPRECATED          ((unsigned int)(1 << 3))
-
-/**
- * Class indicating that this test is a known failure.
- * These tests have typically been run and failed, but marking them as a known
- * failure means it is easier to triage results.
- *
- * It is typically more convenient to triage known failures using the
- * results database and web UI, as this means there is no need to modify the
- * test code.
- */
-#define KUTF_F_TEST_EXPECTED_FAILURE    ((unsigned int)(1 << 4))
-
-/**
- * Class indicating that this test is a generic test, which is not a member of
- * a more specific test class. Tests which are not created with a specific set
- * of filter flags by the user are assigned this test class by default.
- */
-#define KUTF_F_TEST_GENERIC             ((unsigned int)(1 << 5))
-
-/**
- * Class indicating this test is a resource allocation failure test.
- * A resource allocation failure test will test that an error code is
- * correctly propagated when an allocation fails.
- */
-#define KUTF_F_TEST_RESFAIL             ((unsigned int)(1 << 6))
-
-/**
- * Additional flag indicating that this test is an expected failure when
- * run in resource failure mode. These tests are never run when running
- * the low resource mode.
- */
-#define KUTF_F_TEST_EXPECTED_FAILURE_RF ((unsigned int)(1 << 7))
-
-/**
- * Flag reserved for user-defined filter zero.
- */
-#define KUTF_F_TEST_USER_0 ((unsigned int)(1 << 24))
-
-/**
- * Flag reserved for user-defined filter one.
- */
-#define KUTF_F_TEST_USER_1 ((unsigned int)(1 << 25))
-
-/**
- * Flag reserved for user-defined filter two.
- */
-#define KUTF_F_TEST_USER_2 ((unsigned int)(1 << 26))
-
-/**
- * Flag reserved for user-defined filter three.
- */
-#define KUTF_F_TEST_USER_3 ((unsigned int)(1 << 27))
-
-/**
- * Flag reserved for user-defined filter four.
- */
-#define KUTF_F_TEST_USER_4 ((unsigned int)(1 << 28))
-
-/**
- * Flag reserved for user-defined filter five.
- */
-#define KUTF_F_TEST_USER_5 ((unsigned int)(1 << 29))
-
-/**
- * Flag reserved for user-defined filter six.
- */
-#define KUTF_F_TEST_USER_6 ((unsigned int)(1 << 30))
-
-/**
- * Flag reserved for user-defined filter seven.
- */
-#define KUTF_F_TEST_USER_7 ((unsigned int)(1 << 31))
-
-/**
- * Pseudo-flag indicating that all test classes should be executed.
- */
-#define KUTF_F_TEST_ALL                 ((unsigned int)(0xFFFFFFFFU))
-
-/**
- * union kutf_callback_data - Union used to store test callback data
- * @ptr_value:		pointer to the location where test callback data
- *                      are stored
- * @u32_value:		a number which represents test callback data
- */
-union kutf_callback_data {
-	void *ptr_value;
-	u32  u32_value;
-};
-
-/**
- * struct kutf_context - Structure representing a kernel test context
- * @suite:		Convenience pointer to the suite this context
- *                      is running
- * @test_fix:		The fixture that is being run in this context
- * @fixture_pool:	The memory pool used for the duration of
- *                      the fixture/text context.
- * @fixture:		The user provided fixture structure.
- * @fixture_index:	The index (id) of the current fixture.
- * @fixture_name:	The name of the current fixture (or NULL if unnamed).
- * @test_data:		Any user private data associated with this test
- * @result_set:		All the results logged by this test context
- * @status:		The status of the currently running fixture.
- * @expected_status:	The expected status on exist of the currently
- *                      running fixture.
- */
-struct kutf_context {
-	struct kutf_suite               *suite;
-	struct kutf_test_fixture        *test_fix;
-	struct kutf_mempool             fixture_pool;
-	void                            *fixture;
-	unsigned int                    fixture_index;
-	const char                      *fixture_name;
-	union kutf_callback_data        test_data;
-	struct kutf_result_set          *result_set;
-	enum kutf_result_status         status;
-	enum kutf_result_status         expected_status;
-};
-
-/**
- * struct kutf_suite - Structure representing a kernel test suite
- * @app:			The application this suite belongs to.
- * @name:			The name of this suite.
- * @suite_data:			Any user private data associated with this
- *                              suite.
- * @create_fixture:		Function used to create a new fixture instance
- * @remove_fixture:		Function used to destroy a new fixture instance
- * @fixture_variants:		The number of variants (must be at least 1).
- * @suite_default_flags:	Suite global filter flags which are set on
- *                              all tests.
- * @node:			List node for suite_list
- * @dir:			The debugfs directory for this suite
- * @test_list:			List head to store all the tests which are
- *                              part of this suite
- */
-struct kutf_suite {
-	struct kutf_application        *app;
-	const char                     *name;
-	union kutf_callback_data       suite_data;
-	void *(*create_fixture)(struct kutf_context *context);
-	void  (*remove_fixture)(struct kutf_context *context);
-	unsigned int                   fixture_variants;
-	unsigned int                   suite_default_flags;
-	struct list_head               node;
-	struct dentry                  *dir;
-	struct list_head               test_list;
-};
-
-/* ============================================================================
-	Application functions
-============================================================================ */
-
-/**
- * kutf_create_application() - Create an in kernel test application.
- * @name:	The name of the test application.
- *
- * Return: pointer to the kutf_application  on success or NULL
- * on failure
- */
-struct kutf_application *kutf_create_application(const char *name);
-
-/**
- * kutf_destroy_application() - Destroy an in kernel test application.
- *
- * @app:	The test application to destroy.
- */
-void kutf_destroy_application(struct kutf_application *app);
-
-/* ============================================================================
-	Suite functions
-============================================================================ */
-
-/**
- * kutf_create_suite() - Create a kernel test suite.
- * @app:		The test application to create the suite in.
- * @name:		The name of the suite.
- * @fixture_count:	The number of fixtures to run over the test
- *                      functions in this suite
- * @create_fixture:	Callback used to create a fixture. The returned value
- *                      is stored in the fixture pointer in the context for
- *                      use in the test functions.
- * @remove_fixture:	Callback used to remove a previously created fixture.
- *
- * Suite names must be unique. Should two suites with the same name be
- * registered with the same application then this function will fail, if they
- * are registered with different applications then the function will not detect
- * this and the call will succeed.
- *
- * Return: pointer to the created kutf_suite on success or NULL
- * on failure
- */
-struct kutf_suite *kutf_create_suite(
-		struct kutf_application *app,
-		const char *name,
-		unsigned int fixture_count,
-		void *(*create_fixture)(struct kutf_context *context),
-		void (*remove_fixture)(struct kutf_context *context));
-
-/**
- * kutf_create_suite_with_filters() - Create a kernel test suite with user
- *                                    defined default filters.
- * @app:		The test application to create the suite in.
- * @name:		The name of the suite.
- * @fixture_count:	The number of fixtures to run over the test
- *                      functions in this suite
- * @create_fixture:	Callback used to create a fixture. The returned value
- *			is stored in the fixture pointer in the context for
- *			use in the test functions.
- * @remove_fixture:	Callback used to remove a previously created fixture.
- * @filters:		Filters to apply to a test if it doesn't provide its own
- *
- * Suite names must be unique. Should two suites with the same name be
- * registered with the same application then this function will fail, if they
- * are registered with different applications then the function will not detect
- * this and the call will succeed.
- *
- * Return: pointer to the created kutf_suite on success or NULL on failure
- */
-struct kutf_suite *kutf_create_suite_with_filters(
-		struct kutf_application *app,
-		const char *name,
-		unsigned int fixture_count,
-		void *(*create_fixture)(struct kutf_context *context),
-		void (*remove_fixture)(struct kutf_context *context),
-		unsigned int filters);
-
-/**
- * kutf_create_suite_with_filters_and_data() - Create a kernel test suite with
- *                                             user defined default filters.
- * @app:		The test application to create the suite in.
- * @name:		The name of the suite.
- * @fixture_count:	The number of fixtures to run over the test
- *			functions in this suite
- * @create_fixture:	Callback used to create a fixture. The returned value
- *			is stored in the fixture pointer in the context for
- *			use in the test functions.
- * @remove_fixture:	Callback used to remove a previously created fixture.
- * @filters:		Filters to apply to a test if it doesn't provide its own
- * @suite_data:		Suite specific callback data, provided during the
- *			running of the test in the kutf_context
- *
- * Return: pointer to the created kutf_suite on success or NULL
- * on failure
- */
-struct kutf_suite *kutf_create_suite_with_filters_and_data(
-		struct kutf_application *app,
-		const char *name,
-		unsigned int fixture_count,
-		void *(*create_fixture)(struct kutf_context *context),
-		void (*remove_fixture)(struct kutf_context *context),
-		unsigned int filters,
-		union kutf_callback_data suite_data);
-
-/**
- * kutf_add_test() - Add a test to a kernel test suite.
- * @suite:	The suite to add the test to.
- * @id:		The ID of the test.
- * @name:	The name of the test.
- * @execute:	Callback to the test function to run.
- *
- * Note: As no filters are provided the test will use the suite filters instead
- */
-void kutf_add_test(struct kutf_suite *suite,
-		unsigned int id,
-		const char *name,
-		void (*execute)(struct kutf_context *context));
-
-/**
- * kutf_add_test_with_filters() - Add a test to a kernel test suite with filters
- * @suite:	The suite to add the test to.
- * @id:		The ID of the test.
- * @name:	The name of the test.
- * @execute:	Callback to the test function to run.
- * @filters:	A set of filtering flags, assigning test categories.
- */
-void kutf_add_test_with_filters(struct kutf_suite *suite,
-		unsigned int id,
-		const char *name,
-		void (*execute)(struct kutf_context *context),
-		unsigned int filters);
-
-/**
- * kutf_add_test_with_filters_and_data() - Add a test to a kernel test suite
- *					   with filters.
- * @suite:	The suite to add the test to.
- * @id:		The ID of the test.
- * @name:	The name of the test.
- * @execute:	Callback to the test function to run.
- * @filters:	A set of filtering flags, assigning test categories.
- * @test_data:	Test specific callback data, provoided during the
- *		running of the test in the kutf_context
- */
-void kutf_add_test_with_filters_and_data(
-		struct kutf_suite *suite,
-		unsigned int id,
-		const char *name,
-		void (*execute)(struct kutf_context *context),
-		unsigned int filters,
-		union kutf_callback_data test_data);
-
-/* ============================================================================
-	Test functions
-============================================================================ */
-/**
- * kutf_test_log_result_external() - Log a result which has been created
- *                                   externally into a in a standard form
- *                                   recognized by the log parser.
- * @context:	The test context the test is running in
- * @message:	The message for this result
- * @new_status:	The result status of this log message
- */
-void kutf_test_log_result_external(
-	struct kutf_context *context,
-	const char *message,
-	enum kutf_result_status new_status);
-
-/**
- * kutf_test_expect_abort() - Tell the kernel that you expect the current
- *                            fixture to produce an abort.
- * @context:	The test context this test is running in.
- */
-void kutf_test_expect_abort(struct kutf_context *context);
-
-/**
- * kutf_test_expect_fatal() - Tell the kernel that you expect the current
- *                            fixture to produce a fatal error.
- * @context:	The test context this test is running in.
- */
-void kutf_test_expect_fatal(struct kutf_context *context);
-
-/**
- * kutf_test_expect_fail() - Tell the kernel that you expect the current
- *                           fixture to fail.
- * @context:	The test context this test is running in.
- */
-void kutf_test_expect_fail(struct kutf_context *context);
-
-/**
- * kutf_test_expect_warn() - Tell the kernel that you expect the current
- *                           fixture to produce a warning.
- * @context:	The test context this test is running in.
- */
-void kutf_test_expect_warn(struct kutf_context *context);
-
-/**
- * kutf_test_expect_pass() - Tell the kernel that you expect the current
- *                           fixture to pass.
- * @context:	The test context this test is running in.
- */
-void kutf_test_expect_pass(struct kutf_context *context);
-
-/**
- * kutf_test_skip() - Tell the kernel that the test should be skipped.
- * @context:	The test context this test is running in.
- */
-void kutf_test_skip(struct kutf_context *context);
-
-/**
- * kutf_test_skip_msg() - Tell the kernel that this test has been skipped,
- *                        supplying a reason string.
- * @context:	The test context this test is running in.
- * @message:	A message string containing the reason for the skip.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a prebaked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_skip_msg(struct kutf_context *context, const char *message);
-
-/**
- * kutf_test_pass() - Tell the kernel that this test has passed.
- * @context:	The test context this test is running in.
- * @message:	A message string containing the reason for the pass.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a pre-baked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_pass(struct kutf_context *context, char const *message);
-
-/**
- * kutf_test_debug() - Send a debug message
- * @context:	The test context this test is running in.
- * @message:	A message string containing the debug information.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a pre-baked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_debug(struct kutf_context *context, char const *message);
-
-/**
- * kutf_test_info() - Send an information message
- * @context:	The test context this test is running in.
- * @message:	A message string containing the information message.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a pre-baked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_info(struct kutf_context *context, char const *message);
-
-/**
- * kutf_test_warn() - Send a warning message
- * @context:	The test context this test is running in.
- * @message:	A message string containing the warning message.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a pre-baked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_warn(struct kutf_context *context, char const *message);
-
-/**
- * kutf_test_fail() - Tell the kernel that a test has failed
- * @context:	The test context this test is running in.
- * @message:	A message string containing the failure message.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a pre-baked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_fail(struct kutf_context *context, char const *message);
-
-/**
- * kutf_test_fatal() - Tell the kernel that a test has triggered a fatal error
- * @context:	The test context this test is running in.
- * @message:	A message string containing the fatal error message.
- *
- * Note: The message must not be freed during the lifetime of the test run.
- * This means it should either be a pre-baked string, or if a dynamic string
- * is required it must be created with kutf_dsprintf which will store
- * the resultant string in a buffer who's lifetime is the same as the test run.
- */
-void kutf_test_fatal(struct kutf_context *context, char const *message);
-
-/**
- * kutf_test_abort() - Tell the kernel that a test triggered an abort in the test
- *
- * @context:	The test context this test is running in.
- */
-void kutf_test_abort(struct kutf_context *context);
-
-#endif	/* _KERNEL_UTF_SUITE_H_ */
diff --git a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_utils.h b/drivers/gpu/arm/midgard/tests/include/kutf/kutf_utils.h
deleted file mode 100644
index c458c1f..0000000
--- a/drivers/gpu/arm/midgard/tests/include/kutf/kutf_utils.h
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#ifndef _KERNEL_UTF_UTILS_H_
-#define _KERNEL_UTF_UTILS_H_
-
-/* kutf_utils.h
- * Utilities for the kernel UTF test infrastructure.
- *
- * This collection of library functions are provided for use by kernel UTF
- * and users of kernel UTF which don't directly fit within the other
- * code modules.
- */
-
-#include <kutf/kutf_mem.h>
-
-/**
- * Maximum size of the message strings within kernel UTF, messages longer then
- * this will be truncated.
- */
-#define KUTF_MAX_DSPRINTF_LEN	1024
-
-/**
- * kutf_dsprintf() - dynamic sprintf
- * @pool:	memory pool to allocate from
- * @fmt:	The format string describing the string to document.
- * @...		The parameters to feed in to the format string.
- *
- * This function implements sprintf which dynamically allocates memory to store
- * the string. The library will free the memory containing the string when the
- * result set is cleared or destroyed.
- *
- * Note The returned string may be truncated to fit an internal temporary
- * buffer, which is KUTF_MAX_DSPRINTF_LEN bytes in length.
- *
- * Return: Returns pointer to allocated string, or NULL on error.
- */
-const char *kutf_dsprintf(struct kutf_mempool *pool,
-		const char *fmt, ...);
-
-#endif	/* _KERNEL_UTF_UTILS_H_ */
diff --git a/drivers/gpu/arm/midgard/tests/kutf/Kbuild b/drivers/gpu/arm/midgard/tests/kutf/Kbuild
deleted file mode 100644
index 6b840c2..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/Kbuild
+++ /dev/null
@@ -1,20 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-ccflags-y += -I$(src)/../include
-
-obj-$(CONFIG_MALI_KUTF) += kutf.o
-
-kutf-y := kutf_mem.o kutf_resultset.o kutf_suite.o kutf_utils.o
diff --git a/drivers/gpu/arm/midgard/tests/kutf/Kconfig b/drivers/gpu/arm/midgard/tests/kutf/Kconfig
deleted file mode 100644
index 8436471..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/Kconfig
+++ /dev/null
@@ -1,22 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-
-config MALI_KUTF
- tristate "Mali Kernel Unit Test Framework"
- default n
- help
-   Enables MALI testing framework. To compile it as a module,
-   choose M here - this will generate a single module called kutf.
diff --git a/drivers/gpu/arm/midgard/tests/kutf/Makefile b/drivers/gpu/arm/midgard/tests/kutf/Makefile
deleted file mode 100644
index 010c92c..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/Makefile
+++ /dev/null
@@ -1,29 +0,0 @@
-#
-# (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-# linux build system bootstrap for out-of-tree module
-
-# default to building for the host
-ARCH ?= $(shell uname -m)
-
-ifeq ($(KDIR),)
-$(error Must specify KDIR to point to the kernel to target))
-endif
-
-all:
-	$(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(CURDIR) $(SCONS_CONFIGS) EXTRA_CFLAGS=-I$(CURDIR)/../include modules
-
-clean:
-	$(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(CURDIR) clean
diff --git a/drivers/gpu/arm/midgard/tests/kutf/kutf_mem.c b/drivers/gpu/arm/midgard/tests/kutf/kutf_mem.c
deleted file mode 100644
index 5408e57..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/kutf_mem.c
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/* Kernel UTF memory management functions */
-
-#include <linux/list.h>
-#include <linux/slab.h>
-#include <linux/module.h>
-
-#include <kutf/kutf_mem.h>
-
-
-/**
- * struct kutf_alloc_entry - Structure representing an allocation.
- * @node:	List node for use with kutf_mempool.
- * @data:	Data area of the allocation
- */
-struct kutf_alloc_entry {
-	struct list_head node;
-	u8 data[0];
-};
-
-int kutf_mempool_init(struct kutf_mempool *pool)
-{
-	if (!pool) {
-		pr_err("NULL pointer passed to %s\n", __func__);
-		return -1;
-	}
-
-	INIT_LIST_HEAD(&pool->head);
-
-	return 0;
-}
-EXPORT_SYMBOL(kutf_mempool_init);
-
-void kutf_mempool_destroy(struct kutf_mempool *pool)
-{
-	struct list_head *remove;
-	struct list_head *tmp;
-
-	if (!pool) {
-		pr_err("NULL pointer passed to %s\n", __func__);
-		return;
-	}
-
-	list_for_each_safe(remove, tmp, &pool->head) {
-		struct kutf_alloc_entry *remove_alloc;
-
-		remove_alloc = list_entry(remove, struct kutf_alloc_entry, node);
-		list_del(&remove_alloc->node);
-		kfree(remove_alloc);
-	}
-}
-EXPORT_SYMBOL(kutf_mempool_destroy);
-
-void *kutf_mempool_alloc(struct kutf_mempool *pool, size_t size)
-{
-	struct kutf_alloc_entry *ret;
-
-	if (!pool) {
-		pr_err("NULL pointer passed to %s\n", __func__);
-		goto fail_pool;
-	}
-
-	ret = kmalloc(sizeof(*ret) + size, GFP_KERNEL);
-	if (!ret) {
-		pr_err("Failed to allocate memory\n");
-		goto fail_alloc;
-	}
-
-	INIT_LIST_HEAD(&ret->node);
-	list_add(&ret->node, &pool->head);
-
-	return &ret->data[0];
-
-fail_alloc:
-fail_pool:
-	return NULL;
-}
-EXPORT_SYMBOL(kutf_mempool_alloc);
diff --git a/drivers/gpu/arm/midgard/tests/kutf/kutf_resultset.c b/drivers/gpu/arm/midgard/tests/kutf/kutf_resultset.c
deleted file mode 100644
index 5bd0496..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/kutf_resultset.c
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/* Kernel UTF result management functions */
-
-#include <linux/list.h>
-#include <linux/slab.h>
-#include <linux/printk.h>
-
-#include <kutf/kutf_resultset.h>
-
-/**
- * struct kutf_result_set - Represents a set of results.
- * @results:	Pointer to the linked list where the results are stored.
- */
-struct kutf_result_set {
-	struct list_head          results;
-};
-
-struct kutf_result_set *kutf_create_result_set(void)
-{
-	struct kutf_result_set *set;
-
-	set = kmalloc(sizeof(*set), GFP_KERNEL);
-	if (!set) {
-		pr_err("Failed to allocate resultset");
-		goto fail_alloc;
-	}
-
-	INIT_LIST_HEAD(&set->results);
-
-	return set;
-
-fail_alloc:
-	return NULL;
-}
-
-void kutf_add_result(struct kutf_mempool *mempool,
-		struct kutf_result_set *set,
-		enum kutf_result_status status,
-		const char *message)
-{
-	/* Create the new result */
-	struct kutf_result *new_result;
-
-	BUG_ON(set == NULL);
-
-	new_result = kutf_mempool_alloc(mempool, sizeof(*new_result));
-	if (!new_result) {
-		pr_err("Result allocation failed\n");
-		return;
-	}
-
-	INIT_LIST_HEAD(&new_result->node);
-	new_result->status = status;
-	new_result->message = message;
-
-	list_add_tail(&new_result->node, &set->results);
-}
-
-void kutf_destroy_result_set(struct kutf_result_set *set)
-{
-	if (!list_empty(&set->results))
-		pr_err("kutf_destroy_result_set: Unread results from test\n");
-
-	kfree(set);
-}
-
-struct kutf_result *kutf_remove_result(struct kutf_result_set *set)
-{
-	if (!list_empty(&set->results)) {
-		struct kutf_result *ret;
-
-		ret = list_first_entry(&set->results, struct kutf_result, node);
-		list_del(&ret->node);
-		return ret;
-	}
-
-	return NULL;
-}
-
diff --git a/drivers/gpu/arm/midgard/tests/kutf/kutf_suite.c b/drivers/gpu/arm/midgard/tests/kutf/kutf_suite.c
deleted file mode 100644
index a7cfd3b..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/kutf_suite.c
+++ /dev/null
@@ -1,1041 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/* Kernel UTF suite, test and fixture management including user to kernel
- * interaction */
-
-#include <linux/list.h>
-#include <linux/slab.h>
-#include <linux/debugfs.h>
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/uaccess.h>
-#include <linux/fs.h>
-#include <linux/version.h>
-
-#include <generated/autoconf.h>
-
-#include <kutf/kutf_suite.h>
-#include <kutf/kutf_resultset.h>
-#include <kutf/kutf_utils.h>
-
-#if defined(CONFIG_DEBUG_FS)
-
-/**
- * struct kutf_application - Structure which represents kutf application
- * @name:	The name of this test application.
- * @dir:	The debugfs directory for this test
- * @suite_list:	List head to store all the suites which are part of this
- *              application
- */
-struct kutf_application {
-	const char         *name;
-	struct dentry      *dir;
-	struct list_head   suite_list;
-};
-
-/**
- * struct kutf_test_function - Structure which represents kutf test function
- * @suite:		Back reference to the suite this test function
- *                      belongs to
- * @filters:		Filters that apply to this test function
- * @test_id:		Test ID
- * @execute:		Function to run for this test
- * @test_data:		Static data for this test
- * @node:		List node for test_list
- * @variant_list:	List head to store all the variants which can run on
- *                      this function
- * @dir:		debugfs directory for this test function
- */
-struct kutf_test_function {
-	struct kutf_suite  *suite;
-	unsigned int       filters;
-	unsigned int       test_id;
-	void (*execute)(struct kutf_context *context);
-	union kutf_callback_data test_data;
-	struct list_head   node;
-	struct list_head   variant_list;
-	struct dentry      *dir;
-};
-
-/**
- * struct kutf_test_fixture - Structure which holds information on the kutf
- *                            test fixture
- * @test_func:		Test function this fixture belongs to
- * @fixture_index:	Index of this fixture
- * @node:		List node for variant_list
- * @dir:		debugfs directory for this test fixture
- */
-struct kutf_test_fixture {
-	struct kutf_test_function *test_func;
-	unsigned int              fixture_index;
-	struct list_head          node;
-	struct dentry             *dir;
-};
-
-struct dentry *base_dir;
-
-/**
- * struct kutf_convert_table - Structure which keeps test results
- * @result_name:	Status of the test result
- * @result:		Status value for a single test
- */
-struct kutf_convert_table {
-	char                    result_name[50];
-	enum kutf_result_status result;
-};
-
-struct kutf_convert_table kutf_convert[] = {
-#define ADD_UTF_RESULT(_name) \
-{ \
-	#_name, \
-	_name, \
-},
-ADD_UTF_RESULT(KUTF_RESULT_BENCHMARK)
-ADD_UTF_RESULT(KUTF_RESULT_SKIP)
-ADD_UTF_RESULT(KUTF_RESULT_UNKNOWN)
-ADD_UTF_RESULT(KUTF_RESULT_PASS)
-ADD_UTF_RESULT(KUTF_RESULT_DEBUG)
-ADD_UTF_RESULT(KUTF_RESULT_INFO)
-ADD_UTF_RESULT(KUTF_RESULT_WARN)
-ADD_UTF_RESULT(KUTF_RESULT_FAIL)
-ADD_UTF_RESULT(KUTF_RESULT_FATAL)
-ADD_UTF_RESULT(KUTF_RESULT_ABORT)
-};
-
-#define UTF_CONVERT_SIZE (ARRAY_SIZE(kutf_convert))
-
-/**
- * kutf_create_context() - Create a test context in which a specific fixture
- *                         of an application will be run and its results
- *                         reported back to the user
- * @test_fix:	Test fixture to be run.
- *
- * Return: Returns the created test context on success or NULL on failure
- */
-static struct kutf_context *kutf_create_context(
-		struct kutf_test_fixture *test_fix);
-
-/**
- * kutf_destroy_context() - Destroy a previously created test context
- * @context:	Test context to destroy
- */
-static void kutf_destroy_context(struct kutf_context *context);
-
-/**
- * kutf_set_result() - Set the test result against the specified test context
- * @context:	Test context
- * @status:	Result status
- */
-static void kutf_set_result(struct kutf_context *context,
-		enum kutf_result_status status);
-
-/**
- * kutf_set_expected_result() - Set the expected test result for the specified
- *                              test context
- * @context:		Test context
- * @expected_status:	Expected result status
- */
-static void kutf_set_expected_result(struct kutf_context *context,
-		enum kutf_result_status expected_status);
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0))
-/* Pre 3.4.0 kernels don't have the simple_open helper */
-
-/**
- * simple_open() - Helper for file opening which stores the inode private data
- *                 into the file private data
- * @inode:	File entry representation
- * @file:	A specific opening of the file
- *
- * Return: always 0; if inode private data do not exist, the file will not
- *         be assigned private data
- */
-static int simple_open(struct inode *inode, struct file *file)
-{
-	if (inode->i_private)
-		file->private_data = inode->i_private;
-	return 0;
-}
-#endif
-
-/**
- * kutf_result_to_string() - Converts a KUTF result into a string
- * @result_str:      Output result string
- * @result:          Result status to convert
- *
- * Return: 1 if test result was successfully converted to string, 0 otherwise
- */
-static int kutf_result_to_string(char **result_str,
-		enum kutf_result_status result)
-{
-	int i;
-	int ret = 0;
-
-	for (i = 0; i < UTF_CONVERT_SIZE; i++) {
-		if (result == kutf_convert[i].result) {
-			*result_str = kutf_convert[i].result_name;
-			ret = 1;
-		}
-	}
-	return ret;
-}
-
-/**
- * kutf_debugfs_const_string_read() - Simple debugfs read callback which
- *                                    returns a constant string
- * @file:	Opened file to read from
- * @buf:	User buffer to write the data into
- * @len:	Amount of data to read
- * @ppos:	Offset into file to read from
- *
- * Return: On success, the number of bytes read and offset @ppos advanced by
- *         this number; on error, negative value
- */
-static ssize_t kutf_debugfs_const_string_read(struct file *file,
-		char __user *buf, size_t len, loff_t *ppos)
-{
-	char *str = file->private_data;
-
-	return simple_read_from_buffer(buf, len, ppos, str, strlen(str));
-}
-
-static const struct file_operations kutf_debugfs_const_string_ops = {
-	.owner = THIS_MODULE,
-	.open = simple_open,
-	.read = kutf_debugfs_const_string_read,
-	.llseek  = default_llseek,
-};
-
-/**
- * kutf_add_explicit_result() - Check if an explicit result needs to be added
- * @context:	KUTF test context
- */
-static void kutf_add_explicit_result(struct kutf_context *context)
-{
-	switch (context->expected_status) {
-	case KUTF_RESULT_UNKNOWN:
-		if (context->status == KUTF_RESULT_UNKNOWN)
-			kutf_test_pass(context, "(implicit pass)");
-		break;
-
-	case KUTF_RESULT_WARN:
-		if (context->status == KUTF_RESULT_WARN)
-			kutf_test_pass(context,
-					"Pass (expected warn occurred)");
-		else if (context->status != KUTF_RESULT_SKIP)
-			kutf_test_fail(context,
-					"Fail (expected warn missing)");
-		break;
-
-	case KUTF_RESULT_FAIL:
-		if (context->status == KUTF_RESULT_FAIL)
-			kutf_test_pass(context,
-					"Pass (expected fail occurred)");
-		else if (context->status != KUTF_RESULT_SKIP) {
-			/* Force the expected status so the fail gets logged */
-			context->expected_status = KUTF_RESULT_PASS;
-			kutf_test_fail(context,
-					"Fail (expected fail missing)");
-		}
-		break;
-
-	case KUTF_RESULT_FATAL:
-		if (context->status == KUTF_RESULT_FATAL)
-			kutf_test_pass(context,
-					"Pass (expected fatal occurred)");
-		else if (context->status != KUTF_RESULT_SKIP)
-			kutf_test_fail(context,
-					"Fail (expected fatal missing)");
-		break;
-
-	case KUTF_RESULT_ABORT:
-		if (context->status == KUTF_RESULT_ABORT)
-			kutf_test_pass(context,
-					"Pass (expected abort occurred)");
-		else if (context->status != KUTF_RESULT_SKIP)
-			kutf_test_fail(context,
-					"Fail (expected abort missing)");
-		break;
-	default:
-		break;
-	}
-}
-
-/**
- * kutf_debugfs_run_open() Debugfs open callback for the "run" entry.
- * @inode:	inode of the opened file
- * @file:	Opened file to read from
- *
- * This function retrieves the test fixture data that is associated with the
- * opened file and works back to get the test, suite and application so
- * it can then run the test that is associated with the file entry.
- *
- * Return: 0 on success
- */
-static int kutf_debugfs_run_open(struct inode *inode, struct file *file)
-{
-	struct kutf_test_fixture *test_fix = inode->i_private;
-	struct kutf_test_function *test_func = test_fix->test_func;
-	struct kutf_suite *suite = test_func->suite;
-	struct kutf_context *test_context;
-
-	test_context = kutf_create_context(test_fix);
-	if (!test_context)
-		return -ENODEV;
-
-	file->private_data = test_context;
-
-	/*
-	 *  Call the create fixture function if required before the
-	 * fixture is run
-	 */
-	if (suite->create_fixture)
-		test_context->fixture = suite->create_fixture(test_context);
-
-	/* Only run the test if the fixture was created (if required) */
-	if ((suite->create_fixture && test_context->fixture) ||
-			(!suite->create_fixture)) {
-		/* Run this fixture */
-		test_func->execute(test_context);
-
-		if (suite->remove_fixture)
-			suite->remove_fixture(test_context);
-
-		kutf_add_explicit_result(test_context);
-	}
-	return 0;
-}
-
-/**
- * kutf_debugfs_run_read() - Debugfs read callback for the "run" entry.
- * @file:	Opened file to read from
- * @buf:	User buffer to write the data into
- * @len:	Amount of data to read
- * @ppos:	Offset into file to read from
- *
- * This function emits the results which where logged during the opening of
- * the file kutf_debugfs_run_open.
- * Results will be emitted one at a time, once all the results have been read
- * 0 will be returned to indicate there is no more data.
- *
- * Return: Number of bytes read.
- */
-static ssize_t kutf_debugfs_run_read(struct file *file, char __user *buf,
-		size_t len, loff_t *ppos)
-{
-	struct kutf_context *test_context = file->private_data;
-	struct kutf_result *res;
-	unsigned long bytes_not_copied;
-	ssize_t bytes_copied = 0;
-
-	/* Note: This code assumes a result is read completely */
-	res = kutf_remove_result(test_context->result_set);
-	if (res) {
-		char *kutf_str_ptr = NULL;
-		unsigned int kutf_str_len = 0;
-		unsigned int message_len = 0;
-		char separator = ':';
-		char terminator = '\n';
-
-		kutf_result_to_string(&kutf_str_ptr, res->status);
-		if (kutf_str_ptr)
-			kutf_str_len = strlen(kutf_str_ptr);
-
-		if (res->message)
-			message_len = strlen(res->message);
-
-		if ((kutf_str_len + 1 + message_len + 1) > len) {
-			pr_err("Not enough space in user buffer for a single result");
-			return 0;
-		}
-
-		/* First copy the result string */
-		if (kutf_str_ptr) {
-			bytes_not_copied = copy_to_user(&buf[0], kutf_str_ptr,
-							kutf_str_len);
-			bytes_copied += kutf_str_len - bytes_not_copied;
-			if (bytes_not_copied)
-				goto exit;
-		}
-
-		/* Then the separator */
-		bytes_not_copied = copy_to_user(&buf[bytes_copied],
-						&separator, 1);
-		bytes_copied += 1 - bytes_not_copied;
-		if (bytes_not_copied)
-			goto exit;
-
-		/* Finally Next copy the result string */
-		if (res->message) {
-			bytes_not_copied = copy_to_user(&buf[bytes_copied],
-							res->message, message_len);
-			bytes_copied += message_len - bytes_not_copied;
-			if (bytes_not_copied)
-				goto exit;
-		}
-
-		/* Finally the terminator */
-		bytes_not_copied = copy_to_user(&buf[bytes_copied],
-						&terminator, 1);
-		bytes_copied += 1 - bytes_not_copied;
-	}
-exit:
-	return bytes_copied;
-}
-
-/**
- * kutf_debugfs_run_release() - Debugfs release callback for the "run" entry.
- * @inode:	File entry representation
- * @file:	A specific opening of the file
- *
- * Release any resources that where created during the opening of the file
- *
- * Return: 0 on success
- */
-static int kutf_debugfs_run_release(struct inode *inode, struct file *file)
-{
-	struct kutf_context *test_context = file->private_data;
-
-	kutf_destroy_context(test_context);
-	return 0;
-}
-
-static const struct file_operations kutf_debugfs_run_ops = {
-	.owner = THIS_MODULE,
-	.open = kutf_debugfs_run_open,
-	.read = kutf_debugfs_run_read,
-	.release = kutf_debugfs_run_release,
-	.llseek  = default_llseek,
-};
-
-/**
- * create_fixture_variant() - Creates a fixture variant for the specified
- *                            test function and index and the debugfs entries
- *                            that represent it.
- * @test_func:		Test function
- * @fixture_index:	Fixture index
- *
- * Return: 0 on success, negative value corresponding to error code in failure
- */
-static int create_fixture_variant(struct kutf_test_function *test_func,
-		unsigned int fixture_index)
-{
-	struct kutf_test_fixture *test_fix;
-	char name[11];	/* Enough to print the MAX_UINT32 + the null terminator */
-	struct dentry *tmp;
-	int err;
-
-	test_fix = kmalloc(sizeof(*test_fix), GFP_KERNEL);
-	if (!test_fix) {
-		pr_err("Failed to create debugfs directory when adding fixture\n");
-		err = -ENOMEM;
-		goto fail_alloc;
-	}
-
-	test_fix->test_func = test_func;
-	test_fix->fixture_index = fixture_index;
-
-	snprintf(name, sizeof(name), "%d", fixture_index);
-	test_fix->dir = debugfs_create_dir(name, test_func->dir);
-	if (!test_func->dir) {
-		pr_err("Failed to create debugfs directory when adding fixture\n");
-		/* Might not be the right error, we don't get it passed back to us */
-		err = -EEXIST;
-		goto fail_dir;
-	}
-
-	tmp = debugfs_create_file("type", S_IROTH, test_fix->dir, "fixture\n",
-				  &kutf_debugfs_const_string_ops);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"type\" when adding fixture\n");
-		/* Might not be the right error, we don't get it passed back to us */
-		err = -EEXIST;
-		goto fail_file;
-	}
-
-	tmp = debugfs_create_file("run", S_IROTH, test_fix->dir, test_fix,
-				  &kutf_debugfs_run_ops);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"run\" when adding fixture\n");
-		/* Might not be the right error, we don't get it passed back to us */
-		err = -EEXIST;
-		goto fail_file;
-	}
-
-	list_add(&test_fix->node, &test_func->variant_list);
-	return 0;
-
-fail_file:
-	debugfs_remove_recursive(test_fix->dir);
-fail_dir:
-	kfree(test_fix);
-fail_alloc:
-	return err;
-}
-
-/**
- * kutf_remove_test_variant() - Destroy a previously created fixture variant.
- * @test_fix:	Test fixture
- */
-static void kutf_remove_test_variant(struct kutf_test_fixture *test_fix)
-{
-	debugfs_remove_recursive(test_fix->dir);
-	kfree(test_fix);
-}
-
-void kutf_add_test_with_filters_and_data(
-		struct kutf_suite *suite,
-		unsigned int id,
-		const char *name,
-		void (*execute)(struct kutf_context *context),
-		unsigned int filters,
-		union kutf_callback_data test_data)
-{
-	struct kutf_test_function *test_func;
-	struct dentry *tmp;
-	unsigned int i;
-
-	test_func = kmalloc(sizeof(*test_func), GFP_KERNEL);
-	if (!test_func) {
-		pr_err("Failed to allocate memory when adding test %s\n", name);
-		goto fail_alloc;
-	}
-
-	INIT_LIST_HEAD(&test_func->variant_list);
-
-	test_func->dir = debugfs_create_dir(name, suite->dir);
-	if (!test_func->dir) {
-		pr_err("Failed to create debugfs directory when adding test %s\n", name);
-		goto fail_dir;
-	}
-
-	tmp = debugfs_create_file("type", S_IROTH, test_func->dir, "test\n",
-				  &kutf_debugfs_const_string_ops);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"type\" when adding test %s\n", name);
-		goto fail_file;
-	}
-
-	test_func->filters = filters;
-	tmp = debugfs_create_x32("filters", S_IROTH, test_func->dir,
-				 &test_func->filters);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"filters\" when adding test %s\n", name);
-		goto fail_file;
-	}
-
-	test_func->test_id = id;
-	tmp = debugfs_create_u32("test_id", S_IROTH, test_func->dir,
-				 &test_func->test_id);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"test_id\" when adding test %s\n", name);
-		goto fail_file;
-	}
-
-	for (i = 0; i < suite->fixture_variants; i++) {
-		if (create_fixture_variant(test_func, i)) {
-			pr_err("Failed to create fixture %d when adding test %s\n", i, name);
-			goto fail_file;
-		}
-	}
-
-	test_func->suite = suite;
-	test_func->execute = execute;
-	test_func->test_data = test_data;
-
-	list_add(&test_func->node, &suite->test_list);
-	return;
-
-fail_file:
-	debugfs_remove_recursive(test_func->dir);
-fail_dir:
-	kfree(test_func);
-fail_alloc:
-	return;
-}
-EXPORT_SYMBOL(kutf_add_test_with_filters_and_data);
-
-void kutf_add_test_with_filters(
-		struct kutf_suite *suite,
-		unsigned int id,
-		const char *name,
-		void (*execute)(struct kutf_context *context),
-		unsigned int filters)
-{
-	union kutf_callback_data data;
-
-	data.ptr_value = NULL;
-
-	kutf_add_test_with_filters_and_data(suite,
-					    id,
-					    name,
-					    execute,
-					    suite->suite_default_flags,
-					    data);
-}
-EXPORT_SYMBOL(kutf_add_test_with_filters);
-
-void kutf_add_test(struct kutf_suite *suite,
-		unsigned int id,
-		const char *name,
-		void (*execute)(struct kutf_context *context))
-{
-	union kutf_callback_data data;
-
-	data.ptr_value = NULL;
-
-	kutf_add_test_with_filters_and_data(suite,
-					    id,
-					    name,
-					    execute,
-					    suite->suite_default_flags,
-					    data);
-}
-EXPORT_SYMBOL(kutf_add_test);
-
-/**
- * kutf_remove_test(): Remove a previously added test function.
- * @test_func: Test function
- */
-static void kutf_remove_test(struct kutf_test_function *test_func)
-{
-	struct list_head *pos;
-	struct list_head *tmp;
-
-	list_for_each_safe(pos, tmp, &test_func->variant_list) {
-		struct kutf_test_fixture *test_fix;
-
-		test_fix = list_entry(pos, struct kutf_test_fixture, node);
-		kutf_remove_test_variant(test_fix);
-	}
-
-	list_del(&test_func->node);
-	debugfs_remove_recursive(test_func->dir);
-	kfree(test_func);
-}
-
-struct kutf_suite *kutf_create_suite_with_filters_and_data(
-		struct kutf_application *app,
-		const char *name,
-		unsigned int fixture_count,
-		void *(*create_fixture)(struct kutf_context *context),
-		void (*remove_fixture)(struct kutf_context *context),
-		unsigned int filters,
-		union kutf_callback_data suite_data)
-{
-	struct kutf_suite *suite;
-	struct dentry *tmp;
-
-	suite = kmalloc(sizeof(*suite), GFP_KERNEL);
-	if (!suite) {
-		pr_err("Failed to allocate memory when creating suite %s\n", name);
-		goto fail_kmalloc;
-	}
-
-	suite->dir = debugfs_create_dir(name, app->dir);
-	if (!suite->dir) {
-		pr_err("Failed to create debugfs directory when adding test %s\n", name);
-		goto fail_debugfs;
-	}
-
-	tmp = debugfs_create_file("type", S_IROTH, suite->dir, "suite\n",
-				  &kutf_debugfs_const_string_ops);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"type\" when adding test %s\n", name);
-		goto fail_file;
-	}
-
-	INIT_LIST_HEAD(&suite->test_list);
-	suite->app = app;
-	suite->name = name;
-	suite->fixture_variants = fixture_count;
-	suite->create_fixture = create_fixture;
-	suite->remove_fixture = remove_fixture;
-	suite->suite_default_flags = filters;
-	suite->suite_data = suite_data;
-
-	list_add(&suite->node, &app->suite_list);
-
-	return suite;
-
-fail_file:
-	debugfs_remove_recursive(suite->dir);
-fail_debugfs:
-	kfree(suite);
-fail_kmalloc:
-	return NULL;
-}
-EXPORT_SYMBOL(kutf_create_suite_with_filters_and_data);
-
-struct kutf_suite *kutf_create_suite_with_filters(
-		struct kutf_application *app,
-		const char *name,
-		unsigned int fixture_count,
-		void *(*create_fixture)(struct kutf_context *context),
-		void (*remove_fixture)(struct kutf_context *context),
-		unsigned int filters)
-{
-	union kutf_callback_data data;
-
-	data.ptr_value = NULL;
-	return kutf_create_suite_with_filters_and_data(app,
-						       name,
-						       fixture_count,
-						       create_fixture,
-						       remove_fixture,
-						       filters,
-						       data);
-}
-EXPORT_SYMBOL(kutf_create_suite_with_filters);
-
-struct kutf_suite *kutf_create_suite(
-		struct kutf_application *app,
-		const char *name,
-		unsigned int fixture_count,
-		void *(*create_fixture)(struct kutf_context *context),
-		void (*remove_fixture)(struct kutf_context *context))
-{
-	union kutf_callback_data data;
-
-	data.ptr_value = NULL;
-	return kutf_create_suite_with_filters_and_data(app,
-						       name,
-						       fixture_count,
-						       create_fixture,
-						       remove_fixture,
-						       KUTF_F_TEST_GENERIC,
-						       data);
-}
-EXPORT_SYMBOL(kutf_create_suite);
-
-/**
- * kutf_destroy_suite() - Destroy a previously added test suite.
- * @suite:	Test suite
- */
-static void kutf_destroy_suite(struct kutf_suite *suite)
-{
-	struct list_head *pos;
-	struct list_head *tmp;
-
-	list_for_each_safe(pos, tmp, &suite->test_list) {
-		struct kutf_test_function *test_func;
-
-		test_func = list_entry(pos, struct kutf_test_function, node);
-		kutf_remove_test(test_func);
-	}
-
-	list_del(&suite->node);
-	debugfs_remove_recursive(suite->dir);
-	kfree(suite);
-}
-
-struct kutf_application *kutf_create_application(const char *name)
-{
-	struct kutf_application *app;
-	struct dentry *tmp;
-
-	app = kmalloc(sizeof(*app), GFP_KERNEL);
-	if (!app) {
-		pr_err("Failed to create allocate memory when creating application %s\n", name);
-		goto fail_kmalloc;
-	}
-
-	app->dir = debugfs_create_dir(name, base_dir);
-	if (!app->dir) {
-		pr_err("Failed to create debugfs direcotry when creating application %s\n", name);
-		goto fail_debugfs;
-	}
-
-	tmp = debugfs_create_file("type", S_IROTH, app->dir, "application\n",
-				  &kutf_debugfs_const_string_ops);
-	if (!tmp) {
-		pr_err("Failed to create debugfs file \"type\" when creating application %s\n", name);
-		goto fail_file;
-	}
-
-	INIT_LIST_HEAD(&app->suite_list);
-	app->name = name;
-
-	return app;
-
-fail_file:
-	debugfs_remove_recursive(app->dir);
-fail_debugfs:
-	kfree(app);
-fail_kmalloc:
-	return NULL;
-}
-EXPORT_SYMBOL(kutf_create_application);
-
-void kutf_destroy_application(struct kutf_application *app)
-{
-	struct list_head *pos;
-	struct list_head *tmp;
-
-	list_for_each_safe(pos, tmp, &app->suite_list) {
-		struct kutf_suite *suite;
-
-		suite = list_entry(pos, struct kutf_suite, node);
-		kutf_destroy_suite(suite);
-	}
-
-	debugfs_remove_recursive(app->dir);
-	kfree(app);
-}
-EXPORT_SYMBOL(kutf_destroy_application);
-
-static struct kutf_context *kutf_create_context(
-		struct kutf_test_fixture *test_fix)
-{
-	struct kutf_context *new_context;
-
-	new_context = kmalloc(sizeof(*new_context), GFP_KERNEL);
-	if (!new_context) {
-		pr_err("Failed to allocate test context");
-		goto fail_alloc;
-	}
-
-	new_context->result_set = kutf_create_result_set();
-	if (!new_context->result_set) {
-		pr_err("Failed to create resultset");
-		goto fail_result_set;
-	}
-
-	new_context->test_fix = test_fix;
-	/* Save the pointer to the suite as the callbacks will require it */
-	new_context->suite = test_fix->test_func->suite;
-	new_context->status = KUTF_RESULT_UNKNOWN;
-	new_context->expected_status = KUTF_RESULT_UNKNOWN;
-
-	kutf_mempool_init(&new_context->fixture_pool);
-	new_context->fixture = NULL;
-	new_context->fixture_index = test_fix->fixture_index;
-	new_context->fixture_name = NULL;
-	new_context->test_data = test_fix->test_func->test_data;
-
-	return new_context;
-
-fail_result_set:
-	kfree(new_context);
-fail_alloc:
-	return NULL;
-}
-
-static void kutf_destroy_context(struct kutf_context *context)
-{
-	kutf_destroy_result_set(context->result_set);
-	kutf_mempool_destroy(&context->fixture_pool);
-	kfree(context);
-}
-
-static void kutf_set_result(struct kutf_context *context,
-		enum kutf_result_status status)
-{
-	context->status = status;
-}
-
-static void kutf_set_expected_result(struct kutf_context *context,
-		enum kutf_result_status expected_status)
-{
-	context->expected_status = expected_status;
-}
-
-/**
- * kutf_test_log_result() - Log a result for the specified test context
- * @context:	Test context
- * @message:	Result string
- * @new_status:	Result status
- */
-static void kutf_test_log_result(
-	struct kutf_context *context,
-	const char *message,
-	enum kutf_result_status new_status)
-{
-	if (context->status < new_status)
-		context->status = new_status;
-
-	if (context->expected_status != new_status)
-		kutf_add_result(&context->fixture_pool, context->result_set,
-				new_status, message);
-}
-
-void kutf_test_log_result_external(
-	struct kutf_context *context,
-	const char *message,
-	enum kutf_result_status new_status)
-{
-	kutf_test_log_result(context, message, new_status);
-}
-EXPORT_SYMBOL(kutf_test_log_result_external);
-
-void kutf_test_expect_abort(struct kutf_context *context)
-{
-	kutf_set_expected_result(context, KUTF_RESULT_ABORT);
-}
-EXPORT_SYMBOL(kutf_test_expect_abort);
-
-void kutf_test_expect_fatal(struct kutf_context *context)
-{
-	kutf_set_expected_result(context, KUTF_RESULT_FATAL);
-}
-EXPORT_SYMBOL(kutf_test_expect_fatal);
-
-void kutf_test_expect_fail(struct kutf_context *context)
-{
-	kutf_set_expected_result(context, KUTF_RESULT_FAIL);
-}
-EXPORT_SYMBOL(kutf_test_expect_fail);
-
-void kutf_test_expect_warn(struct kutf_context *context)
-{
-	kutf_set_expected_result(context, KUTF_RESULT_WARN);
-}
-EXPORT_SYMBOL(kutf_test_expect_warn);
-
-void kutf_test_expect_pass(struct kutf_context *context)
-{
-	kutf_set_expected_result(context, KUTF_RESULT_PASS);
-}
-EXPORT_SYMBOL(kutf_test_expect_pass);
-
-void kutf_test_skip(struct kutf_context *context)
-{
-	kutf_set_result(context, KUTF_RESULT_SKIP);
-	kutf_set_expected_result(context, KUTF_RESULT_UNKNOWN);
-
-	kutf_test_log_result(context, "Test skipped", KUTF_RESULT_SKIP);
-}
-EXPORT_SYMBOL(kutf_test_skip);
-
-void kutf_test_skip_msg(struct kutf_context *context, const char *message)
-{
-	kutf_set_result(context, KUTF_RESULT_SKIP);
-	kutf_set_expected_result(context, KUTF_RESULT_UNKNOWN);
-
-	kutf_test_log_result(context, kutf_dsprintf(&context->fixture_pool,
-			     "Test skipped: %s", message), KUTF_RESULT_SKIP);
-	kutf_test_log_result(context, "!!!Test skipped!!!", KUTF_RESULT_SKIP);
-}
-EXPORT_SYMBOL(kutf_test_skip_msg);
-
-void kutf_test_debug(struct kutf_context *context, char const *message)
-{
-	kutf_test_log_result(context, message, KUTF_RESULT_DEBUG);
-}
-EXPORT_SYMBOL(kutf_test_debug);
-
-void kutf_test_pass(struct kutf_context *context, char const *message)
-{
-	static const char explicit_message[] = "(explicit pass)";
-
-	if (!message)
-		message = explicit_message;
-
-	kutf_test_log_result(context, message, KUTF_RESULT_PASS);
-}
-EXPORT_SYMBOL(kutf_test_pass);
-
-void kutf_test_info(struct kutf_context *context, char const *message)
-{
-	kutf_test_log_result(context, message, KUTF_RESULT_INFO);
-}
-EXPORT_SYMBOL(kutf_test_info);
-
-void kutf_test_warn(struct kutf_context *context, char const *message)
-{
-	kutf_test_log_result(context, message, KUTF_RESULT_WARN);
-}
-EXPORT_SYMBOL(kutf_test_warn);
-
-void kutf_test_fail(struct kutf_context *context, char const *message)
-{
-	kutf_test_log_result(context, message, KUTF_RESULT_FAIL);
-}
-EXPORT_SYMBOL(kutf_test_fail);
-
-void kutf_test_fatal(struct kutf_context *context, char const *message)
-{
-	kutf_test_log_result(context, message, KUTF_RESULT_FATAL);
-}
-EXPORT_SYMBOL(kutf_test_fatal);
-
-void kutf_test_abort(struct kutf_context *context)
-{
-	kutf_test_log_result(context, "", KUTF_RESULT_ABORT);
-}
-EXPORT_SYMBOL(kutf_test_abort);
-
-/**
- * init_kutf_core() - Module entry point.
- *
- * Create the base entry point in debugfs.
- */
-static int __init init_kutf_core(void)
-{
-	int ret;
-
-	base_dir = debugfs_create_dir("kutf_tests", NULL);
-	if (!base_dir) {
-		ret = -ENODEV;
-		goto exit_dir;
-	}
-
-	return 0;
-
-exit_dir:
-	return ret;
-}
-
-/**
- * exit_kutf_core() - Module exit point.
- *
- * Remove the base entry point in debugfs.
- */
-static void __exit exit_kutf_core(void)
-{
-	debugfs_remove_recursive(base_dir);
-}
-
-#else	/* defined(CONFIG_DEBUG_FS) */
-
-/**
- * init_kutf_core() - Module entry point.
- *
- * Stub for when build against a kernel without debugfs support
- */
-static int __init init_kutf_core(void)
-{
-	pr_debug("KUTF requires a kernel with debug fs support");
-
-	return -ENODEV;
-}
-
-/**
- * exit_kutf_core() - Module exit point.
- *
- * Stub for when build against a kernel without debugfs support
- */
-static void __exit exit_kutf_core(void)
-{
-}
-#endif	/* defined(CONFIG_DEBUG_FS) */
-
-MODULE_LICENSE("GPL");
-
-module_init(init_kutf_core);
-module_exit(exit_kutf_core);
diff --git a/drivers/gpu/arm/midgard/tests/kutf/kutf_utils.c b/drivers/gpu/arm/midgard/tests/kutf/kutf_utils.c
deleted file mode 100644
index a429a2d..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/kutf_utils.c
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-/* Kernel UTF utility functions */
-
-#include <linux/mutex.h>
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/printk.h>
-
-#include <kutf/kutf_utils.h>
-#include <kutf/kutf_mem.h>
-
-static char tmp_buffer[KUTF_MAX_DSPRINTF_LEN];
-
-DEFINE_MUTEX(buffer_lock);
-
-const char *kutf_dsprintf(struct kutf_mempool *pool,
-		const char *fmt, ...)
-{
-	va_list args;
-	int len;
-	int size;
-	void *buffer;
-
-	mutex_lock(&buffer_lock);
-	va_start(args, fmt);
-	len = vsnprintf(tmp_buffer, sizeof(tmp_buffer), fmt, args);
-	va_end(args);
-
-	if (len < 0) {
-		pr_err("kutf_dsprintf: Bad format dsprintf format %s\n", fmt);
-		goto fail_format;
-	}
-
-	if (len >= sizeof(tmp_buffer)) {
-		pr_warn("kutf_dsprintf: Truncated dsprintf message %s\n", fmt);
-		size = sizeof(tmp_buffer);
-	} else {
-		size = len + 1;
-	}
-
-	buffer = kutf_mempool_alloc(pool, size);
-	if (!buffer)
-		goto fail_alloc;
-
-	memcpy(buffer, tmp_buffer, size);
-	mutex_unlock(&buffer_lock);
-
-	return buffer;
-
-fail_alloc:
-fail_format:
-	mutex_unlock(&buffer_lock);
-	return NULL;
-}
-EXPORT_SYMBOL(kutf_dsprintf);
diff --git a/drivers/gpu/arm/midgard/tests/kutf/sconscript b/drivers/gpu/arm/midgard/tests/kutf/sconscript
deleted file mode 100644
index d7f1124..0000000
--- a/drivers/gpu/arm/midgard/tests/kutf/sconscript
+++ /dev/null
@@ -1,21 +0,0 @@
-#
-# (C) COPYRIGHT 2014-2016, 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-Import('kutf_env')
-
-make_args = kutf_env.kernel_get_config_defines(ret_list = True)
-
-mod = kutf_env.BuildKernelModule('$STATIC_LIB_PATH/kutf.ko', Glob('*.c'), make_args = make_args)
-kutf_env.KernelObjTarget('kutf', mod)
diff --git a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kbuild b/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kbuild
deleted file mode 100644
index 0cd9ceb..0000000
--- a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kbuild
+++ /dev/null
@@ -1,20 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-ccflags-y += -I$(src)/../include -I$(src)/../../../ -I$(src)/../../ -I$(src)/../../backend/gpu -I$(srctree)/drivers/staging/android
-
-obj-$(CONFIG_MALI_IRQ_LATENCY) += mali_kutf_irq_test.o
-
-mali_kutf_irq_test-y := mali_kutf_irq_test_main.o
diff --git a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kconfig b/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kconfig
deleted file mode 100644
index 16f68d1..0000000
--- a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kconfig
+++ /dev/null
@@ -1,23 +0,0 @@
-#
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-config MALI_IRQ_LATENCY
- tristate "Mali GPU IRQ latency measurement"
- depends on MALI_MIDGARD && MALI_DEBUG && MALI_KUTF
- default n
- help
-   This option will build a test module mali_kutf_irq_test that
-   can determine the latency of the Mali GPU IRQ on your system.
-   Choosing M here will generate a single module called mali_kutf_irq_test.
diff --git a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Makefile b/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Makefile
deleted file mode 100644
index 4e94876..0000000
--- a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Makefile
+++ /dev/null
@@ -1,51 +0,0 @@
-#
-# (C) COPYRIGHT 2015, 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-# linux build system bootstrap for out-of-tree module
-
-# default to building for the host
-ARCH ?= $(shell uname -m)
-
-ifeq ($(KDIR),)
-$(error Must specify KDIR to point to the kernel to target))
-endif
-
-TEST_CCFLAGS := \
-	-DMALI_DEBUG=$(MALI_DEBUG) \
-	-DMALI_BACKEND_KERNEL=$(MALI_BACKEND_KERNEL) \
-	-DMALI_MODEL=$(MALI_MODEL) \
-	-DMALI_NO_MALI=$(MALI_NO_MALI) \
-	-DMALI_BASE_QA_LEAK=$(MALI_BASE_QA_LEAK) \
-	-DMALI_BASE_QA_RESFAIL=$(MALI_BASE_QA_RESFAIL) \
-	-DMALI_BASE_QA_USE_AFTER_FREE=$(MALI_BASE_QA_USE_AFTER_FREE) \
-	-DMALI_UNIT_TEST=$(MALI_UNIT_TEST) \
-	-DMALI_USE_UMP=$(MALI_USE_UMP) \
-	-DMALI_ERROR_INJECT_ON=$(MALI_ERROR_INJECT_ON) \
-	-DMALI_CUSTOMER_RELEASE=$(MALI_CUSTOMER_RELEASE) \
-	$(SCONS_CFLAGS) \
-	-I$(CURDIR)/../include \
-	-I$(CURDIR)/../../../../../../include \
-	-I$(CURDIR)/../../../ \
-	-I$(CURDIR)/../../ \
-	-I$(CURDIR)/../../backend/gpu \
-	-I$(CURDIR)/ \
-	-I$(srctree)/drivers/staging/android \
-	-I$(srctree)/include/linux
-
-all:
-	$(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(CURDIR) $(SCONS_CONFIGS) EXTRA_CFLAGS="$(TEST_CCFLAGS)" KBUILD_EXTRA_SYMBOLS="$(CURDIR)/../kutf/Module.symvers $(CURDIR)/../../Module.symvers" modules
-
-clean:
-	$(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(CURDIR) clean
diff --git a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c b/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
deleted file mode 100644
index ffd802f..0000000
--- a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
+++ /dev/null
@@ -1,257 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2016, 2017 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * A copy of the licence is included with the program, and can also be obtained
- * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
- * Boston, MA  02110-1301, USA.
- *
- */
-
-
-
-#include <linux/module.h>
-#include <linux/delay.h>
-#include <linux/interrupt.h>
-
-#include "mali_kbase.h"
-#include <midgard/backend/gpu/mali_kbase_device_internal.h>
-
-#include <kutf/kutf_suite.h>
-#include <kutf/kutf_utils.h>
-
-/*
- * This file contains the code which is used for measuring interrupt latency
- * of the Mali GPU IRQ. In particular, function mali_kutf_irq_latency() is
- * used with this purpose and it is called within KUTF framework - a kernel
- * unit test framework. The measured latency provided by this test should
- * be representative for the latency of the Mali JOB/MMU IRQs as well.
- */
-
-/* KUTF test application pointer for this test */
-struct kutf_application *irq_app;
-
-/**
- * struct kutf_irq_fixture data - test fixture used by the test functions.
- * @kbdev:	kbase device for the GPU.
- *
- */
-struct kutf_irq_fixture_data {
-	struct kbase_device *kbdev;
-};
-
-#define SEC_TO_NANO(s)	      ((s)*1000000000LL)
-
-/* ID for the GPU IRQ */
-#define GPU_IRQ_HANDLER 2
-
-#define NR_TEST_IRQS 1000000
-
-/* IRQ for the test to trigger. Currently MULTIPLE_GPU_FAULTS as we would not
- * expect to see this in normal use (e.g., when Android is running). */
-#define TEST_IRQ MULTIPLE_GPU_FAULTS
-
-#define IRQ_TIMEOUT HZ
-
-/* Kernel API for setting irq throttle hook callback and irq time in us*/
-extern int kbase_set_custom_irq_handler(struct kbase_device *kbdev,
-		irq_handler_t custom_handler,
-		int irq_type);
-extern irqreturn_t kbase_gpu_irq_handler(int irq, void *data);
-
-static DECLARE_WAIT_QUEUE_HEAD(wait);
-static bool triggered;
-static u64 irq_time;
-
-static void *kbase_untag(void *ptr)
-{
-	return (void *)(((uintptr_t) ptr) & ~3);
-}
-
-/**
- * kbase_gpu_irq_custom_handler - Custom IRQ throttle handler
- * @irq:  IRQ number
- * @data: Data associated with this IRQ
- *
- * Return: state of the IRQ
- */
-static irqreturn_t kbase_gpu_irq_custom_handler(int irq, void *data)
-{
-	struct kbase_device *kbdev = kbase_untag(data);
-	u32 val;
-
-	val = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_STATUS), NULL);
-	if (val & TEST_IRQ) {
-		struct timespec tval;
-
-		getnstimeofday(&tval);
-		irq_time = SEC_TO_NANO(tval.tv_sec) + (tval.tv_nsec);
-
-		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), val,
-				NULL);
-
-		triggered = true;
-		wake_up(&wait);
-
-		return IRQ_HANDLED;
-	}
-
-	/* Trigger main irq handler */
-	return kbase_gpu_irq_handler(irq, data);
-}
-
-/**
- * mali_kutf_irq_default_create_fixture() - Creates the fixture data required
- *                                          for all the tests in the irq suite.
- * @context:             KUTF context.
- *
- * Return: Fixture data created on success or NULL on failure
- */
-static void *mali_kutf_irq_default_create_fixture(
-		struct kutf_context *context)
-{
-	struct kutf_irq_fixture_data *data;
-
-	data = kutf_mempool_alloc(&context->fixture_pool,
-			sizeof(struct kutf_irq_fixture_data));
-
-	if (!data)
-		goto fail;
-
-	/* Acquire the kbase device */
-	data->kbdev = kbase_find_device(-1);
-	if (data->kbdev == NULL) {
-		kutf_test_fail(context, "Failed to find kbase device");
-		goto fail;
-	}
-
-	return data;
-
-fail:
-	return NULL;
-}
-
-/**
- * mali_kutf_irq_default_remove_fixture() - Destroy fixture data previously
- *                          created by mali_kutf_irq_default_create_fixture.
- *
- * @context:             KUTF context.
- */
-static void mali_kutf_irq_default_remove_fixture(
-		struct kutf_context *context)
-{
-	struct kutf_irq_fixture_data *data = context->fixture;
-	struct kbase_device *kbdev = data->kbdev;
-
-	kbase_release_device(kbdev);
-}
-
-/**
- * mali_kutf_irq_latency() - measure GPU IRQ latency
- * @context:		kutf context within which to perform the test
- *
- * The test triggers IRQs manually, and measures the
- * time between triggering the IRQ and the IRQ handler being executed.
- *
- * This is not a traditional test, in that the pass/fail status has little
- * meaning (other than indicating that the IRQ handler executed at all). Instead
- * the results are in the latencies provided with the test result. There is no
- * meaningful pass/fail result that can be obtained here, instead the latencies
- * are provided for manual analysis only.
- */
-static void mali_kutf_irq_latency(struct kutf_context *context)
-{
-	struct kutf_irq_fixture_data *data = context->fixture;
-	struct kbase_device *kbdev = data->kbdev;
-	u64 min_time = U64_MAX, max_time = 0, average_time = 0;
-	int i;
-	bool test_failed = false;
-
-	/* Force GPU to be powered */
-	kbase_pm_context_active(kbdev);
-
-	kbase_set_custom_irq_handler(kbdev, kbase_gpu_irq_custom_handler,
-			GPU_IRQ_HANDLER);
-
-	for (i = 0; i < NR_TEST_IRQS; i++) {
-		struct timespec tval;
-		u64 start_time;
-		int ret;
-
-		triggered = false;
-		getnstimeofday(&tval);
-		start_time = SEC_TO_NANO(tval.tv_sec) + (tval.tv_nsec);
-
-		/* Trigger fake IRQ */
-		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT),
-				TEST_IRQ, NULL);
-
-		ret = wait_event_timeout(wait, triggered != false, IRQ_TIMEOUT);
-
-		if (ret == 0) {
-			kutf_test_fail(context, "Timed out waiting for IRQ\n");
-			test_failed = true;
-			break;
-		}
-
-		if ((irq_time - start_time) < min_time)
-			min_time = irq_time - start_time;
-		if ((irq_time - start_time) > max_time)
-			max_time = irq_time - start_time;
-		average_time += irq_time - start_time;
-
-		udelay(10);
-	}
-
-	/* Go back to default handler */
-	kbase_set_custom_irq_handler(kbdev, NULL, GPU_IRQ_HANDLER);
-
-	kbase_pm_context_idle(kbdev);
-
-	if (!test_failed) {
-		const char *results;
-
-		do_div(average_time, NR_TEST_IRQS);
-		results = kutf_dsprintf(&context->fixture_pool,
-				"Min latency = %lldns, Max latency = %lldns, Average latency = %lldns\n",
-				min_time, max_time, average_time);
-		kutf_test_pass(context, results);
-	}
-}
-
-/**
- * Module entry point for this test.
- */
-int mali_kutf_irq_test_main_init(void)
-{
-	struct kutf_suite *suite;
-
-	irq_app = kutf_create_application("irq");
-	suite = kutf_create_suite(irq_app, "irq_default",
-			1, mali_kutf_irq_default_create_fixture,
-			mali_kutf_irq_default_remove_fixture);
-
-	kutf_add_test(suite, 0x0, "irq_latency",
-			mali_kutf_irq_latency);
-	return 0;
-}
-
-/**
- * Module exit point for this test.
- */
-void mali_kutf_irq_test_main_exit(void)
-{
-	kutf_destroy_application(irq_app);
-}
-
-module_init(mali_kutf_irq_test_main_init);
-module_exit(mali_kutf_irq_test_main_exit);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("ARM Ltd.");
-MODULE_VERSION("1.0");
diff --git a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/sconscript b/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/sconscript
deleted file mode 100644
index ec837f1..0000000
--- a/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/sconscript
+++ /dev/null
@@ -1,30 +0,0 @@
-#
-# (C) COPYRIGHT 2015, 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-import os
-Import('env')
-
-src = [Glob('#kernel/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/*.c'), Glob('#kernel/drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Makefile')]
-
-if env.GetOption('clean') :
-	env.Execute(Action("make clean", '[CLEAN] mali_kutf_irq_test'))
-	cmd = env.Command('$STATIC_LIB_PATH/mali_kutf_irq_test.ko', src, [])
-	env.KernelObjTarget('mali_kutf_irq_test', cmd)
-else:
-	makeAction=Action("cd ${SOURCE.dir} && make MALI_DEBUG=${debug} MALI_BACKEND_KERNEL=1 MALI_ERROR_INJECT_ON=${error_inject} MALI_MODEL=${mali_model} MALI_NO_MALI=${no_mali} MALI_HW_VERSION=${hwver} MALI_UNIT_TEST=${unit} MALI_USE_UMP=${ump} MALI_CUSTOMER_RELEASE=${release} %s %s && ( ( [ -f mali_kutf_irq_test.ko ] && cp mali_kutf_irq_test.ko $STATIC_LIB_PATH/ ) || touch $STATIC_LIB_PATH/mali_kutf_irq_test.ko)" % (env.base_get_qa_settings(), env.kernel_get_config_defines()), '$MAKECOMSTR')
-	cmd = env.Command('$STATIC_LIB_PATH/mali_kutf_irq_test.ko', src, [makeAction])
-	env.Depends('$STATIC_LIB_PATH/mali_kutf_irq_test.ko', '$STATIC_LIB_PATH/kutf.ko')
-	env.Depends('$STATIC_LIB_PATH/mali_kutf_irq_test.ko', '$STATIC_LIB_PATH/mali_kbase.ko')
-	env.KernelObjTarget('mali_kutf_irq_test', cmd)
diff --git a/drivers/gpu/arm/midgard/tests/sconscript b/drivers/gpu/arm/midgard/tests/sconscript
deleted file mode 100644
index 5337e10..0000000
--- a/drivers/gpu/arm/midgard/tests/sconscript
+++ /dev/null
@@ -1,37 +0,0 @@
-#
-# (C) COPYRIGHT 2010-2011, 2013, 2017 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU licence.
-#
-# A copy of the licence is included with the program, and can also be obtained
-# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
-# Boston, MA  02110-1301, USA.
-#
-#
-
-
-Import ('env')
-
-kutf_env = env.Clone()
-kutf_env.Append(CPPPATH = '#kernel/drivers/gpu/arm/midgard/tests/include')
-Export('kutf_env')
-
-if Glob('internal/sconscript'):
-	SConscript('internal/sconscript')
-
-if kutf_env['debug'] == '1':
-	SConscript('kutf/sconscript')
-	SConscript('mali_kutf_irq_test/sconscript')
-
-	if Glob('kutf_test/sconscript'):
-		SConscript('kutf_test/sconscript')
-
-	if Glob('kutf_test_runner/sconscript'):
-		SConscript('kutf_test_runner/sconscript')
-
-if env['unit'] == '1':
-	SConscript('mali_kutf_ipa_test/sconscript')
-	SConscript('mali_kutf_vinstr_test/sconscript')
-- 
2.7.4

